---
phase: 26-engine-foundation
plan: 02
type: tdd
wave: 2
depends_on: ["26-01"]
files_modified:
  - bae/repl/engine.py
  - bae/repl/shell.py
  - tests/repl/test_engine.py
autonomous: true

must_haves:
  truths:
    - "GraphRegistry.submit() creates a GraphRun with RUNNING state and submits it to TaskManager"
    - "GraphRun transitions to DONE/FAILED/CANCELLED on completion/error/cancellation"
    - "TimingLM wraps all four LM protocol methods and records per-node durations"
    - "Running graphs appear in Ctrl-C task menu with graph: prefix"
    - "Completed runs are archived in bounded deque (max 20)"
  artifacts:
    - path: "bae/repl/engine.py"
      provides: "GraphState, GraphRun, NodeTiming, TimingLM, GraphRegistry"
      contains: "GraphRegistry"
    - path: "bae/repl/shell.py"
      provides: "Engine instance on CortexShell"
      contains: "self.engine"
    - path: "tests/repl/test_engine.py"
      provides: "Unit tests for engine lifecycle, timing, and registry"
      contains: "TestGraphRegistry"
  key_links:
    - from: "bae/repl/engine.py"
      to: "bae/repl/tasks.py"
      via: "GraphRegistry.submit() calls TaskManager.submit()"
      pattern: "tm\\.submit"
    - from: "bae/repl/engine.py"
      to: "bae/graph.py"
      via: "Engine calls graph.arun(dep_cache=...) with TimingLM"
      pattern: "graph\\.arun.*dep_cache"
    - from: "bae/repl/engine.py"
      to: "bae/lm.py"
      via: "TimingLM delegates to inner LM, conforms to LM Protocol"
      pattern: "class TimingLM"
    - from: "bae/repl/shell.py"
      to: "bae/repl/engine.py"
      via: "CortexShell stores self.engine = GraphRegistry()"
      pattern: "self\\.engine"
---

<objective>
Build GraphRegistry, TimingLM, and engine wrapper for managed graph execution.

Purpose: Enable graphs to run as tracked concurrent tasks inside cortex with lifecycle states and per-node timing. This is the foundation all subsequent v6.0 phases build on.
Output: New bae/repl/engine.py module, shell integration, comprehensive tests.
</objective>

<execution_context>
@/Users/dzaramelcone/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dzaramelcone/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/26-engine-foundation/26-RESEARCH.md
@.planning/phases/26-engine-foundation/26-01-SUMMARY.md

@bae/graph.py
@bae/lm.py
@bae/repl/tasks.py
@bae/repl/shell.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create engine module with GraphRegistry, TimingLM, and lifecycle tracking</name>
  <files>bae/repl/engine.py, tests/repl/test_engine.py</files>
  <action>
**RED phase -- write failing tests first:**

Create `tests/repl/test_engine.py` with these test classes:

**TestGraphState:**
- `test_states_exist` -- GraphState has RUNNING, DONE, FAILED, CANCELLED members

**TestNodeTiming:**
- `test_duration_ms` -- NodeTiming with start_ns=0, end_ns=1_000_000 has duration_ms == 1.0

**TestTimingLM:**
- `test_conforms_to_lm_protocol` -- `isinstance(TimingLM(mock_lm, mock_run), LM)` is True (LM is @runtime_checkable)
- `test_fill_records_timing` -- Call timing_lm.fill(), verify it delegates to inner LM AND appends a NodeTiming to run.node_timings with correct node_type
- `test_choose_type_delegates` -- Call timing_lm.choose_type(), verify it delegates and returns correct type (no timing recorded for routing decisions)
- `test_make_delegates` -- Call timing_lm.make(), verify delegation
- `test_decide_delegates` -- Call timing_lm.decide(), verify delegation

**TestGraphRegistry:**
- `test_submit_creates_running_graphrun` -- submit() returns GraphRun with state RUNNING and incremented run_id
- `test_submit_creates_taskmanager_task` -- After submit(), TaskManager.active() contains a task with name matching `graph:gN:NodeName`
- `test_run_completes_to_done` -- Submit a graph with MockLM, await the task, verify run.state == DONE
- `test_run_failure_sets_failed` -- Submit a graph that raises, await the task, verify run.state == FAILED
- `test_run_cancellation_sets_cancelled` -- Submit a graph, revoke via TaskManager, verify run.state == CANCELLED
- `test_completed_runs_archived` -- After run completes, it moves from active runs to completed deque
- `test_completed_deque_bounded` -- Submit and complete 25 graphs, verify completed deque has exactly 20 (maxlen=20)
- `test_active_runs_lists_running` -- Submit two graphs, verify active() returns both
- `test_timing_lm_injected_via_dep_cache` -- Submit with an LM, verify dep_cache contains LM_KEY pointing to a TimingLM wrapping the provided LM

For test fixtures, create a simple graph:
```python
class Start(Node):
    text: str
    def __call__(self) -> End: ...

class End(Node):
    reply: str
```

Use a MockLM that returns `End(reply="done")` for fill calls. Use a real TaskManager instance (it's lightweight, no I/O).

Run tests: `uv run pytest tests/repl/test_engine.py -x -q` -- should FAIL.

**GREEN phase -- implement `bae/repl/engine.py`:**

```python
"""Graph engine: registry, lifecycle tracking, and per-node timing."""

from __future__ import annotations

import asyncio
import enum
import time
from collections import deque
from dataclasses import dataclass, field
from typing import TYPE_CHECKING

from bae.lm import LM
from bae.resolver import LM_KEY

if TYPE_CHECKING:
    from bae.graph import Graph
    from bae.repl.tasks import TaskManager


class GraphState(enum.Enum):
    RUNNING = "running"
    DONE = "done"
    FAILED = "failed"
    CANCELLED = "cancelled"


@dataclass
class NodeTiming:
    node_type: str
    start_ns: int
    end_ns: int = 0

    @property
    def duration_ms(self) -> float:
        return (self.end_ns - self.start_ns) / 1_000_000


@dataclass
class GraphRun:
    run_id: str
    graph: Graph
    state: GraphState = GraphState.RUNNING
    node_timings: list[NodeTiming] = field(default_factory=list)
    current_node: str = ""
    started_ns: int = field(default_factory=time.perf_counter_ns)
    ended_ns: int = 0


class TimingLM:
    """LM wrapper that records fill durations for engine instrumentation."""

    def __init__(self, inner: LM, run: GraphRun):
        self._inner = inner
        self._run = run

    async def fill(self, target, resolved, instruction, source=None):
        start = time.perf_counter_ns()
        result = await self._inner.fill(target, resolved, instruction, source)
        end = time.perf_counter_ns()
        self._run.current_node = target.__name__
        self._run.node_timings.append(
            NodeTiming(node_type=target.__name__, start_ns=start, end_ns=end)
        )
        return result

    async def choose_type(self, types, context):
        return await self._inner.choose_type(types, context)

    async def make(self, node, target):
        start = time.perf_counter_ns()
        result = await self._inner.make(node, target)
        end = time.perf_counter_ns()
        self._run.current_node = target.__name__
        self._run.node_timings.append(
            NodeTiming(node_type=target.__name__, start_ns=start, end_ns=end)
        )
        return result

    async def decide(self, node):
        return await self._inner.decide(node)


class GraphRegistry:
    def __init__(self):
        self._runs: dict[str, GraphRun] = {}
        self._next_id: int = 1
        self._completed: deque[GraphRun] = deque(maxlen=20)

    def submit(
        self, graph: Graph, tm: TaskManager, *, lm: LM | None = None, **kwargs
    ) -> GraphRun:
        run_id = f"g{self._next_id}"
        self._next_id += 1
        run = GraphRun(run_id=run_id, graph=graph)
        self._runs[run_id] = run
        coro = self._execute(run, lm=lm, **kwargs)
        tm.submit(coro, name=f"graph:{run_id}:{graph.start.__name__}", mode="graph")
        return run

    async def _execute(self, run: GraphRun, *, lm: LM | None = None, **kwargs):
        try:
            if lm is None:
                from bae.lm import ClaudeCLIBackend
                lm = ClaudeCLIBackend()
            timing_lm = TimingLM(lm, run)
            dep_cache = {LM_KEY: timing_lm}
            result = await run.graph.arun(dep_cache=dep_cache, **kwargs)
            run.state = GraphState.DONE
            return result
        except asyncio.CancelledError:
            run.state = GraphState.CANCELLED
            raise
        except Exception:
            run.state = GraphState.FAILED
            raise
        finally:
            run.ended_ns = time.perf_counter_ns()
            self._archive(run)

    def _archive(self, run: GraphRun) -> None:
        self._runs.pop(run.run_id, None)
        self._completed.append(run)

    def active(self) -> list[GraphRun]:
        return [r for r in self._runs.values() if r.state == GraphState.RUNNING]

    def get(self, run_id: str) -> GraphRun | None:
        run = self._runs.get(run_id)
        if run:
            return run
        for r in self._completed:
            if r.run_id == run_id:
                return r
        return None
```

Key design decisions:
- TimingLM records timing on `fill` and `make` (the LM calls that produce nodes). `choose_type` and `decide` are routing decisions -- not timed as separate node events.
- The engine passes `dep_cache={LM_KEY: timing_lm}` to `graph.arun()`. This means the TimingLM replaces the default LM in the resolver cache. The `lm` parameter of `arun()` is NOT passed (it would be overridden by dep_cache anyway since dep_cache.update runs after LM_KEY is set).
- Actually, since `arun()` sets `cache = {LM_KEY: lm}` first, then `cache.update(dep_cache)`, we need to NOT pass lm to arun. Instead pass it only via dep_cache. So call: `await run.graph.arun(dep_cache=dep_cache, **kwargs)` -- arun will use its default LM for the initial {LM_KEY: lm} but dep_cache.update will overwrite it with TimingLM. This works correctly.

Run tests: `uv run pytest tests/repl/test_engine.py -x -q` -- should PASS.
  </action>
  <verify>
`uv run pytest tests/repl/test_engine.py -x -q` passes all engine tests.
`uv run pytest tests/ -x -q --ignore=tests/test_integration.py` passes (no regressions).
  </verify>
  <done>
GraphRegistry tracks runs by ID with lifecycle states. TimingLM conforms to LM protocol and records per-fill timing. Completed runs archived in bounded deque. All tests pass.
  </done>
</task>

<task type="auto">
  <name>Task 2: Integrate engine into CortexShell</name>
  <files>bae/repl/shell.py</files>
  <action>
In `bae/repl/shell.py`:

1. Add import at top: `from bae.repl.engine import GraphRegistry`

2. In `CortexShell.__init__()`, after `self.tm = TaskManager()`, add:
   ```python
   self.engine = GraphRegistry()
   ```

3. Add `self.engine` to namespace so Dzara can use it from PY mode:
   ```python
   self.namespace["engine"] = self.engine
   ```

4. Update `_run_graph()` to use the engine instead of `channel_arun()`:
   ```python
   async def _run_graph(self, text: str) -> None:
       """GRAPH mode: graph execution via engine."""
       graph = self.namespace.get("graph")
       if not graph:
           self.router.write("graph", "(no graph in namespace)", mode="GRAPH")
           return
       try:
           run = self.engine.submit(graph, self.tm, lm=self._lm, text=text)
           self.router.write(
               "graph", f"submitted {run.run_id}", mode="GRAPH",
               metadata={"type": "lifecycle", "run_id": run.run_id},
           )
       except Exception as exc:
           tb = traceback.format_exc()
           self.router.write("graph", tb.rstrip("\n"), mode="GRAPH", metadata={"type": "error"})
   ```

   Note: The old `_run_graph` awaited the graph inline. The new version submits via engine (which uses TaskManager), so the graph runs in the background. The `_dispatch` method already wraps GRAPH mode in `tm.submit()` -- but now _run_graph itself submits via engine. So we need to adjust _dispatch to NOT double-wrap GRAPH mode. Change the GRAPH dispatch to call `await self._run_graph(text)` directly (not via tm.submit), since _run_graph now handles its own task submission internally via engine.

5. In `_dispatch()`, change the GRAPH mode branch from:
   ```python
   self.tm.submit(self._run_graph(text), name=f"graph:{text[:30]}", mode="graph")
   ```
   to:
   ```python
   await self._run_graph(text)
   ```
   This is safe because _run_graph is now a quick submit-and-return (no awaiting the graph). The actual graph execution is handled by the engine's TaskManager submission.

Verify: `uv run pytest tests/ -x -q --ignore=tests/test_integration.py` -- all pass.
  </action>
  <verify>
`uv run pytest tests/ -x -q --ignore=tests/test_integration.py` passes.
Verify `self.engine` appears in CortexShell.__init__().
Verify GRAPH mode dispatch no longer double-wraps in tm.submit().
  </verify>
  <done>
CortexShell has self.engine = GraphRegistry(). GRAPH mode submits via engine. Engine is in namespace for PY mode access. Graph tasks appear in Ctrl-C menu via TaskManager.
  </done>
</task>

</tasks>

<verification>
1. `uv run pytest tests/repl/test_engine.py -x -q` -- all engine tests pass
2. `uv run pytest tests/ -x -q --ignore=tests/test_integration.py` -- full suite passes
3. Verify `isinstance(TimingLM(mock_lm, mock_run), LM)` in tests
4. Verify GraphRun state transitions: RUNNING -> DONE, RUNNING -> FAILED, RUNNING -> CANCELLED
5. Verify completed deque is bounded at 20
6. Verify CortexShell.engine exists and is in namespace
</verification>

<success_criteria>
- GraphRegistry tracks graph runs with lifecycle states
- TimingLM conforms to LM protocol and records per-fill timing
- Engine is backend-agnostic (any LM works)
- Graphs appear in Ctrl-C task menu
- Completed runs archived in bounded deque
- CortexShell integrates engine
- Zero test regressions
</success_criteria>

<output>
After completion, create `.planning/phases/26-engine-foundation/26-02-SUMMARY.md`
</output>

---
phase: 30-agent-core-extraction
plan: 02
type: execute
wave: 2
depends_on: ["30-01"]
files_modified:
  - bae/repl/ai.py
  - bae/lm.py
  - bae/__init__.py
  - tests/test_agent.py
  - tests/repl/test_ai.py
autonomous: true

must_haves:
  truths:
    - "AI.__call__ delegates <run> block handling to agent_loop and produces identical behavior"
    - "AI.extract_executable still works (delegates to bae.agent.extract_executable)"
    - "AgenticBackend implements the LM protocol (fill, choose_type, make, decide)"
    - "AgenticBackend.fill uses agent_loop for research, then structured extraction for output"
    - "All existing REPL AI tests still pass unchanged"
    - "AgenticBackend is importable from bae"
  artifacts:
    - path: "bae/repl/ai.py"
      provides: "REPL AI wrapper using agent_loop"
      contains: "agent_loop"
    - path: "bae/lm.py"
      provides: "AgenticBackend class"
      contains: "class AgenticBackend"
    - path: "bae/__init__.py"
      provides: "AgenticBackend export"
      contains: "AgenticBackend"
  key_links:
    - from: "bae/repl/ai.py"
      to: "bae/agent.py"
      via: "import agent_loop, extract_executable"
      pattern: "from bae\\.agent import"
    - from: "bae/lm.py"
      to: "bae/agent.py"
      via: "import agent_loop, _agent_namespace, _cli_send"
      pattern: "from bae\\.agent import"
---

<objective>
Refactor REPL AI to use the extracted agent core, build AgenticBackend in bae/lm.py, and export from bae.

Purpose: Complete the extraction by wiring both consumers (REPL AI and AgenticBackend) to the shared agent core.
Output: Refactored AI, working AgenticBackend, updated exports, all tests passing.
</objective>

<execution_context>
@/Users/dzaramelcone/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dzaramelcone/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/30-agent-core-extraction/30-RESEARCH.md
@.planning/phases/30-agent-core-extraction/30-01-SUMMARY.md
@bae/repl/ai.py
@bae/lm.py
@bae/agent.py
@bae/__init__.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Refactor AI.__call__ to use agent_loop</name>
  <files>bae/repl/ai.py</files>
  <action>
Refactor `AI.__call__` in `bae/repl/ai.py` to delegate `<run>` block handling to `agent_loop` from `bae.agent`.

The REPL AI has TWO response handlers that interleave in the loop: tool tags (REPL-only) and `<run>` blocks (universal). The cleanest refactor:

1. Import `agent_loop` and `extract_executable` from `bae.agent` at module level.

2. Remove `_EXEC_BLOCK_RE` from ai.py (it now lives in agent.py). Keep all tool tag regexes in ai.py.

3. Make `AI.extract_executable` delegate: `return extract_executable(text)` (import from bae.agent). This preserves the public API.

4. Refactor `AI.__call__`:
   - Keep the preamble (cross-session context, _build_context, full_prompt assembly).
   - Keep the initial `_send` + router.write for the first response.
   - The loop now: check tool tags first (existing logic, stays). If no tool tags, delegate to agent_loop for the remainder. The agent_loop handles `<run>` extraction + execution + feedback. BUT agent_loop calls its own `send`, which won't do router.write.

   The key design challenge: agent_loop's send function must be the REPL's `_send`, and we need router.write calls for each response. Solution: wrap `_send` with a callback that also writes to router:

   ```python
   async def _repl_send(text: str) -> str:
       response = await self._send(text)
       await asyncio.sleep(0)
       self._router.write("ai", response, mode="NL",
           metadata={"type": "response", "label": self._label})
       return response
   ```

   Then the outer loop becomes:
   ```python
   response = await self._send(full_prompt)
   self._router.write(...)

   _iters = 0
   while not self._max_eval_iters or _iters < self._max_eval_iters:
       _iters += 1
       # Tool tags first (REPL-only)
       tool_results = run_tool_calls(response)
       if tool_results:
           # ... existing tool tag handling, get next response ...
           continue

       # No tool tags -- hand off to agent_loop for <run> blocks
       # But agent_loop expects to call send(prompt) first.
       # Since we already have the response, we need agent_loop
       # to process THIS response, not send a new prompt.
       #
       # Solution: Don't use agent_loop here. Instead, just import
       # extract_executable from bae.agent and keep the execution
       # logic inline but using the shared extract function.
       code, extra = extract_executable(response)
       if code is None:
           break
       # ... rest of execution logic stays the same ...
   ```

   ACTUALLY -- the cleanest approach per the research is simpler. The REPL AI's loop interleaves tool tags and `<run>` blocks on each iteration. The agent_loop only handles `<run>` blocks. These can't be cleanly composed because tool tags might appear in responses BETWEEN `<run>` iterations. So the right refactor is:

   - Move `extract_executable` and `_EXEC_BLOCK_RE` to `bae/agent.py` (done in plan 01)
   - AI.__call__ imports and uses `extract_executable` from agent.py (replacing the static method's body)
   - The eval loop body in AI.__call__ stays structurally the same but uses the imported function
   - `agent_loop` is used by AgenticBackend (which has no tool tags), NOT by the REPL AI

   This is the minimal, correct refactor. The REPL AI keeps its interleaved loop. The agent core provides the clean loop for headless use.

5. Remove `_EXEC_BLOCK_RE` from ai.py. The `extract_executable` static method becomes:
   ```python
   @staticmethod
   def extract_executable(text: str) -> tuple[str | None, int]:
       return extract_executable(text)
   ```

Run existing AI tests to confirm no behavior change.
  </action>
  <verify>uv run pytest tests/repl/test_ai.py -x -q</verify>
  <done>AI.__call__ uses extract_executable from bae.agent. _EXEC_BLOCK_RE removed from ai.py. All existing AI tests pass unchanged.</done>
</task>

<task type="auto">
  <name>Task 2: Build AgenticBackend and export from bae</name>
  <files>bae/lm.py, bae/__init__.py, tests/test_agent.py</files>
  <action>
Add `AgenticBackend` class to `bae/lm.py` that implements the `LM` protocol using `agent_loop` for fill().

1. Add imports at top of lm.py:
   ```python
   from bae.agent import agent_loop, _agent_namespace, _cli_send
   ```

2. Add `AgenticBackend` class after `ClaudeCLIBackend`:
   ```python
   class AgenticBackend:
       """LM backend with multi-turn tool use during fill().

       Two-phase fill: agent_loop runs <run> blocks for research,
       then structured extraction produces typed output.
       Delegates choose_type/make/decide to a wrapped ClaudeCLIBackend.
       """

       def __init__(self, model: str = "claude-sonnet-4-20250514", max_iters: int = 5):
           self.model = model
           self.max_iters = max_iters
           self._cli = ClaudeCLIBackend(model=model)

       async def fill(self, target, resolved, instruction, source=None):
           import uuid
           session_id = str(uuid.uuid4())
           call_count = 0

           async def send(text: str) -> str:
               nonlocal call_count
               result = await _cli_send(
                   text, model=self.model, session_id=session_id,
                   call_count=call_count,
               )
               call_count += 1
               return result

           # Build prompt from target schema + resolved deps
           prompt = _build_fill_prompt(target, resolved, instruction, source)
           prompt += "\n\nUse Python in <run> tags to research and gather information, then provide your answer."

           namespace = _agent_namespace()

           # Phase 1: Agentic research
           final_response = await agent_loop(
               prompt, send=send, namespace=namespace, max_iters=self.max_iters,
           )

           # Phase 2: Structured extraction
           plain_model = _build_plain_model(target)
           schema = transform_schema(plain_model)
           extraction_prompt = (
               f"Based on your research:\n\n{final_response}\n\n"
               f"Extract the structured data for: {instruction}"
           )
           data = await self._cli._run_cli_json(extraction_prompt, schema)

           validated = validate_plain_fields(data, target)
           all_fields = dict(resolved)
           all_fields.update(validated)
           return target.model_construct(**all_fields)

       async def choose_type(self, types, context):
           return await self._cli.choose_type(types, context)

       async def make(self, node, target):
           return await self._cli.make(node, target)

       async def decide(self, node):
           return await self._cli.decide(node)
   ```

   NOTE: `_build_fill_prompt` already exists in lm.py (it's the helper that builds the prompt for ClaudeCLIBackend.fill). If it doesn't exist as a standalone function, extract the prompt-building logic from `ClaudeCLIBackend.fill` into a module-level `_build_fill_prompt(target, resolved, instruction, source)` function that both backends call. Check `ClaudeCLIBackend.fill` for the exact prompt construction.

   If `_build_plain_model` and `validate_plain_fields` are methods on ClaudeCLIBackend rather than module-level, access them via `self._cli` or refactor to module-level as appropriate. The research says these already exist in lm.py -- check and use whatever form they're in.

3. Update `bae/__init__.py`:
   - Add `AgenticBackend` to imports: `from bae.lm import LM, ClaudeCLIBackend, AgenticBackend`
   - Add `"AgenticBackend"` to `__all__` list under LM backends section

4. Add tests to `tests/test_agent.py`:
   - `test_agentic_backend_delegates_choose_type` -- mock _cli.choose_type, verify delegation
   - `test_agentic_backend_fill_two_phase` -- mock agent_loop and _cli._run_cli_json, verify fill calls agent_loop first then structured extraction
   - These are unit tests with mocks, no real CLI calls

Run full test suite to confirm nothing breaks.
  </action>
  <verify>uv run pytest tests/test_agent.py tests/repl/test_ai.py -x -q && uv run python -c "from bae import AgenticBackend; print('OK')"</verify>
  <done>AgenticBackend in bae/lm.py implements LM protocol. Exported from bae. All tests pass. Import works.</done>
</task>

</tasks>

<verification>
- `uv run pytest tests/ -x -q --ignore=tests/test_integration.py` -- full suite passes
- `uv run python -c "from bae import AgenticBackend; from bae.agent import agent_loop, extract_executable; print('All imports OK')"` -- all exports work
- `grep -c "router\|ChannelRouter\|SessionStore" bae/agent.py` -- returns 0
- AI.extract_executable still works as before (existing tests cover this)
</verification>

<success_criteria>
REPL AI uses extract_executable from bae.agent. AgenticBackend exists in bae/lm.py with two-phase fill. Both are tested. Full test suite passes. bae/__init__.py exports AgenticBackend.
</success_criteria>

<output>
After completion, create `.planning/phases/30-agent-core-extraction/30-02-SUMMARY.md`
</output>

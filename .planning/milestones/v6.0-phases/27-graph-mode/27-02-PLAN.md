---
phase: 27-graph-mode
plan: 02
type: execute
wave: 2
depends_on: ["27-01"]
files_modified:
  - bae/repl/graph_commands.py
  - bae/repl/shell.py
  - tests/repl/test_graph_commands.py
autonomous: true

must_haves:
  truths:
    - "run <expr> evaluates a namespace expression and submits the resulting graph to the engine"
    - "list shows all graph runs with state, elapsed time, and current node"
    - "cancel <id> stops a running graph and cleans up resources"
    - "inspect <id> displays full execution trace with node timings and field values"
    - "trace <id> shows node transition history for a running or completed graph"
    - "Unknown commands show help text with available commands"
  artifacts:
    - path: "bae/repl/graph_commands.py"
      provides: "GRAPH mode command dispatcher and handlers"
      exports: ["dispatch_graph"]
    - path: "bae/repl/shell.py"
      provides: "Updated _dispatch routing to graph_commands"
      contains: "dispatch_graph"
    - path: "tests/repl/test_graph_commands.py"
      provides: "Tests for all 5 commands and dispatcher"
      contains: "test_cmd_run"
  key_links:
    - from: "bae/repl/graph_commands.py"
      to: "bae/repl/exec.py"
      via: "async_exec for run <expr> evaluation"
      pattern: "async_exec"
    - from: "bae/repl/graph_commands.py"
      to: "bae/repl/engine.py"
      via: "registry.get(), registry.active(), registry.submit_coro()"
      pattern: "registry\\.(get|active|submit_coro)"
    - from: "bae/repl/shell.py"
      to: "bae/repl/graph_commands.py"
      via: "dispatch_graph replaces _run_graph"
      pattern: "dispatch_graph"
---

<objective>
Implement the GRAPH mode command dispatcher with all 5 commands: run, list, cancel, inspect, trace.

Purpose: This gives Dzara full graph lifecycle management from GRAPH mode. `run <expr>` leverages the graph() factory callables from Plan 01. The other commands query GraphRegistry data.

Output: `graph_commands.py` module with dispatcher, shell integration, complete test coverage.
</objective>

<execution_context>
@/Users/dzaramelcone/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dzaramelcone/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/27-graph-mode/27-RESEARCH.md
@.planning/phases/27-graph-mode/27-01-SUMMARY.md
@bae/repl/shell.py
@bae/repl/engine.py
@bae/repl/exec.py
@bae/repl/views.py
@bae/repl/tasks.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Command dispatcher and all 5 handlers</name>
  <files>bae/repl/graph_commands.py, bae/repl/shell.py</files>
  <action>
**Create `bae/repl/graph_commands.py`:**

This module contains `dispatch_graph(text, shell)` and individual command handlers. Follow the existing codebase pattern: output through `shell.router.write("graph", ..., mode="GRAPH")`.

```python
async def dispatch_graph(text: str, shell) -> None:
```

Parse `text.strip().split(None, 1)` to get `(cmd, arg)`. Dispatch to handlers via a simple dict. Unknown commands write help text listing available commands.

**Command: `run <expr>`** (MODE-02)

```python
async def _cmd_run(arg: str, shell) -> None:
```

- If `arg` is empty, write usage: `"usage: run <expr>"` and return.
- Use `async_exec(arg, shell.namespace)` to evaluate the expression. Import `async_exec` from `bae.repl.exec`.
- Check the result type:
  - If `asyncio.iscoroutine(result)`: The expression returned a coroutine (e.g., `ootd(user_info=..., user_message=...)`). Extract a display name: try `result.cr_code.co_qualname` or fall back to `"graph"`. Submit via `shell.engine.submit_coro(result, shell.tm, name=name)`.
  - If it's a Graph instance (import Graph from bae.graph): Submit via `shell.engine.submit(result, shell.tm, lm=shell._lm)`.
  - Otherwise: Write error `f"expected coroutine or Graph, got {type(result).__name__}"`. If result is a coroutine, close it first with `.close()`.
- On successful submission, write `f"submitted {run.run_id}"` with metadata `{"type": "lifecycle", "run_id": run.run_id}`.
- Attach a done callback on the TaskManager task (same pattern as current `_run_graph` in shell.py) to surface completion/failure/cancellation through the graph channel.
- Wrap everything in try/except, format traceback on error.

**The done callback pattern** (extract from current shell.py _run_graph):
```python
def _attach_done_callback(run, shell):
    for tt in shell.tm.active():
        if tt.name.startswith(f"graph:{run.run_id}:"):
            def _on_done(task, _run=run):
                if task.cancelled():
                    shell.router.write("graph", f"{_run.run_id} cancelled", mode="GRAPH",
                        metadata={"type": "lifecycle", "run_id": _run.run_id})
                elif task.exception() is not None:
                    shell.router.write("graph", f"{_run.run_id} failed: {_run.error}", mode="GRAPH",
                        metadata={"type": "error", "run_id": _run.run_id})
                else:
                    shell.router.write("graph", f"{_run.run_id} done", mode="GRAPH",
                        metadata={"type": "lifecycle", "run_id": _run.run_id})
            tt.task.add_done_callback(_on_done)
            break
```

**Command: `list` (alias: `ls`)** (MODE-03)

```python
async def _cmd_list(arg: str, shell) -> None:
```

- Collect all runs: `shell.engine.active()` + `list(shell.engine._completed)`.
- If no runs, write `"(no graph runs)"` and return.
- Format as a Rich table using `rich.table.Table` with columns: ID, STATE, ELAPSED, NODE.
  - STATE: `run.state.value`
  - ELAPSED: Calculate from `run.started_ns`. If `run.ended_ns > 0`, elapsed = `(run.ended_ns - run.started_ns) / 1e9`. Otherwise elapsed = `(time.perf_counter_ns() - run.started_ns) / 1e9`. Format as `f"{elapsed:.1f}s"`.
  - NODE: `run.current_node or "-"`
- Render via `_rich_to_ansi(table)` from `bae.repl.views` and write through the graph channel.

**Command: `cancel <id>`** (MODE-04)

```python
async def _cmd_cancel(arg: str, shell) -> None:
```

- Strip arg. If empty, write `"usage: cancel <id>"`.
- `run = shell.engine.get(arg)`. If not found, write `f"no run {arg}"`.
- If `run.state != GraphState.RUNNING`, write `f"{arg} not running ({run.state.value})"`.
- Find the TaskManager task: iterate `shell.tm.active()`, find `tt.name.startswith(f"graph:{run.run_id}:")`.
- Call `shell.tm.revoke(tt.task_id)`.
- Write `f"cancelled {run.run_id}"`.

**Command: `inspect <id>`** (MODE-05)

```python
async def _cmd_inspect(arg: str, shell) -> None:
```

- Strip arg. If empty, write `"usage: inspect <id>"`.
- `run = shell.engine.get(arg)`. If not found, write `f"no run {arg}"`.
- Build Rich output:
  - Header: `f"Run {run.run_id} ({run.state.value}, {elapsed})"` where elapsed is computed as in list.
  - If `run.graph`: `f"Graph: {run.graph.start.__name__}"`.
  - Node timings section: For each `nt` in `run.node_timings`, show `f"  {nt.node_type}  {nt.duration_ms:.0f}ms"`.
  - Trace section: If `run.result` and `run.result.trace`, show each node with its field values. For the terminal node (last in trace), show all fields via `node.model_dump()`. For intermediate nodes, show just the class name.
  - Use `rich.text.Text` for styled output, render via `_rich_to_ansi`.

**Command: `trace <id>`** (MODE-05 -- trace is the compact view of transitions)

```python
async def _cmd_trace(arg: str, shell) -> None:
```

- Strip arg. If empty, write `"usage: trace <id>"`.
- `run = shell.engine.get(arg)`. If not found, write `f"no run {arg}"`.
- If `run.result` and `run.result.trace`:
  - For each `(i, node)` in enumerate(trace, 1), show `f"  {i}. {type(node).__name__}"`.
  - If node timings available, append timing next to matching node names.
- If no trace available (running or failed before any nodes), write `f"{arg}: no trace available"`.
- Use plain text (no Rich tables needed -- this is the compact view).

**Update `bae/repl/shell.py`:**

1. Add import at top: `from bae.repl.graph_commands import dispatch_graph`
2. In `_dispatch()`, replace the GRAPH mode handler:
   ```python
   # Old:
   elif self.mode == Mode.GRAPH:
       await self._run_graph(text)
   # New:
   elif self.mode == Mode.GRAPH:
       await dispatch_graph(text, self)
   ```
3. Delete the `_run_graph` method entirely -- it is fully replaced by `dispatch_graph` -> `_cmd_run`.
  </action>
  <verify>
`uv run python -c "from bae.repl.graph_commands import dispatch_graph; print('ok')"` -- imports cleanly.
`uv run pytest tests/ -x -q --ignore=tests/test_integration.py` -- no regressions from shell.py changes.
  </verify>
  <done>
GRAPH mode has 5 commands: run/list/cancel/inspect/trace. Shell routes to dispatcher. Old _run_graph deleted. All output goes through [graph] channel.
  </done>
</task>

<task type="auto">
  <name>Task 2: Tests for GRAPH mode commands</name>
  <files>tests/repl/test_graph_commands.py</files>
  <action>
**Create `tests/repl/test_graph_commands.py`:**

Use a `FakeShell` fixture that provides the minimal interface the command handlers need:
- `shell.engine` = `GraphRegistry()`
- `shell.tm` = `TaskManager()`
- `shell.namespace` = a dict with test objects
- `shell._lm` = MockLM
- `shell.router` = a FakeRouter that captures `write()` calls into a list for assertion

```python
class FakeRouter:
    def __init__(self):
        self.writes: list[tuple[str, str, dict]] = []

    def write(self, channel, content, *, mode=None, metadata=None):
        self.writes.append((channel, content, metadata or {}))
```

```python
@dataclass
class FakeShell:
    engine: GraphRegistry
    tm: TaskManager
    namespace: dict
    _lm: MockLM
    router: FakeRouter
```

Define module-level test nodes (same pattern as test_engine.py to avoid forward ref issues):

```python
class TStart(Node):
    message: str
    def __call__(self) -> TEnd: ...

class TEnd(Node):
    reply: str
```

**Test class: TestDispatch**
- `test_unknown_command`: dispatch `"foo"` -> router writes contain "unknown command"
- `test_empty_input`: dispatch `""` -> no writes (or help)

**Test class: TestCmdRun**
- `test_run_coroutine_submits`: Put `graph(start=TStart)` in namespace as `"mygraph"`. dispatch `"run mygraph(message='hi')"`. Verify router writes contain "submitted g1". Await the task, verify router writes contain "g1 done" or "g1 failed" (depends on MockLM -- configure MockLM to make it succeed).
- `test_run_graph_object_submits`: Put a `Graph(start=TStart)` in namespace as `"g"`. dispatch `"run g"`. Verify "submitted g1".
- `test_run_no_expr_shows_usage`: dispatch `"run"` or `"run "`. Verify router writes contain "usage".
- `test_run_bad_expr_shows_error`: dispatch `"run nonexistent_var"`. Verify router writes contain error traceback.

**Test class: TestCmdList**
- `test_list_empty`: dispatch `"list"` with no runs. Verify "(no graph runs)" in output.
- `test_list_shows_runs`: Submit a graph via engine, dispatch `"list"`. Verify output contains run_id and state.
- `test_ls_alias`: dispatch `"ls"` works same as `"list"`.

**Test class: TestCmdCancel**
- `test_cancel_running`: Submit a slow graph (SlowLM), dispatch `"cancel g1"`. Verify "cancelled g1" in output.
- `test_cancel_nonexistent`: dispatch `"cancel g99"`. Verify "no run g99".
- `test_cancel_no_arg`: dispatch `"cancel"`. Verify "usage".

**Test class: TestCmdInspect**
- `test_inspect_completed_run`: Submit a graph with MockLM, await completion, dispatch `"inspect g1"`. Verify output contains run_id, state "done", and node type names.
- `test_inspect_nonexistent`: dispatch `"inspect g99"`. Verify "no run g99".

**Test class: TestCmdTrace**
- `test_trace_completed_run`: Submit graph, await completion, dispatch `"trace g1"`. Verify output contains numbered node names (e.g., "1. TStart").
- `test_trace_nonexistent`: dispatch `"trace g99"`. Verify "no run g99".

**Important test patterns:**
- After submitting via engine, await tasks to completion before testing inspect/trace:
  ```python
  for tt in list(shell.tm._tasks.values()):
      try: await tt.task
      except Exception: pass
  ```
- Use `MockLM` from existing test_engine.py pattern (or define locally -- same shape).
- Use `asyncio.sleep(0.05)` before cancel tests to let the task start.
- For run command tests with coroutines, need `async_exec` to work with the namespace. The namespace needs the graph factory callable and any types it references. Pre-populate: `shell.namespace.update({"mygraph": graph(start=TStart), "Graph": Graph, "TStart": TStart})`.
  </action>
  <verify>
`uv run pytest tests/repl/test_graph_commands.py -x -q` -- all tests pass.
`uv run pytest tests/ -x -q --ignore=tests/test_integration.py` -- full suite passes, no regressions.
  </verify>
  <done>
All 5 GRAPH mode commands tested: run (coroutine + Graph object + error cases), list (empty + with runs + alias), cancel (running + nonexistent), inspect (completed + nonexistent), trace (completed + nonexistent). Full test coverage for dispatcher routing.
  </done>
</task>

</tasks>

<verification>
1. `uv run pytest tests/repl/test_graph_commands.py -x -q` -- command tests pass
2. `uv run pytest tests/ -x -q --ignore=tests/test_integration.py` -- full suite, no regressions
3. Manual verification: GRAPH mode commands parse correctly, `run` submits graphs, `list` shows runs, `cancel` stops them, `inspect` shows details, `trace` shows transitions
</verification>

<success_criteria>
- MODE-01: GRAPH mode parses all 5 commands (run, list, cancel, inspect, trace)
- MODE-02: run <expr> evaluates namespace expression and submits to engine
- MODE-03: list shows all runs with state, timing, current node
- MODE-04: cancel <id> revokes via TaskManager
- MODE-05: inspect <id> shows full trace with timings and field values
- trace <id> shows compact node transition history
- Unknown commands show help text
- Old _run_graph deleted from shell.py
</success_criteria>

<output>
After completion, create `.planning/phases/27-graph-mode/27-02-SUMMARY.md`
</output>

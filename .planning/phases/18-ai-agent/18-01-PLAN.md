---
phase: 18-ai-agent
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - bae/repl/ai.py
  - tests/repl/test_ai.py
autonomous: true

must_haves:
  truths:
    - "AI class is an async callable with __call__, fill, choose_type, and extract_code"
    - "Context builder summarizes namespace (graph topology, trace, user variables) within 2000 chars"
    - "Code extractor finds python/py fenced code blocks from markdown text"
    - "Agent construction is lazy -- deferred until first __call__ invocation"
    - "Message history is maintained across calls, capped at 20 messages"
  artifacts:
    - path: "bae/repl/ai.py"
      provides: "AI callable class, _build_context, _system_prompt, extract_code"
      exports: ["AI"]
    - path: "tests/repl/test_ai.py"
      provides: "Unit tests for context builder, code extractor, AI repr, lazy init"
      min_lines: 80
  key_links:
    - from: "bae/repl/ai.py"
      to: "pydantic_ai.Agent"
      via: "lazy _ensure_agent() construction"
      pattern: "Agent\\("
    - from: "bae/repl/ai.py"
      to: "bae.lm.LM"
      via: "self._lm.fill and self._lm.choose_type delegation"
      pattern: "self\\._lm\\.(fill|choose_type)"
    - from: "bae/repl/ai.py"
      to: "bae.repl.channels.ChannelRouter"
      via: "self._router.write('ai', ...) for output routing"
      pattern: "router\\.write\\(\"ai\""
---

<objective>
Create the AI callable class that serves as the NL conversation agent in cortex.

Purpose: AI-01 through AI-07 all depend on this core artifact. The AI class wraps pydantic-ai Agent for NL conversation, delegates fill/choose_type to the existing bae LM protocol, builds namespace context for each prompt, extracts code blocks from responses, and manages conversation history.

Output: `bae/repl/ai.py` with the AI class and all supporting functions, plus unit tests for the pure functions (context builder, code extractor).
</objective>

<execution_context>
@/Users/dzaramelcone/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dzaramelcone/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/18-ai-agent/18-RESEARCH.md
@bae/repl/namespace.py
@bae/repl/channels.py
@bae/lm.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create AI callable class</name>
  <files>bae/repl/ai.py</files>
  <action>
Create `bae/repl/ai.py` with the `AI` class following the research patterns (18-RESEARCH.md "Complete AI Class" section). Key implementation details:

1. **Module structure:** `from __future__ import annotations`, stdlib imports (re, inspect), pydantic_ai.Agent import, TYPE_CHECKING guard for LM/Node/ChannelRouter.

2. **Constants:** `_CODE_BLOCK_RE` regex for triple-backtick python/py code fences. `MAX_CONTEXT_CHARS = 2000`. `MAX_HISTORY_MESSAGES = 20`.

3. **AI class:**
   - `__init__(self, *, lm: LM, router: ChannelRouter, namespace: dict, model: str = "anthropic:claude-opus-4-6")` -- stores references, initializes empty `_history: list = []`, sets `_agent: Agent | None = None` and `_model = model`. No Agent construction here (lazy init per Pitfall 1).
   - `_ensure_agent(self) -> Agent` -- lazy init: if `_agent` is None, construct `Agent(self._model, system_prompt=_system_prompt())` and cache it. Return the agent.
   - `async __call__(self, prompt: str) -> str` -- calls `_ensure_agent()`, builds context via `_build_context(self._namespace)`, prepends to prompt if non-empty, calls `agent.run(full_prompt, message_history=self._history[-MAX_HISTORY_MESSAGES:] or None)`, stores `result.all_messages()` in `self._history`, writes `result.output` to `self._router.write("ai", response, mode="NL", metadata={"type": "response"})`, returns response string.
   - `async fill(self, target: type[Node], context: dict | None = None) -> Node` -- delegates `await self._lm.fill(target, context or {}, target.__name__)`.
   - `async choose_type(self, types: list[type[Node]], context: dict | None = None) -> type[Node]` -- delegates `await self._lm.choose_type(types, context or {})`.
   - `extract_code(text: str) -> list[str]` -- static method, returns `_CODE_BLOCK_RE.findall(text)`.
   - `__repr__(self) -> str` -- returns `f"ai -- await ai('question'). {len(self._history)} messages in history."`.

4. **`_system_prompt() -> str`** -- module-level function returning the static system prompt string from research (Pattern 3). Instructs AI about cortex role, bae API, code formatting rules, namespace awareness.

5. **`_build_context(namespace: dict) -> str`** -- module-level function from research (Pattern 2 / "Context Builder" section). Priority order: graph topology first, then trace (last 5), then user variables. Skips internals (`__builtins__`, `_`-prefixed), modules, and known bae names (Node, Graph, Dep, Recall, etc.). Truncates to `MAX_CONTEXT_CHARS`.

6. **Anti-patterns to avoid:**
   - Do NOT put dynamic context in the system prompt (it goes in the user message).
   - Do NOT auto-execute extracted code blocks.
   - Do NOT import Graph/Node at module level -- use TYPE_CHECKING guard and runtime imports inside `_build_context` to avoid circular imports.
  </action>
  <verify>
`python -c "from bae.repl.ai import AI; print('AI class imported')"` succeeds. Verify AI has __call__, fill, choose_type, extract_code methods. Verify _build_context and _system_prompt are module-level functions.
  </verify>
  <done>
bae/repl/ai.py exists with AI class implementing all seven requirements (AI-01 through AI-07). AI is importable with no errors. Agent construction is deferred (no ANTHROPIC_API_KEY needed at import time).
  </done>
</task>

<task type="auto">
  <name>Task 2: Unit tests for pure functions</name>
  <files>tests/repl/test_ai.py</files>
  <action>
Create `tests/repl/test_ai.py` with unit tests covering the pure/testable parts of the AI class. Do NOT test __call__ (requires API key and network). Use the project's testing patterns: test classes, docstrings on every test, direct assertions.

Test classes and cases:

1. **TestExtractCode:**
   - `test_single_python_block` -- input with one ```python block, returns list of 1.
   - `test_multiple_blocks` -- input with two code blocks, returns list of 2.
   - `test_bare_backticks` -- input with bare ``` (no language), returns the code.
   - `test_py_shorthand` -- input with ```py, returns the code.
   - `test_no_code_blocks` -- plain text, returns empty list.
   - `test_nested_text` -- code block containing backticks in a string (edge case).

2. **TestBuildContext:**
   - `test_empty_namespace` -- namespace with only `__builtins__`, returns empty string.
   - `test_user_variables` -- namespace with `x = 42`, `name = "hello"`, returns "Variables:" section with those entries.
   - `test_graph_topology` -- namespace with a real Graph object (use bae.graph.Graph with simple test nodes), returns "Graph:" section with edges.
   - `test_trace_summary` -- namespace with `_trace` list of Node instances (last 5), returns "Trace:" section.
   - `test_skips_internals` -- namespace with `ai`, `ns`, `store`, `Node`, `__builtins__` -- none appear in output.
   - `test_truncation` -- namespace with many large variables exceeding 2000 chars, output ends with "... (truncated)".

3. **TestAIRepr:**
   - `test_repr_no_history` -- AI with empty history shows "0 messages".
   - `test_repr_with_history` -- AI with `_history` set to a list of 5 items shows "5 messages".

4. **TestAILazyInit:**
   - `test_agent_none_at_construction` -- after `AI(lm=mock_lm, router=mock_router, namespace={})`, verify `ai._agent is None`.

For mock objects: create minimal stubs for LM (object with fill/choose_type), ChannelRouter (object with write method), and simple Node subclasses for graph tests.

Import `_build_context` directly from `bae.repl.ai` since it's module-level.
  </action>
  <verify>
`pytest tests/repl/test_ai.py -v` -- all tests pass. No warnings, no unexpected output.
  </verify>
  <done>
All unit tests pass covering extract_code (6 cases), _build_context (6 cases), AI repr (2 cases), and lazy init (1 case). At least 15 test cases total.
  </done>
</task>

</tasks>

<verification>
- `python -c "from bae.repl.ai import AI"` imports without error
- `pytest tests/repl/test_ai.py -v` all pass
- `pytest tests/repl/ -v` no regressions in existing repl tests
</verification>

<success_criteria>
- AI class exists in bae/repl/ai.py with async __call__, fill, choose_type, extract_code
- Agent construction is lazy (no API key needed at import/init)
- Context builder produces truncated namespace summaries
- Code extractor handles python/py/bare fenced blocks
- 15+ unit tests pass covering pure functions
- Zero regressions in existing repl tests
</success_criteria>

<output>
After completion, create `.planning/phases/18-ai-agent/18-01-SUMMARY.md`
</output>

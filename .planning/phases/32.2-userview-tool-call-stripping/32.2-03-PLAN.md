---
phase: 32.2-userview-tool-call-stripping
plan: 03
type: execute
wave: 2
depends_on: ["32.2-01"]
files_modified:
  - bae/repl/tools.py
  - tests/test_tools_router.py
autonomous: true

must_haves:
  truths:
    - "Tool parameters are validated via pydantic before execution"
    - "Validation errors return clear messages guiding the AI to correct its call"
    - "Valid calls proceed to execution normally"
    - "Home dispatch (no current resource) still works without validation"
  artifacts:
    - path: "bae/repl/tools.py"
      provides: "Pydantic parameter validation on tool dispatch"
      contains: "_validate_tool_params"
  key_links:
    - from: "bae/repl/tools.py"
      to: "bae/repl/spaces/source/service.py"
      via: "dispatch calls method with validated params"
      pattern: "validate.*param|ValidationError"
---

<objective>
Add pydantic parameter validation to ToolRouter dispatch before calling resource methods.

Purpose: When the AI sends malformed tool call parameters, return clear error messages guiding it to correct the call, rather than cryptic Python TypeErrors.

Output: Validation layer in `ToolRouter.dispatch()` using pydantic `TypeAdapter` or `create_model`.
</objective>

<execution_context>
@/Users/dzaramelcone/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dzaramelcone/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/32.2-userview-tool-call-stripping/32.2-RESEARCH.md
@.planning/phases/32.2-userview-tool-call-stripping/32.2-01-SUMMARY.md
@bae/repl/tools.py
@bae/repl/spaces/view.py
@tests/test_tools_router.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add pydantic validation to ToolRouter.dispatch</name>
  <files>bae/repl/tools.py, tests/test_tools_router.py</files>
  <action>
Add `import inspect` and `from pydantic import TypeAdapter, ValidationError` at the top of `bae/repl/tools.py`.

**1. Add `_build_validator` function.**
Build a pydantic validator from a method's signature. Cache validators per method to avoid repeated introspection.

```python
_validator_cache: dict[int, tuple[list[str], TypeAdapter | None]] = {}

def _build_validator(method) -> tuple[list[str], TypeAdapter | None]:
    """Build parameter names and a TypeAdapter for validation. Returns (param_names, adapter)."""
    mid = id(method)
    if mid in _validator_cache:
        return _validator_cache[mid]

    try:
        sig = inspect.signature(method)
    except (ValueError, TypeError):
        _validator_cache[mid] = ([], None)
        return [], None

    params = []
    for pname, param in sig.parameters.items():
        if pname == "self":
            continue
        params.append(pname)

    # For single-string-param tools (read, glob), no validation needed — just pass through
    # For multi-param tools, build a simple type check on kwargs
    _validator_cache[mid] = (params, None)
    return params, None
```

Actually, the simpler approach per the research: since most tool parameters come as strings from the AI's XML tags (which is text), the primary validation is: (a) required params are present, (b) types are compatible. The AI always sends strings. The real value is in catching missing required parameters and providing the method signature in error messages.

**2. Add `_format_validation_error` function.**
When a tool call fails due to wrong parameters, format a helpful error:

```python
def _format_validation_error(tool: str, method, error: str) -> str:
    """Format a helpful error message for bad tool parameters."""
    try:
        sig = inspect.signature(method)
        params = []
        for pname, param in sig.parameters.items():
            if pname == "self":
                continue
            ann = param.annotation
            if ann is not inspect.Parameter.empty:
                tname = ann.__name__ if isinstance(ann, type) else str(ann)
                params.append(f"{pname}: {tname}")
            else:
                params.append(pname)
        sig_str = f"{tool}({', '.join(params)})"
    except Exception:
        sig_str = f"{tool}(...)"

    doc = (getattr(method, "__doc__", None) or "").split("\n")[0].strip()
    lines = [f"Tool '{tool}' parameter error: {error}"]
    lines.append(f"Usage: {sig_str}")
    if doc:
        lines.append(f"  {doc}")
    return "\n".join(lines)
```

**3. Update `ToolRouter.dispatch()` to validate and provide helpful errors.**
Wrap the method call in a try/except that catches `TypeError` (wrong number of args, wrong types) and formats it helpfully:

```python
def dispatch(self, tool: str, arg: str, **kwargs) -> str:
    current = self._registry.current
    if current is None:
        return self._home_dispatch(tool, arg, **kwargs)

    if tool not in current.supported_tools():
        return format_unsupported_error(current, tool)

    try:
        method = getattr(current, tool)
        result = method(arg, **kwargs)
    except ResourceError as e:
        return str(e)
    except TypeError as e:
        # Parameter mismatch — format helpful error
        method = getattr(current, tool)
        return _format_validation_error(tool, method, str(e))

    return _prune(result)
```

This is the minimal, YAGNI approach: catch Python's own TypeError on bad calls (which already gives messages like "read() got an unexpected keyword argument 'foo'" or "read() missing 1 required positional argument: 'target'") and wrap it in a formatted message with the correct signature and docstring. No upfront pydantic model building needed — Python's own parameter binding already validates effectively, and we add the helpful formatting on error.

**4. Update tests in `tests/test_tools_router.py`.**
Add tests:
- Test that calling a tool with wrong kwargs returns a formatted error with signature hint
- Test that calling a tool with correct params still works normally
- Test that the error message includes the tool's docstring
- Test that home dispatch is unaffected (no validation layer there)
  </action>
  <verify>`uv run pytest tests/test_tools_router.py -x -q`</verify>
  <done>ToolRouter.dispatch() catches TypeError on bad parameters and returns helpful error with method signature and docstring</done>
</task>

</tasks>

<verification>
```bash
uv run pytest tests/test_tools_router.py -x -q
uv run pytest tests/ -x -q --ignore=tests/test_integration.py
```
</verification>

<success_criteria>
- Bad tool parameters return clear error with signature hint and docstring
- Correct tool calls work exactly as before
- ResourceError still caught and returned as-is
- Home dispatch unaffected
- All tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/32.2-userview-tool-call-stripping/32.2-03-SUMMARY.md`
</output>

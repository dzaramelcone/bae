---
phase: 32.2-userview-tool-call-stripping
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - bae/repl/ai.py
  - bae/repl/views.py
  - tests/repl/test_ai.py
  - tests/repl/test_views.py
autonomous: true

must_haves:
  truths:
    - "Tool calls display as diamond-bullet one-liners with call signature and return type"
    - "Error tool calls render with ResourceError or exception type name as return type"
    - "Intermediate AI prose between tool calls is hidden in UserView"
    - "Only the final AI response after all tools resolve renders as [ai] text"
    - "DebugView and AISelfView continue showing everything unchanged"
    - "run block panels stay as-is"
  artifacts:
    - path: "bae/repl/ai.py"
      provides: "New _tool_summary format and intermediate response suppression"
      contains: "_tool_summary"
    - path: "bae/repl/views.py"
      provides: "UserView tool summary rendering with color and response filtering"
      contains: "tool_translated"
  key_links:
    - from: "bae/repl/ai.py"
      to: "bae/repl/views.py"
      via: "tool_summary and is_error metadata on tool_translated writes"
      pattern: "tool_summary|is_error"
---

<objective>
Reformat tool call display and suppress intermediate AI responses in UserView.

Purpose: Replace verbose tool output display with clean `◆ read(args) -> str` one-liners, hide AI orchestration prose ("Let me check that file"), and only show the final AI response.

Output: Updated `_tool_summary` format, `run_tool_calls` returning error status, UserView rendering with color-coded tool lines and response filtering.
</objective>

<execution_context>
@/Users/dzaramelcone/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dzaramelcone/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/32.2-userview-tool-call-stripping/32.2-RESEARCH.md
@bae/repl/ai.py
@bae/repl/views.py
@tests/repl/test_ai.py
@tests/repl/test_views.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Reformat tool summaries and suppress intermediate responses in AI backend</name>
  <files>bae/repl/ai.py, tests/repl/test_ai.py</files>
  <action>
Three changes in `bae/repl/ai.py`:

**1. New `_tool_summary` format with resource context prefix (DSC-03).**
Replace `_tool_summary()` to produce: `[resource] ◆ name(args) -> return_type (count_hint)`

The resource context prefix is the current resource name (e.g., `[source]`). Pass it as a parameter from the AI backend, which has access to `self._registry.current.name`. When no resource is active (home), omit the prefix.

- Map tool types to human names: `{"R": "read", "W": "write", "E": "edit", "G": "glob", "Grep": "grep"}`
- For success: return type is `str`, add count hint where applicable (read: line count, glob/grep: match count, write/edit: pass output as-is)
- For errors: return type is the exception class name (e.g., `ResourceError`, `FileNotFoundError`)
- Error detection: check if output matches pattern `SomePascalCaseWord: rest` at the start — this is how `run_tool_calls` and `ToolRouter.dispatch` format caught exceptions

Add a helper `_is_error_output(output: str) -> bool` that checks if the output starts with a PascalCase word followed by `: ` (matching patterns like `ResourceError: message` or `FileNotFoundError: path`). Use `re.match(r"^[A-Z][a-zA-Z]*Error:", output)` as the heuristic.

Add a helper `_error_type_name(output: str) -> str` that extracts the error type name before the first `:`.

Signature: `_tool_summary(tag: str, output: str, *, is_error: bool = False, resource: str = "") -> str`

The `resource` param is the current resource name. When non-empty, prepend `[{resource}] ` before the `◆`.

Example outputs:
- `[source] ◆ read(bae.repl.ai) -> str (42 lines)`
- `[source] ◆ glob(bae.repl.*) -> str (5 matches)`
- `[source] ◆ write(bae.repl.new_mod) -> str`
- `[source] ◆ read(nonexistent) -> ResourceError`
- `◆ read(nonexistent) -> ResourceError` (no resource context when at home)

**2. Pass error flag from `run_tool_calls`.**
Change the return type from `list[tuple[str, str]]` to `list[tuple[str, str, bool]]` where the third element is `is_error`. In the exception handler at line 432, set `is_error=True`. For successful calls, set `is_error=False`. Update the call in `_tool_summary` to accept an `is_error` parameter so it can use it instead of string-heuristic detection.

**3. Suppress intermediate `response` writes in the eval loop.**
In `AI.__call__`:
- Remove the `router.write("ai", response, ...)` call at line 126 (the initial response write before the loop).
- Remove the `router.write("ai", response, ...)` calls at lines 147-148 (inside the tool call branch) and line 196 (inside the run block branch).
- After the while loop exits (line 198), add a single final response write:
  ```python
  self._router.write("ai", response, mode="NL",
      metadata={"type": "response", "label": self._label})
  ```
  This ensures only the last AI response (after all tool calls and run blocks resolve) renders.

Update `AI.__call__` tool call processing to unpack the new 3-tuple from `run_tool_calls` and pass resource context:
```python
resource_name = ""
if self._registry and self._registry.current:
    resource_name = self._registry.current.name

for tag, output, is_error in tool_results:
    summary = _tool_summary(tag, output, is_error=is_error, resource=resource_name)
    self._router.write("py", tag, mode="PY",
        metadata={"type": "tool_translated", "label": self._label,
                  "tool_summary": summary, "is_error": is_error})
```

Update tests in `tests/repl/test_ai.py`:
- Update `TestRunToolCalls` assertions to expect 3-tuples `(tag, output, is_error)`.
- Update `TestEvalLoopToolCalls` assertions for the changed router.write call patterns: no intermediate response writes, only tool_translated writes during the loop, and one final response write after.
- Add tests for `_tool_summary` new format: test read with lines, glob with matches, error output, write pass-through.
- Add tests for resource context prefix: `_tool_summary(..., resource="source")` produces `[source] ◆ ...`, `_tool_summary(..., resource="")` produces `◆ ...` (no prefix).
  </action>
  <verify>`uv run pytest tests/repl/test_ai.py -x -q`</verify>
  <done>_tool_summary returns `◆ name(args) -> type (hint)` format; run_tool_calls returns is_error flag; eval loop writes only one response at the end</done>
</task>

<task type="auto">
  <name>Task 2: Update UserView tool rendering with color and response filtering</name>
  <files>bae/repl/views.py, tests/repl/test_views.py</files>
  <action>
Two changes in `bae/repl/views.py`:

**1. Update `tool_translated` rendering in `UserView.render()`.**
Replace the current `tool_translated` block (lines 161-166) with color-coded rendering:

```python
if content_type == "tool_translated":
    summary = meta.get("tool_summary", content)
    is_error = meta.get("is_error", False)
    if is_error:
        # Red for error tool calls
        styled = f"\033[31m{summary}\033[0m"
    else:
        # Dim for normal tool calls (matches current italic dim style)
        styled = f"\033[3;38;5;244m{summary}\033[0m"
    print_formatted_text(ANSI(styled))
    return
```

Drop the channel prefix `[py]` — the `◆` bullet is the visual indicator. Drop the `linkify_paths` call since the new format uses module paths (dotted notation), not file paths.

**2. No changes needed for response filtering.**
The AI backend now only writes the final response. UserView's `response` handler at line 155 already strips executable content and renders with prefix — this continues working as-is. No `final` flag needed since intermediate responses are simply not written anymore.

Update tests in `tests/repl/test_views.py`:
- Update `test_tool_summary_read`, `test_tool_summary_glob`, `test_tool_summary_write` to match the new `◆` format and the absence of `[py]` prefix.
- Add test for error tool call rendering (red ANSI).
- Add test that `response` type renders normally (no change in behavior, but confirms the flow).
  </action>
  <verify>`uv run pytest tests/repl/test_views.py -x -q`</verify>
  <done>UserView renders tool summaries with `◆` bullet, dim style for success, red for errors; no `[py]` prefix on tool lines</done>
</task>

</tasks>

<verification>
```bash
uv run pytest tests/repl/test_ai.py tests/repl/test_views.py -x -q
uv run pytest tests/ -x -q --ignore=tests/test_integration.py
```
</verification>

<success_criteria>
- `_tool_summary` produces `[resource] ◆ name(args) -> type (hint)` format (with resource context prefix per DSC-03)
- `run_tool_calls` returns `list[tuple[str, str, bool]]` with error flag
- AI eval loop writes only one `response` per `__call__` invocation (the final one)
- UserView renders tool calls as color-coded one-liners without channel prefix
- Error tool calls render in red
- DebugView and AISelfView unaffected
- All existing tests pass with updated assertions
</success_criteria>

<output>
After completion, create `.planning/phases/32.2-userview-tool-call-stripping/32.2-01-SUMMARY.md`
</output>

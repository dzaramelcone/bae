# Phase 32.2: UserView Tool Call Stripping - Research

**Researched:** 2026-02-16
**Domain:** REPL display layer, AI tool call rendering, parameter validation
**Confidence:** HIGH

<user_constraints>
## User Constraints (from CONTEXT.md)

### Locked Decisions

#### Display stripping
- Tool calls render as: `◆ read(my_args) -> str` -- one line per call with bullet, call signature, arrow, return type
- Error tool calls render in red: `◆ read(my_args) -> ResourceError`
- Intermediate AI prose (between tool calls) is completely hidden in UserView
- Only the AI's final response (after all tools resolve) renders as `[ai]` prefixed text
- `<run>` block panels (code+output) stay as-is -- they're the interesting part
- Render each tool result as it completes -- sequential execution, immediate display per completion
- View cycling (Ctrl+V) handles "expand" -- no new toggle mechanism needed

#### Tool tag format on resource entry
- Functions table keeps current layout, but Tool column shows XML tag syntax with full typed signature
- Example: `<Read:path:str>` or `<Edit:path:str, old:str, new:str>`
- Inline params with type hints -- all parameters shown, not just primary
- Docstrings pulled directly from the callable's docstring -- no modification
- Body content for multi-line tools (Write, Edit) keeps current `<W:path>content</W>` format

#### Tool parameter validation
- Pydantic validation on tool call parameters with helpful error messages
- Similar to FastAPI's Query()/Form() pattern -- investigate if Field() works for method params
- Validation errors return clear messages guiding the AI to correct its call

#### AI response filtering
- Intermediate AI responses (orchestration prose between tool calls) completely hidden in UserView
- Only tool summary lines + final AI response shown
- DebugView shows full tracebacks for errors; UserView shows type only
- Current `[ai]` prefix rendering for final responses stays unchanged

#### Context history
- Keep `--resume` session management as-is -- no own history management this phase
- Display-side changes only -- what the LM sees is unchanged
- Context % reporting on toolbar deferred (depends on CLI payload inspection)

### Claude's Discretion
- Exact bullet styling and colors for tool summary lines (beyond red for errors)
- How to handle the async yield of tool results without adding streaming complexity
- Pydantic model structure for tool parameter validation

### Deferred Ideas (OUT OF SCOPE)
- Own conversation history management (replacing --resume) -- big project, defer to future phase
- Context % of max reporting on toolbar -- needs CLI payload inspection
- Docstring length linter rule -- separate cleanup concern
</user_constraints>

## Summary

This phase modifies the display layer between the AI backend (`AI` class in `ai.py`) and the view formatters (`views.py`) to strip tool call noise from UserView. The current architecture already separates tool execution from display -- `AI.__call__` detects tool tags via `run_tool_calls()`, writes `tool_translated` metadata to the router, and writes `response` metadata for AI text. The changes are almost entirely in two files: `ai.py` (formatting tool summary lines with the `◆ call(args) -> type` format, suppressing intermediate responses) and `views.py` (rendering the new format, hiding intermediate prose).

The functions table on resource entry currently lives in `ResourceRegistry._entry_display()` in `spaces/view.py`. It renders a generic `| tool | tool() | tool result |` table. This needs to pull actual method signatures and show XML tag syntax with typed parameters.

Tool parameter validation using pydantic is straightforward -- the project already uses `pydantic.TypeAdapter` for gate input validation in `shell.py`. The same pattern applies: wrap tool parameters in a pydantic model, validate on dispatch, return helpful error messages.

**Primary recommendation:** Work in three layers -- (1) AI backend formats `◆` summary lines and tracks intermediate vs. final responses, (2) UserView renders summaries and suppresses intermediate prose, (3) entry display shows typed tool signatures. Validation is a cross-cutting concern in ToolRouter dispatch.

## Standard Stack

### Core
| Library | Version | Purpose | Why Standard |
|---------|---------|---------|--------------|
| pydantic | 2.x (already in project) | Tool parameter validation | Already used for Node/BaseModel/TypeAdapter throughout |
| Rich | (already in project) | ANSI text formatting for tool summaries | Already used for panels, syntax, rules in views.py |
| prompt_toolkit | (already in project) | Terminal output via print_formatted_text | Already the display layer |

### Supporting
| Library | Version | Purpose | When to Use |
|---------|---------|---------|-------------|
| inspect | stdlib | Extract method signatures for typed tool tags | Building the functions table |
| typing.get_type_hints | stdlib | Resolve type annotations on tool methods | Getting param types for display |

No new dependencies needed. Everything required is already in the project.

## Architecture Patterns

### Current Data Flow (Tool Calls)

```
AI.__call__()
  -> _send() to Claude CLI subprocess
  <- response text
  -> run_tool_calls(response, router=self._tool_router)
     -> returns list[(tag, output)]
  -> for each (tag, output):
       router.write("py", tag, metadata={"type": "tool_translated", "tool_summary": summary})
  -> combine outputs, feed back to Claude CLI
  -> router.write("ai", response, metadata={"type": "response"})
  -> loop until no tool calls
```

### Key Finding: Where Backend Formats, Where View Renders

Per Dzara's design note, the AI backend knows about tool calls and should format the summary lines. Currently:

- `_tool_summary()` in `ai.py` (line 317) already generates summaries like `"read bae/repl/ai.py (3 lines)"`
- `UserView.render()` with `tool_translated` type (line 161) renders these as `[py] summary` in italic dim

The change: `_tool_summary()` should produce the full `◆ read(my_args) -> str` format. UserView renders it with the bullet styling. Errors get red coloring based on whether the output contains a ResourceError or exception type.

### Pattern 1: Intermediate vs. Final Response Tracking

**What:** The AI makes multiple round-trips during a tool call loop. All intermediate responses contain tool tags and orchestration prose. Only the final response (no tool calls remaining) is "the answer."

**Current behavior:** Every `response` type write goes to the `[ai]` channel and gets displayed (with tool tags stripped). Intermediate prose like "Let me read that file" shows up.

**Required behavior:** Only the final response renders. Intermediate responses are suppressed in UserView.

**Implementation approach:** Track response index in the eval loop. The `AI.__call__` loop already knows when it breaks (no tool calls, no run blocks). Mark the final response with metadata `{"type": "response", "final": True}`. UserView checks this flag.

```python
# In AI.__call__, after the loop breaks:
self._router.write("ai", response, mode="NL",
    metadata={"type": "response", "label": self._label, "final": True})
```

All intermediate responses get `{"type": "response", "final": False}` (or omit the key). UserView skips rendering when `final` is not True for `response` type.

**DebugView:** Shows everything regardless (already does -- no changes needed).

### Pattern 2: Tool Summary Format

**Current:** `_tool_summary("<R:bae/repl/ai.py>", output)` -> `"read bae/repl/ai.py (3 lines)"`

**New format:** `"◆ read(bae/repl/ai.py) -> str"` for success, `"◆ read(bae/repl/ai.py) -> ResourceError"` for errors.

The return type determination:
- Success: infer from tool type (`read` -> `str`, `glob` -> `str`, `write` -> `str`, `edit` -> `str`)
- Error: parse the output for exception type name (`ResourceError: ...` -> `ResourceError`, `FileNotFoundError: ...` -> `FileNotFoundError`)

Include a count hint in parens: `"◆ read(bae/repl/ai.py) -> str (42 lines)"` -- keeps the useful info from the current summary.

### Pattern 3: Resource Entry Functions Table with Typed Signatures

**Current rendering** (in `ResourceRegistry._entry_display`, line 244 of `spaces/view.py`):

```
| Tool | Function | Returns |
|------|----------|---------|
| read | read() | read result |
| write | write() | write result |
```

**New rendering:**

```
| Tool | Signature | Description |
|------|-----------|-------------|
| read | <Read:target:str> | Read package listing, module summary, or symbol source |
| write | <Write:target:str>content</Write> | Create a new module or subresource package |
| edit | <Edit:target:str, new_source:str> | Replace a symbol's source by name |
| glob | <Glob:pattern:str> | Match modules by dotted glob pattern |
| grep | <Grep:pattern:str, path:str> | Search source content by regex |
```

**How to get the signatures:** Use `inspect.signature()` on the bound methods returned by `space.tools()`. Each tool's `tools()` method returns `{"read": self.read, "write": self.write, ...}`. These are bound methods with proper type annotations.

**How to get the docstrings:** First line of `method.__doc__` (or `method.__func__.__doc__` for bound methods).

### Pattern 4: Pydantic Tool Parameter Validation

**What:** Validate tool call parameters before execution. Return clear error messages guiding the AI.

**Where:** `ToolRouter.dispatch()` in `tools.py`. Before calling `method = getattr(current, tool); result = method(arg, **kwargs)`, validate the arguments.

**Approach:** Build a pydantic model dynamically from the method signature, similar to how `_build_plain_model` works in `lm.py`. Use `TypeAdapter` for simple single-param tools, or `create_model` for multi-param tools.

The project already uses `TypeAdapter` for gate validation:
```python
# shell.py line 376-379
from pydantic import TypeAdapter
adapter = TypeAdapter(gate.field_type)
value = adapter.validate_python(raw_value)
```

For tool validation, the approach is:
1. On first dispatch to a tool, introspect the method signature
2. Cache a validator (TypeAdapter or BaseModel subclass)
3. Validate params before calling
4. Return formatted error on ValidationError

**Example validation error:**
```
Tool 'read' parameter error: target must be a string, got int
Available: read(target: str) -- Read package listing, module summary, or symbol source
```

### Anti-Patterns to Avoid

- **Modifying what the LM sees:** This phase is display-only. The feedback loop to Claude CLI (`_send()`) must not change. The LM should still see all tool outputs and intermediate context.
- **Adding streaming/async complexity:** The current tool execution is synchronous within the eval loop. `run_tool_calls()` returns a list. Don't add async generators or streaming yields.
- **Breaking DebugView/AISelfView:** These views should continue showing everything. Only UserView gets the filtering treatment.

## Don't Hand-Roll

| Problem | Don't Build | Use Instead | Why |
|---------|-------------|-------------|-----|
| Method signature extraction | Manual string parsing of __doc__ | `inspect.signature()` + `typing.get_type_hints()` | Handles defaults, *args, **kwargs, Annotated types correctly |
| Parameter validation | Manual type checking | `pydantic.TypeAdapter` or `pydantic.create_model` | Already in project, handles coercion, nested types, clear error messages |
| ANSI color formatting | Raw escape codes for tool summaries | Rich `Text` with styles, rendered to ANSI via `_rich_to_ansi` | Consistent with existing view code, handles terminal width |

## Common Pitfalls

### Pitfall 1: Intermediate Response Identification
**What goes wrong:** The loop writes `response` on every round-trip. Suppressing "intermediate" responses requires knowing which is the last one, but the loop doesn't know in advance.
**Why it happens:** The loop discovers it's done only when `run_tool_calls` returns empty AND `extract_executable` returns empty.
**How to avoid:** Don't try to predict. Instead: suppress ALL response writes during the tool call loop, then write ONLY the final response after the loop exits. The loop already returns `response` at the end -- just move the final `router.write("ai", ...)` to after the loop.
**Warning signs:** Intermediate "Let me check that file" prose appearing in UserView.

### Pitfall 2: Error Tool Calls vs. Tool Errors
**What goes wrong:** Confusion between (a) a tool that returned an error string (ResourceError), and (b) an exception during tool execution.
**Why it happens:** `run_tool_calls` catches exceptions and returns `f"{type(exc).__name__}: {exc}"` as the output string. ResourceError from resource methods is also caught by ToolRouter and returned as a string.
**How to avoid:** Check the output string prefix for known error type patterns (e.g., starts with a PascalCase word followed by `:`) or pass error status as a flag alongside the output.
**Warning signs:** Normal output being colored red, or errors not being colored red.

### Pitfall 3: Functions Table Signature Extraction on HomeResourcespace
**What goes wrong:** Home resourcespace tools are bare functions (not methods with signatures), set via `registry._home_tools = {"read": _exec_read, ...}`.
**Why it happens:** Home tools are raw function references, not bound methods on a Resourcespace class.
**How to avoid:** The functions table is only shown on `_entry_display()` for navigated-into resources, not at home. Home shows the orientation view instead. But verify this assumption holds.
**Warning signs:** Crash on `inspect.signature()` of a partial or lambda.

### Pitfall 4: Existing Test Expectations
**What goes wrong:** Tests in `test_views.py` and `test_ai.py` assert specific metadata types and display behavior that will change.
**Why it happens:** Tests check for `tool_translated`, `response` rendering behavior, and tool summary format.
**How to avoid:** Update tests alongside implementation. The tests are well-structured -- each tests one behavior.
**Warning signs:** Test failures in the `TestRunToolCalls`, `TestEvalLoopToolCalls`, and view rendering tests.

## Code Examples

### Current _tool_summary (to be replaced)
```python
# Source: bae/repl/ai.py:317
def _tool_summary(tag: str, output: str) -> str:
    m = re.match(r"<(\w+):(.+?)>", tag.strip())
    if not m:
        return output
    tool_type = _TOOL_NAMES.get(m.group(1).lower(), m.group(1))
    arg = m.group(2).strip()
    if tool_type == "R":
        n = output.count("\n") + 1 if output else 0
        return f"read {arg} ({n} lines)"
    ...
```

### New _tool_summary format
```python
def _tool_summary(tag: str, output: str) -> str:
    """Format tool call as: ◆ read(args) -> str (count)"""
    m = re.match(r"<(\w+):(.+?)>", tag.strip())
    if not m:
        return output
    tool_type = _TOOL_NAMES.get(m.group(1).lower(), m.group(1))
    arg = m.group(2).strip()

    # Detect error returns
    is_error = _is_error_output(output)
    return_type = _error_type_name(output) if is_error else "str"

    # Tool name mapping
    name = {"R": "read", "W": "write", "E": "edit", "G": "glob", "Grep": "grep"}.get(tool_type, tool_type)

    # Count hint
    hint = _count_hint(tool_type, output)
    suffix = f" ({hint})" if hint else ""

    return f"◆ {name}({arg}) -> {return_type}{suffix}"
```

### UserView tool_translated rendering with color
```python
# In UserView.render, tool_translated branch:
if content_type == "tool_translated":
    summary = meta.get("tool_summary", content)
    is_error = summary.endswith("Error")  # or check metadata flag
    if is_error:
        # Red for errors
        styled = f"\033[31m{summary}\033[0m"
    else:
        # Dim cyan for normal tool calls
        styled = f"\033[38;5;244m{summary}\033[0m"
    print_formatted_text(ANSI(styled))
    return
```

### Intermediate response suppression
```python
# In AI.__call__, modify the loop:
response = await self._send(self._with_location(full_prompt))
# Don't write intermediate responses during tool loop
while ...:
    tool_results = run_tool_calls(response, router=self._tool_router)
    if tool_results:
        for tag, output in tool_results:
            summary = _tool_summary(tag, output)
            self._router.write("py", tag, mode="PY",
                metadata={"type": "tool_translated", ...})
        response = await self._send(...)
        continue  # Don't write "ai" response here

    blocks = self.extract_executable(response)
    if not blocks:
        break
    # ... existing <run> handling ...

# Only write the final response
self._router.write("ai", response, mode="NL",
    metadata={"type": "response", "label": self._label})
```

### Functions table with typed signatures
```python
# In ResourceRegistry._entry_display, replace the current table:
import inspect

tools_map = space.tools()
if tools_map:
    lines.append("")
    lines.append("| Tool | Signature | Description |")
    lines.append("|------|-----------|-------------|")
    for name in sorted(tools_map):
        method = tools_map[name]
        sig = _tool_signature(name, method)  # e.g. "<Read:target:str>"
        doc = (method.__doc__ or "").split("\n")[0].strip()
        lines.append(f"| {name} | {sig} | {doc} |")
```

### Pydantic tool validation in ToolRouter
```python
from pydantic import TypeAdapter, ValidationError

class ToolRouter:
    def dispatch(self, tool: str, arg: str, **kwargs) -> str:
        current = self._registry.current
        if current is None:
            return self._home_dispatch(tool, arg, **kwargs)
        if tool not in current.supported_tools():
            return format_unsupported_error(current, tool)
        try:
            method = getattr(current, tool)
            # Validate before calling
            validated = _validate_tool_params(method, arg, **kwargs)
            result = method(**validated)
        except ValidationError as e:
            return _format_validation_error(tool, method, e)
        except ResourceError as e:
            return str(e)
        return _prune(result)
```

## State of the Art

| Old Approach | Current Approach | When Changed | Impact |
|--------------|------------------|--------------|--------|
| All AI responses displayed | Intermediate prose hidden | This phase | Cleaner UserView |
| Generic functions table | Typed XML signatures | This phase | AI knows exact tool syntax |
| No param validation | Pydantic validation | This phase | Clear error messages on bad calls |
| `_tool_summary` returns prose | Returns `◆ call(args) -> type` | This phase | Consistent, scannable display |

## Open Questions

1. **Error detection heuristic**
   - What we know: Errors come as strings like `"ResourceError: message"` or `"FileNotFoundError: ..."`. The `run_tool_calls` function catches exceptions and formats them as `f"{type(exc).__name__}: {exc}"`.
   - What's unclear: Should the error flag be passed as metadata alongside the output (structured), or detected from the output string (heuristic)?
   - Recommendation: Pass a structured `is_error` flag from `run_tool_calls` alongside each result. This is more reliable than string parsing. Change the return type from `list[tuple[str, str]]` to `list[tuple[str, str, bool]]` where the bool is `is_error`.

2. **Write/Edit body content in functions table**
   - What we know: Decision says "Body content for multi-line tools keeps current `<W:path>content</W>` format"
   - What's unclear: How to represent this in the functions table signature column
   - Recommendation: Show `<Write:filepath:str>content</Write>` in the signature column, and `<Edit:target:str, new_source:str>` for edit (which uses named params, not body content in the source resourcespace).

3. **DSC-03 requirement: resource context in tool summaries**
   - What we know: `DSC-03` says "Tool summaries in UserView include resource context (e.g., `[source] read ai.py (42 lines)`)"
   - What's unclear: This is officially Phase 36 scope, but the new `◆` format could include `[source]` prefix
   - Recommendation: Include resource context in the summary format now: `◆ [source] read(bae.repl.ai) -> str (42 lines)`. The registry tracks the current resource via `self._registry.current.name`. This is trivial to add and satisfies DSC-03 early.

## Sources

### Primary (HIGH confidence)
- Direct codebase inspection: `bae/repl/ai.py`, `bae/repl/views.py`, `bae/repl/tools.py`, `bae/repl/spaces/view.py`, `bae/repl/spaces/source/service.py`
- Existing tests: `tests/repl/test_views.py`, `tests/repl/test_ai.py`, `tests/test_tools_router.py`

### Secondary (MEDIUM confidence)
- pydantic TypeAdapter usage pattern from `bae/repl/shell.py:376` (gate validation)
- inspect.signature behavior on bound methods (stdlib, well-established)

## Metadata

**Confidence breakdown:**
- Standard stack: HIGH -- no new dependencies, all tools already in project
- Architecture: HIGH -- direct codebase analysis, clear data flow, well-tested existing code
- Pitfalls: HIGH -- identified from actual test expectations and code paths

**Research date:** 2026-02-16
**Valid until:** 2026-03-16 (stable internal architecture, no external dependencies)

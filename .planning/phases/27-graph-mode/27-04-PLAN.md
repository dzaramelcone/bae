---
phase: 27-graph-mode
plan: 04
type: execute
wave: 1
depends_on: []
files_modified:
  - bae/lm.py
  - bae/graph.py
  - bae/repl/engine.py
  - tests/repl/test_engine.py
  - tests/test_graph.py
autonomous: true
gap_closure: true

must_haves:
  truths:
    - "Graph runs complete without premature timeout on complex graphs (6+ nodes)"
    - "inspect shows node trace even when a graph fails mid-execution"
    - "trace shows node transition history even when a graph fails mid-execution"
  artifacts:
    - path: "bae/lm.py"
      provides: "Increased per-call timeout"
      contains: "timeout: int = 120"
    - path: "bae/graph.py"
      provides: "Partial trace attached to all exceptions from arun()"
      contains: "err.trace = trace"
    - path: "bae/repl/engine.py"
      provides: "_wrap_coro extracts partial trace from failed coroutine exceptions"
      contains: "hasattr.*trace"
  key_links:
    - from: "bae/graph.py"
      to: "exception.trace"
      via: "top-level try/except in arun attaches trace to any unhandled exception"
      pattern: "except Exception.*trace"
    - from: "bae/repl/engine.py"
      to: "run.result"
      via: "_wrap_coro reads exception.trace on failure and populates run.result"
      pattern: "run\\.result.*GraphResult"
---

<objective>
Fix graph execution pipeline: increase LM timeout, preserve partial trace on failure, and surface trace data through inspect/trace commands for failed runs.

Purpose: UAT revealed that complex graphs timeout prematurely (20s per LM call too low), and when graphs fail, inspect/trace show no data because partial trace is lost.
Output: Graphs complete reliably; failed runs still expose their partial trace through inspect/trace.
</objective>

<execution_context>
@/Users/dzaramelcone/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dzaramelcone/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/27-graph-mode/27-01-SUMMARY.md
@.planning/phases/27-graph-mode/27-02-SUMMARY.md
@.planning/debug/graph-run-timeout-no-trace.md
@bae/lm.py
@bae/graph.py
@bae/repl/engine.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Increase LM timeout and attach partial trace to all arun exceptions</name>
  <files>bae/lm.py, bae/graph.py, tests/test_graph.py</files>
  <action>
Two changes:

**bae/lm.py** (line 413): Change `timeout: int = 20` to `timeout: int = 120` in ClaudeCLIBackend.__init__. 120s is generous for a single LM call while still protecting against hangs.

**bae/graph.py** `arun()` method: Wrap the `while current is not None:` loop body in a try/except that catches any Exception not already carrying a `.trace` attribute, attaches the current `trace` list, and re-raises. This ensures RuntimeError from LM timeout (which graph.py doesn't currently catch) carries the partial trace just like BaeError and DepError already do.

The catch must:
- NOT catch `asyncio.CancelledError` (it's BaseException in 3.9+, but be explicit)
- Check `if not hasattr(e, 'trace')` before attaching (BaeError/DepError already have it)
- Set `e.trace = trace` then `raise`
- Go around the ENTIRE while loop (not inside it) so it captures trace accumulated across all iterations

Place the try/except like:
```python
trace: list[Node] = []
...
try:
    while current is not None:
        ...existing loop body unchanged...
except Exception as e:
    if not hasattr(e, "trace"):
        e.trace = trace
    raise
```

This is minimal -- the existing BaeError/DepError catches inside the loop still work, they set .trace and raise, and our outer catch sees .trace already set and doesn't overwrite.

**tests/test_graph.py**: Add a test that verifies a RuntimeError raised during graph execution carries a `.trace` attribute with the partial trace. Create a MockLM that succeeds for the first N fill calls then raises RuntimeError. Run the graph with it, catch the RuntimeError, and assert `.trace` is a list with the nodes that executed before the error.
  </action>
  <verify>`uv run pytest tests/test_graph.py -x -q` passes with zero warnings. New test confirms RuntimeError from LM carries `.trace` attribute.</verify>
  <done>ClaudeCLIBackend.timeout is 120. Any exception from arun() carries .trace with the partial execution history.</done>
</task>

<task type="auto">
  <name>Task 2: Extract partial trace from failed coroutine in engine _wrap_coro</name>
  <files>bae/repl/engine.py, tests/repl/test_engine.py</files>
  <action>
In `_wrap_coro` (engine.py), the `except Exception as e:` block (line 154-156) currently only sets `run.state` and `run.error`. It needs to also extract partial trace from the exception and populate `run.result` so that `inspect` and `trace` commands can display data for failed runs.

Modify the except clause in `_wrap_coro`:
```python
except Exception as e:
    run.state = GraphState.FAILED
    run.error = f"{type(e).__name__}: {e}"
    # Preserve partial trace from graph execution for inspect/trace commands
    if hasattr(e, "trace"):
        from bae.result import GraphResult
        run.result = GraphResult(node=None, trace=e.trace)
    raise
```

The `from bae.result import GraphResult` import is inside the except block to keep it lazy (matching existing pattern in engine.py where Graph import is TYPE_CHECKING only). GraphResult(node=None, trace=...) matches what arun() returns on success.

Also apply the same pattern to `_execute`'s except clause (line 117-119) for consistency -- _execute already has TimingLM but if graph.arun raises with a .trace, we should still capture it:
```python
except Exception as e:
    run.state = GraphState.FAILED
    run.error = f"{type(e).__name__}: {e}"
    if hasattr(e, "trace"):
        from bae.result import GraphResult
        run.result = GraphResult(node=None, trace=e.trace)
    raise
```

**tests/repl/test_engine.py**: Add tests:
1. `test_wrap_coro_preserves_trace_on_failure`: Create a coroutine that raises an exception with `.trace = [node1, node2]` attribute set. Submit via submit_coro. After task completes, verify `run.result is not None` and `run.result.trace == [node1, node2]`.
2. `test_execute_preserves_trace_on_failure`: Similar for the _execute path -- mock graph.arun to raise BaeError with .trace attached. Verify run.result is populated.
  </action>
  <verify>`uv run pytest tests/repl/test_engine.py -x -q` passes with zero warnings. Failed runs now have `run.result.trace` populated.</verify>
  <done>_wrap_coro and _execute both extract .trace from exceptions and populate run.result, enabling inspect/trace commands to show data for failed graph runs.</done>
</task>

</tasks>

<verification>
1. `uv run pytest tests/test_graph.py tests/repl/test_engine.py -x -q` -- all pass
2. `uv run pytest tests/ -x -q --ignore=tests/test_integration.py` -- full suite passes
3. Grep confirms: `grep "timeout.*120" bae/lm.py` shows new default
4. Grep confirms: `grep -n "e.trace" bae/graph.py` shows outer catch in arun
5. Grep confirms: `grep -n "run.result.*GraphResult" bae/repl/engine.py` shows trace extraction in both _wrap_coro and _execute
</verification>

<success_criteria>
- ClaudeCLIBackend timeout increased from 20 to 120 seconds
- All exceptions from graph.arun() carry a .trace attribute with partial execution history
- Engine _wrap_coro and _execute extract .trace from exceptions and populate run.result
- inspect and trace commands can display data for failed graph runs (no code change needed -- they already check run.result.trace)
- All existing + new tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/27-graph-mode/27-04-SUMMARY.md`
</output>

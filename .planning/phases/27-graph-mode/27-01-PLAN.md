---
phase: 27-graph-mode
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - bae/graph.py
  - bae/repl/engine.py
  - tests/test_graph.py
  - tests/repl/test_engine.py
autonomous: true

must_haves:
  truths:
    - "Graph exposes its required run parameters as a typed dict via .params property"
    - "Engine can submit a pre-built coroutine (from graph.arun(...)) as a managed run"
    - "Completed graph runs store their GraphResult for later inspection"
  artifacts:
    - path: "bae/graph.py"
      provides: "Graph.params property returning dict[str, FieldInfo] of required start-node plain fields"
      contains: "def params"
    - path: "bae/repl/engine.py"
      provides: "GraphRegistry.submit_coro() for pre-built coroutines, GraphRun.result field"
      contains: "submit_coro"
    - path: "tests/test_graph.py"
      provides: "Tests for Graph.params property"
      contains: "test.*params"
    - path: "tests/repl/test_engine.py"
      provides: "Tests for submit_coro and GraphRun.result storage"
      contains: "submit_coro"
  key_links:
    - from: "bae/repl/engine.py"
      to: "bae/graph.py"
      via: "submit_coro accepts coroutine from graph.arun()"
      pattern: "submit_coro"
    - from: "bae/repl/engine.py"
      to: "bae/result.py"
      via: "_execute stores GraphResult on run.result"
      pattern: "run\\.result.*=.*result"
---

<objective>
Graph API introspection and engine coroutine submission for typed callable graph runs.

Purpose: Dzara's design requires graphs to expose typed callable signatures decoupled from start node internals. The engine needs to accept pre-built coroutines (from `graph.arun(field=val)`) so GRAPH mode's `run <expr>` can evaluate arbitrary expressions and submit the result. GraphRun must store the execution result so `inspect` and `trace` commands (Plan 02) can display traces and field values.

Output: Graph.params property, GraphRegistry.submit_coro(), GraphRun.result field, tests for all three.
</objective>

<execution_context>
@/Users/dzaramelcone/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dzaramelcone/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/27-graph-mode/27-RESEARCH.md
@bae/graph.py
@bae/repl/engine.py
@bae/result.py
@tests/repl/test_engine.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add Graph.params property and GraphRun.result field</name>
  <files>bae/graph.py, bae/repl/engine.py, tests/test_graph.py, tests/repl/test_engine.py</files>
  <action>
**bae/graph.py -- Add params property to Graph class:**

Add a `params` property that returns a copy of `self._input_fields` (already computed in `_validate_start`). This exposes the required plain fields on the start node as a typed dict[str, FieldInfo]. The property already has the data -- it just needs to be public.

```python
@property
def params(self) -> dict[str, FieldInfo]:
    """Run parameters -- required plain fields on the start node."""
    return dict(self._input_fields)
```

Place it after the `terminal_nodes` property, before `validate()`.

**bae/repl/engine.py -- Add result field to GraphRun:**

Add `result: GraphResult | None = None` field to the GraphRun dataclass. Import GraphResult under TYPE_CHECKING to avoid circular imports:

```python
if TYPE_CHECKING:
    from bae.graph import Graph
    from bae.repl.tasks import TaskManager
    from bae.result import GraphResult
```

In `_execute`, store the result on run before returning:
```python
result = await run.graph.arun(lm=timing_lm, dep_cache=dep_cache, **kwargs)
run.result = result  # Store for inspect/trace commands
run.state = GraphState.DONE
return result
```

**tests/test_graph.py -- Add params property tests:**

Add two tests:
1. `test_params_returns_required_plain_fields` -- create a Graph with a start node that has required plain fields, verify `graph.params` returns dict with those field names as keys and FieldInfo values, and that it does NOT include Dep/Recall fields.
2. `test_params_empty_when_no_required_fields` -- create a Graph with a start node that has no required plain fields (all optional or all dep/recall), verify `graph.params` returns empty dict.

Use simple test Node classes (similar to existing test patterns in the file).

**tests/repl/test_engine.py -- Add result storage test:**

Add `test_completed_run_stores_result` -- submit a graph via registry, await the task, verify `run.result` is a GraphResult instance with a non-empty trace. Use the existing MockLM and test node patterns from the file.
  </action>
  <verify>
`uv run pytest tests/test_graph.py -x -q --ignore=tests/test_integration.py` passes.
`uv run pytest tests/repl/test_engine.py -x -q` passes.
`uv run pytest tests/ -x -q --ignore=tests/test_integration.py` -- full suite passes.
  </verify>
  <done>
Graph.params returns dict[str, FieldInfo] of required plain start-node fields. GraphRun.result stores the GraphResult after successful execution. All tests pass.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add submit_coro to GraphRegistry</name>
  <files>bae/repl/engine.py, tests/repl/test_engine.py</files>
  <action>
**bae/repl/engine.py -- Add submit_coro method to GraphRegistry:**

Add `submit_coro` that accepts an already-constructed coroutine (from `graph.arun(...)`) and wraps it with lifecycle tracking. This is needed for `run graph.arun(field=val)` where the expression evaluates to a coroutine.

```python
def submit_coro(
    self, coro, tm: TaskManager, *, name: str = "graph",
) -> GraphRun:
    """Submit a pre-built coroutine as a managed graph run.

    Unlike submit(), this accepts an already-constructed coroutine
    (e.g. from graph.arun(field=val)). TimingLM cannot be injected
    since the coroutine is already built.
    """
    run_id = f"g{self._next_id}"
    self._next_id += 1
    run = GraphRun(run_id=run_id, graph=None)
    self._runs[run_id] = run
    wrapped = self._execute_coro(run, coro)
    tm.submit(wrapped, name=f"graph:{run_id}:{name}", mode="graph")
    return run
```

Add `_execute_coro` as the coroutine wrapper:
```python
async def _execute_coro(self, run: GraphRun, coro):
    """Wrap a pre-built coroutine with lifecycle tracking."""
    try:
        result = await coro
        run.state = GraphState.DONE
        if hasattr(result, 'trace'):
            run.result = result
        return result
    except asyncio.CancelledError:
        run.state = GraphState.CANCELLED
        raise
    except Exception as e:
        run.state = GraphState.FAILED
        run.error = f"{type(e).__name__}: {e}"
        raise
    finally:
        run.ended_ns = time.perf_counter_ns()
        self._archive(run)
```

The `graph` field on GraphRun must become optional: change the type annotation to `graph: Graph | None = None`. Since the field currently has no default, it must be reordered AFTER the fields that have defaults, OR given a default of None. Given the dataclass field ordering (run_id has no default, graph has no default), change `graph` to `graph: Graph | None = None` and place it after run_id.

Also update `submit()` to store result on run in `_execute`:
The result storage was added in Task 1. Verify it's consistent.

**GraphRegistry.all_runs() helper:**

Add a method that returns both active and completed runs for the `list` command:
```python
def all_runs(self) -> list[GraphRun]:
    """All runs: active first, then completed (most recent first)."""
    return list(self._runs.values()) + list(reversed(self._completed))
```

**tests/repl/test_engine.py -- Add submit_coro tests:**

1. `test_submit_coro_completes` -- create a simple async coroutine that returns a value, submit via submit_coro, await the task, verify run.state == DONE.
2. `test_submit_coro_failure_captures_error` -- create a coroutine that raises, submit via submit_coro, verify run.state == FAILED and run.error contains the exception.
3. `test_submit_coro_stores_graph_result` -- create a coroutine that returns a GraphResult, submit via submit_coro, verify run.result is the GraphResult.
4. `test_all_runs_returns_active_and_completed` -- submit two graphs, let one complete, verify all_runs returns both.
  </action>
  <verify>
`uv run pytest tests/repl/test_engine.py -x -q` passes.
`uv run pytest tests/ -x -q --ignore=tests/test_integration.py` -- full suite passes.
  </verify>
  <done>
GraphRegistry.submit_coro() wraps pre-built coroutines with lifecycle tracking. GraphRun.graph is optional (None for coroutine submissions). all_runs() returns combined active + completed list. All tests pass.
  </done>
</task>

</tasks>

<verification>
- `uv run pytest tests/ -x -q --ignore=tests/test_integration.py` -- all tests pass
- `uv run python -c "from bae.graph import Graph; from bae.repl.engine import GraphRegistry; print('imports ok')"` -- no import errors
- Graph.params returns dict of required plain fields (verified by tests)
- GraphRegistry.submit_coro wraps coroutines with lifecycle tracking (verified by tests)
- GraphRun.result stores GraphResult after execution (verified by tests)
</verification>

<success_criteria>
1. Graph.params property exists and returns correct FieldInfo dict
2. GraphRegistry.submit_coro() accepts and lifecycle-tracks pre-built coroutines
3. GraphRun.result field stores GraphResult for completed runs
4. GraphRegistry.all_runs() returns combined active + completed runs
5. All existing tests continue to pass (no regressions from GraphRun field changes)
</success_criteria>

<output>
After completion, create `.planning/phases/27-graph-mode/27-01-SUMMARY.md`
</output>

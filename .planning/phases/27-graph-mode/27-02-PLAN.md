---
phase: 27-graph-mode
plan: 02
type: execute
wave: 2
depends_on: ["27-01"]
files_modified:
  - bae/repl/graph_commands.py
  - bae/repl/shell.py
  - tests/repl/test_graph_mode.py
autonomous: true

must_haves:
  truths:
    - "Dzara can type 'run <expr>' in GRAPH mode to evaluate a namespace expression and submit a graph"
    - "Dzara can type 'list' in GRAPH mode to see all runs with state, elapsed, and current node"
    - "Dzara can type 'cancel <id>' in GRAPH mode to stop a running graph"
    - "Dzara can type 'inspect <id>' in GRAPH mode to see trace with node timings and field values"
    - "Dzara can type 'trace <id>' in GRAPH mode to see compact node transition history"
    - "Unknown GRAPH mode input shows help listing available commands"
  artifacts:
    - path: "bae/repl/graph_commands.py"
      provides: "Command dispatcher and handlers for run/list/cancel/inspect/trace"
      exports: ["dispatch_graph"]
    - path: "bae/repl/shell.py"
      provides: "GRAPH mode dispatch wired to dispatch_graph"
      contains: "dispatch_graph"
    - path: "tests/repl/test_graph_mode.py"
      provides: "Tests for command parsing, dispatch, and all command handlers"
      contains: "test_dispatch"
  key_links:
    - from: "bae/repl/graph_commands.py"
      to: "bae/repl/exec.py"
      via: "run command uses async_exec for expression evaluation"
      pattern: "async_exec"
    - from: "bae/repl/graph_commands.py"
      to: "bae/repl/engine.py"
      via: "run submits via engine.submit or engine.submit_coro"
      pattern: "engine\\.submit"
    - from: "bae/repl/shell.py"
      to: "bae/repl/graph_commands.py"
      via: "_dispatch calls dispatch_graph for GRAPH mode"
      pattern: "dispatch_graph"
    - from: "bae/repl/graph_commands.py"
      to: "bae/repl/views.py"
      via: "list/inspect/trace use _rich_to_ansi for formatted output"
      pattern: "_rich_to_ansi"
---

<objective>
GRAPH mode command interface: run, list, cancel, inspect, and trace commands.

Purpose: Replace the current bare-text graph submission (`_run_graph`) with a proper command dispatcher that parses `run <expr>`, `list`, `cancel <id>`, `inspect <id>`, and `trace <id>`. This gives Dzara full lifecycle management of graph runs from GRAPH mode. The `run` command evaluates arbitrary namespace expressions via `async_exec`, supporting both `run graph` (Graph object, submitted via engine.submit with TimingLM) and `run graph.arun(field=val)` (coroutine, submitted via engine.submit_coro).

Output: graph_commands.py module with dispatcher, shell.py wired to use it, full test coverage.
</objective>

<execution_context>
@/Users/dzaramelcone/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dzaramelcone/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/27-graph-mode/27-RESEARCH.md
@.planning/phases/27-graph-mode/27-01-SUMMARY.md
@bae/repl/shell.py
@bae/repl/engine.py
@bae/repl/exec.py
@bae/repl/views.py
@bae/repl/tasks.py
@bae/graph.py
@bae/result.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create graph_commands.py with dispatcher and all command handlers</name>
  <files>bae/repl/graph_commands.py</files>
  <action>
Create `bae/repl/graph_commands.py` with the full command dispatcher and handlers.

**Module structure:**
```python
"""GRAPH mode command dispatcher and handlers."""

from __future__ import annotations

import asyncio
import traceback
from typing import TYPE_CHECKING

from bae.repl.exec import async_exec
from bae.repl.views import _rich_to_ansi

if TYPE_CHECKING:
    from bae.repl.shell import CortexShell
```

**dispatch_graph(text, shell) -> None:**
- Parse `text.strip().split(None, 1)` into `(cmd, arg)`.
- If no input, return silently.
- `cmd = parts[0].lower()`, `arg = parts[1] if len(parts) > 1 else ""`.
- Dispatch via dict: `{"run": _cmd_run, "list": _cmd_list, "ls": _cmd_list, "cancel": _cmd_cancel, "inspect": _cmd_inspect, "trace": _cmd_trace}`.
- Unknown command: write to `[graph]` channel: `"unknown command: {cmd}. try: run, list, cancel, inspect, trace"`.

**_cmd_run(arg, shell) -> None:**
- If `not arg.strip()`: check `shell.namespace.get("graph")`. If it's a Graph object, submit via `shell.engine.submit(graph, shell.tm, lm=shell._lm)` (preserves Phase 26 UX of bare submission). If no graph in namespace, write usage: `"usage: run <expr>  (e.g. run graph, run graph.arun(x=1))"`.
- Otherwise: evaluate `arg` via `async_exec(arg, shell.namespace)`.
  - If result is a coroutine: submit via `shell.engine.submit_coro(result, shell.tm, name=arg[:30])`.
  - If result is a `Graph` object (import Graph): submit via `shell.engine.submit(result, shell.tm, lm=shell._lm)`.
  - Otherwise: write error `"expected Graph or coroutine, got {type(result).__name__}"`. If result is a coroutine-like, close it to avoid RuntimeWarning.
- On success: write `"submitted {run.run_id}"` to `[graph]` channel with `metadata={"type": "lifecycle", "run_id": run.run_id}`.
- Attach done callback (same pattern as current `_run_graph` in shell.py) that surfaces completion/failure/cancellation through `[graph]` channel.
- Wrap everything in try/except: on error, format traceback and write to `[graph]` channel with `metadata={"type": "error"}`.

**_attach_done_callback(run, shell) -> None:**
- Extract the done callback logic from current `_run_graph` in shell.py into a reusable function.
- Find the matching task in `shell.tm.active()` by name prefix `f"graph:{run.run_id}:"`.
- Add done callback that writes completion/failure/cancellation to `[graph]` channel.

**_cmd_list(arg, shell) -> None:**
- Get all runs via `shell.engine.all_runs()`.
- If empty: write `"no graph runs"`.
- Otherwise: build a Rich Table with columns: ID (yellow), STATE, ELAPSED (right-justified), NODE.
- For each run: format elapsed as `f"{(run.ended_ns - run.started_ns) / 1e9:.1f}s"` if ended, or `f"{(time.perf_counter_ns() - run.started_ns) / 1e9:.1f}s"` if still running.
- State should be styled: running=green, done=dim, failed=red, cancelled=yellow.
- Render via `_rich_to_ansi(table)` and print via `print_formatted_text(ANSI(ansi))`.
- Import `time`, `rich.table.Table`, `prompt_toolkit.print_formatted_text`, `prompt_toolkit.formatted_text.ANSI`.

**_cmd_cancel(arg, shell) -> None:**
- `run_id = arg.strip()`. If empty: write `"usage: cancel <id>"`.
- `run = shell.engine.get(run_id)`. If None: write `"no run {run_id}"`.
- Find matching task in `shell.tm.active()` by name prefix `f"graph:{run.run_id}:"`.
- If found: `shell.tm.revoke(tt.task_id)`, write `"cancelled {run.run_id}"`.
- If not found: write `"{run.run_id} not running"`.

**_cmd_inspect(arg, shell) -> None:**
- `run_id = arg.strip()`. If empty: write `"usage: inspect <id>"`.
- `run = shell.engine.get(run_id)`. If None: write `"no run {run_id}"`.
- Build Rich output:
  - Header: `"Run {run.run_id} ({run.state.value}, {elapsed})"`.
  - If `run.graph`: show graph start node name.
  - Node timings section: for each NodeTiming in `run.node_timings`, show `"  {nt.node_type}  {nt.duration_ms:.0f}ms"`.
  - If `run.result` (GraphResult): show terminal node and its field values. Use `run.result.result` (the terminal node). For each field in the terminal node's `model_fields`, show `"  {name}: {repr(getattr(node, name))}"` truncated to 100 chars with `textwrap.shorten`.
  - If `run.error`: show error string.
- Render via `_rich_to_ansi` and print.

**_cmd_trace(arg, shell) -> None:**
- `run_id = arg.strip()`. If empty: write `"usage: trace <id>"`.
- `run = shell.engine.get(run_id)`. If None: write `"no run {run_id}"`.
- If `run.result` (GraphResult with trace): show numbered node list.
  - For each node in `run.result.trace`: `"  {i}. {node.__class__.__name__}"`.
  - If timing data available, append duration next to matching node type.
- If no result but has node_timings: show timing-only trace from `run.node_timings`.
- If neither: write `"{run_id} has no trace data"`.
- Keep it compact -- this is the quick view vs inspect's detailed view.
  </action>
  <verify>
`uv run python -c "from bae.repl.graph_commands import dispatch_graph; print('import ok')"` -- no import errors.
  </verify>
  <done>
graph_commands.py exists with dispatch_graph, _cmd_run, _cmd_list, _cmd_cancel, _cmd_inspect, _cmd_trace. All handlers follow channel output pattern with [graph] channel. Done callback factored into reusable _attach_done_callback.
  </done>
</task>

<task type="auto">
  <name>Task 2: Wire shell.py to dispatch_graph and add tests</name>
  <files>bae/repl/shell.py, tests/repl/test_graph_mode.py</files>
  <action>
**bae/repl/shell.py changes:**

1. Import dispatch_graph:
   ```python
   from bae.repl.graph_commands import dispatch_graph
   ```

2. Replace the GRAPH mode dispatch in `_dispatch`:
   Change:
   ```python
   elif self.mode == Mode.GRAPH:
       await self._run_graph(text)
   ```
   To:
   ```python
   elif self.mode == Mode.GRAPH:
       await dispatch_graph(text, self)
   ```

3. Remove the `_run_graph` method entirely from CortexShell. Its functionality is now in graph_commands.py. The done callback logic has been moved to `_attach_done_callback` in graph_commands.py.

**tests/repl/test_graph_mode.py -- comprehensive test module:**

Create a test file with a mock shell fixture that provides:
- `namespace` dict (seeded with test Graph objects)
- `engine` (real GraphRegistry)
- `tm` (real TaskManager)
- `router` (mock that captures writes to verify output)
- `_lm` (MockLM from existing test patterns)

Use `asyncio` test patterns consistent with `tests/repl/test_engine.py`.

**Dispatch tests:**
1. `test_dispatch_unknown_command` -- dispatch "foobar", verify router.write called with "unknown command" message.
2. `test_dispatch_empty_input` -- dispatch "", verify no output.

**Run command tests:**
3. `test_run_no_args_submits_namespace_graph` -- put a Graph in namespace["graph"], dispatch "run", verify engine has an active run. Use a start node with no required fields so it runs successfully.
4. `test_run_no_args_no_graph_shows_usage` -- no graph in namespace, dispatch "run", verify usage message.
5. `test_run_expr_graph_object` -- put Graph in namespace under a different name, dispatch "run mygraph", verify engine has an active run.
6. `test_run_expr_coroutine` -- dispatch "run graph.arun()" (graph with no required fields), verify submit_coro path used and run created.
7. `test_run_expr_invalid_type` -- dispatch "run 42", verify error message about expected Graph or coroutine.
8. `test_run_expr_syntax_error` -- dispatch "run !!!", verify traceback in output.

**List command tests:**
9. `test_list_empty` -- dispatch "list" with no runs, verify "no graph runs" output.
10. `test_list_shows_runs` -- submit a graph, dispatch "list", verify output contains run_id and state.

**Cancel command tests:**
11. `test_cancel_running_graph` -- submit a long-running graph (use asyncio.sleep in a Dep), dispatch "cancel gN", verify run state is CANCELLED.
12. `test_cancel_nonexistent` -- dispatch "cancel g999", verify "no run" message.

**Inspect command tests:**
13. `test_inspect_completed_run` -- submit graph, let it complete, dispatch "inspect gN", verify output contains run_id, state, and node timings.
14. `test_inspect_nonexistent` -- dispatch "inspect g999", verify "no run" message.

**Trace command tests:**
15. `test_trace_completed_run` -- submit graph, let it complete, dispatch "trace gN", verify numbered node list in output.
16. `test_trace_nonexistent` -- dispatch "trace g999", verify "no run" message.

For the mock router, create a simple class that captures all write() calls into a list:
```python
class MockRouter:
    def __init__(self):
        self.writes = []
    def write(self, channel, content, *, mode=None, metadata=None):
        self.writes.append((channel, content, mode, metadata))
```

For the mock shell, create a simple namespace object:
```python
class MockShell:
    def __init__(self):
        self.namespace = {}
        self.engine = GraphRegistry()
        self.tm = TaskManager()
        self.router = MockRouter()
        self._lm = MockLM()
```

Use `asyncio.run()` or `pytest-asyncio` (check existing test patterns) for async tests. Test nodes should be simple: a terminal node with no required fields for basic tests, and a node with a required field for field-injection tests.
  </action>
  <verify>
`uv run pytest tests/repl/test_graph_mode.py -x -q` passes.
`uv run pytest tests/ -x -q --ignore=tests/test_integration.py` -- full suite passes, no regressions.
  </verify>
  <done>
GRAPH mode dispatch routes through dispatch_graph. _run_graph removed from shell.py. All 5 commands (run, list, cancel, inspect, trace) work. Unknown input shows help. 16+ tests covering all command paths pass.
  </done>
</task>

</tasks>

<verification>
1. `uv run pytest tests/ -x -q --ignore=tests/test_integration.py` -- all tests pass
2. GRAPH mode commands verified by test suite:
   - `run` submits graphs (both Graph objects and coroutines)
   - `list` shows active and completed runs with state/timing
   - `cancel` revokes running graphs via TaskManager
   - `inspect` shows trace details with node timings and field values
   - `trace` shows compact node transition history
3. Requirements coverage:
   - MODE-01: GRAPH mode parses commands -- dispatch_graph handles run/list/cancel/inspect/trace
   - MODE-02: run <expr> evaluates namespace expression -- async_exec + engine.submit/submit_coro
   - MODE-03: list shows runs with state/timing/node -- _cmd_list with Rich table
   - MODE-04: cancel <id> revokes graph -- _cmd_cancel via TaskManager.revoke
   - MODE-05: inspect <id> shows trace with timings and values -- _cmd_inspect with Rich output
</verification>

<success_criteria>
1. All 5 GRAPH mode commands parse and execute correctly
2. `run <expr>` handles: Graph objects (with TimingLM), coroutines (submit_coro), errors
3. `run` with no args submits namespace["graph"] (backward compat with Phase 26)
4. `list` displays formatted table of all runs
5. `cancel` stops running graphs cleanly
6. `inspect` shows detailed run data including field values
7. `trace` shows compact node history
8. Unknown commands show help text
9. All errors surfaced through [graph] channel, no silent failures
10. No regressions in existing test suite
</success_criteria>

<output>
After completion, create `.planning/phases/27-graph-mode/27-02-SUMMARY.md`
</output>

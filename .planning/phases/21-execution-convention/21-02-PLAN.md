---
phase: 21-execution-convention
plan: 02
type: execute
wave: 2
depends_on: ["21-01"]
files_modified:
  - bae/repl/ai.py
  - bae/repl/ai_prompt.md
  - tests/repl/test_ai.py
autonomous: true

must_haves:
  truths:
    - "AI code blocks marked as executable are extracted and run by the eval loop"
    - "AI code blocks shown as illustration are NOT extracted or executed"
    - "Only the first executable block per response is extracted and run"
    - "Extra executable blocks produce feedback to AI and notice to user"
    - "When AI does not use the convention, no code executes"
  artifacts:
    - path: "bae/repl/ai.py"
      provides: "extract_executable() replacing extract_code(), updated eval loop"
      contains: "extract_executable"
    - path: "bae/repl/ai_prompt.md"
      provides: "System prompt with winning convention fewshot examples"
      contains: "Code execution convention"
    - path: "tests/repl/test_ai.py"
      provides: "Updated tests for new extraction and eval loop behavior"
      contains: "TestExtractExecutable"
  key_links:
    - from: "bae/repl/ai.py"
      to: "bae/repl/ai_prompt.md"
      via: "_load_prompt() reads updated system prompt with convention"
      pattern: "_PROMPT_FILE"
    - from: "bae/repl/ai.py __call__"
      to: "bae/repl/ai.py extract_executable"
      via: "eval loop calls extract_executable instead of extract_code"
      pattern: "self.extract_executable"
---

<objective>
Implement the winning convention from Plan 01's eval results. Replace `extract_code()` with `extract_executable()`, update the system prompt with convention-specific fewshot, and modify the eval loop for single-block execution with multi-block feedback.

Purpose: Production implementation of the empirically validated convention. Clean break from current blind extraction.
Output: Convention-aware code extraction and execution in the AI eval loop.
</objective>

<execution_context>
@/Users/dzaramelcone/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dzaramelcone/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/21-execution-convention/21-RESEARCH.md
@.planning/phases/21-execution-convention/21-CONTEXT.md
@.planning/phases/21-execution-convention/21-01-SUMMARY.md

# Files being modified
@bae/repl/ai.py
@bae/repl/ai_prompt.md
@tests/repl/test_ai.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Replace extract_code with extract_executable and update eval loop</name>
  <files>bae/repl/ai.py</files>
  <action>
Read the 21-01-SUMMARY.md to learn which convention won the eval. Use that convention's regex.

**Replace `_CODE_BLOCK_RE` and `extract_code`:**

1. Replace the `_CODE_BLOCK_RE` regex (line 28-31) with the winning convention's regex, named `_EXEC_BLOCK_RE`.

2. Replace the `extract_code` static method with `extract_executable`:
   ```python
   @staticmethod
   def extract_executable(text: str) -> tuple[str | None, int]:
       """Extract first executable code block and count of extras.

       Returns (code, extra_count) where code is the first executable
       block or None, and extra_count is additional blocks ignored.
       """
       matches = _EXEC_BLOCK_RE.findall(text)
       if not matches:
           return None, 0
       return matches[0], len(matches) - 1
   ```

3. Update the docstring on AI class to reference `extract_executable` instead of `extract_code`.

**Update the eval loop in `__call__`:**

Replace the current eval loop body (lines 93-123) with single-block extraction:

```python
for _ in range(self._max_eval_iters):
    code, extra = self.extract_executable(response)
    if code is None:
        break

    # Execute the single block
    output = ""
    try:
        result, captured = await async_exec(code, self._namespace)
        if asyncio.iscoroutine(result):
            result = await result
        output = captured
        if result is not None:
            output += repr(result)
        output = output or "(no output)"
    except (asyncio.CancelledError, KeyboardInterrupt, SystemExit):
        raise
    except BaseException:
        tb = traceback.format_exc()
        output = tb

    self._router.write("py", code, mode="PY", metadata={"type": "ai_exec", "label": self._label})
    if output:
        self._router.write("py", output, mode="PY", metadata={"type": "ai_exec_result", "label": self._label})

    # Build feedback
    feedback = f"[Output]\n{output}"

    # Multi-block notice
    if extra > 0:
        notice = (
            f"Only your first executable block was run. "
            f"{extra} additional block{'s' if extra != 1 else ''} "
            f"{'were' if extra != 1 else 'was'} ignored."
        )
        feedback += f"\n\n{notice}"
        self._router.write(
            "debug", notice, mode="DEBUG",
            metadata={"type": "exec_notice", "label": self._label},
        )

    await asyncio.sleep(0)  # cancellation checkpoint
    response = await self._send(feedback)
    await asyncio.sleep(0)  # cancellation checkpoint
    self._router.write("ai", response, mode="NL", metadata={"type": "response", "label": self._label})
```

Key changes from current code:
- `extract_code` -> `extract_executable` returning (code | None, extra_count) instead of list[str]
- Single block execution (no `for code in blocks` loop)
- Multi-block feedback notice appended to AI feedback and written to debug channel
- Feedback format simplified: `[Output]\n{output}` instead of `[Block 1 output]\n{result}`
  </action>
  <verify>
`python -c "from bae.repl.ai import AI; print(AI.extract_executable.__doc__)"` succeeds. `grep -c "extract_code" bae/repl/ai.py` returns 0 (fully replaced). `grep -c "extract_executable" bae/repl/ai.py` returns at least 2.
  </verify>
  <done>
`extract_code()` fully replaced by `extract_executable()`. Eval loop executes only the first executable block. Multi-block feedback sent to AI and user via debug channel. No references to old `extract_code` remain in ai.py.
  </done>
</task>

<task type="auto">
  <name>Task 2: Update system prompt and tests for winning convention</name>
  <files>bae/repl/ai_prompt.md, tests/repl/test_ai.py</files>
  <action>
**Update ai_prompt.md:**

Read the 21-01-SUMMARY.md to confirm the winning convention. Replace the current prompt with the winning convention's rules and fewshot examples.

The current prompt has a "When to write code" section with "1 fence per turn max. Every fence is immediately executed." Replace this section and the examples section with the convention-specific content.

Add a `## Code execution convention` section that:
- Explains the convention (e.g., for fence_annotation: "Use `python:exec` for code to run, plain `python` for examples")
- States: "Only your FIRST executable block per response is run. Additional executable blocks are ignored."
- Includes 3-5 fewshot examples in `<examples><example>` XML structure covering:
  1. Pure NL answer (no code at all)
  2. Computation (one exec block)
  3. Explanation with illustrative code (no exec block)
  4. Mixed: explain then execute (one illustrative + one exec block)
  5. Namespace inspection (one exec block)

Preserve the rest of the prompt (Session header, Rules, Tools, # Begin section).

Use the fewshot examples from the RESEARCH.md as a starting point, adapted to match the winning convention's syntax.

**Update tests/repl/test_ai.py:**

1. Replace `TestExtractCode` class with `TestExtractExecutable`:
   - `test_single_executable_block`: Text with one convention-marked block extracts (code, 0).
   - `test_illustrative_block_ignored`: Text with only illustrative code returns (None, 0).
   - `test_mixed_blocks`: Text with one exec and one illustrative returns (exec_code, 0).
   - `test_multiple_executable_blocks`: Text with 2+ exec blocks returns (first_code, N-1).
   - `test_no_code_blocks`: Plain text returns (None, 0).
   - `test_bare_fence_not_extracted`: Bare ``` fence (no language tag) returns (None, 0).

   Each test uses the winning convention's syntax in test strings.

2. Update `TestEvalLoop` tests:
   - All tests that use markdown-fenced code must switch to the winning convention syntax.
   - `test_eval_loop_extracts_and_executes`: Use convention syntax instead of bare ```python fence.
   - `test_eval_loop_feeds_back_output`: Update fence syntax + check `[Output]` format.
   - `test_eval_loop_iteration_limit`: Update fence syntax.
   - `test_eval_loop_awaits_coroutine`: Update fence syntax.
   - `test_eval_loop_catches_exec_error`: Update fence syntax.
   - `test_eval_loop_tees_output`: Update fence syntax.
   - `test_eval_loop_tees_error_output`: Update fence syntax.
   - `test_eval_loop_cancellation_propagates`: Update fence syntax.
   - Add `test_eval_loop_multi_block_notice`: Two exec blocks in response -- only first executed, debug channel receives notice, AI feedback includes notice text.
   - Add `test_eval_loop_illustrative_not_executed`: Response with only illustrative code (no convention marker) -- loop breaks without executing anything.

3. Update `TestPromptFile`:
   - `test_prompt_mentions_convention`: System prompt contains "Code execution convention" section.
   - Keep `test_prompt_file_exists` and `test_prompt_loads` as-is.
   - Update `test_prompt_mentions_bae` to also assert the convention keyword is present (e.g., "python:exec" or "<exec>" depending on winner).

4. Update imports: `from bae.repl.ai import AI, _build_context, _load_prompt, _PROMPT_FILE` -- remove any `_CODE_BLOCK_RE` import if present.

Run the full test suite: `pytest tests/repl/test_ai.py -v`

All tests must pass. Zero regressions in other test files: `pytest tests/ -v`
  </action>
  <verify>
`pytest tests/repl/test_ai.py -v` -- all tests pass. `pytest tests/ -v` -- full suite passes, zero regressions. `grep -c "extract_code" tests/repl/test_ai.py` returns 0.
  </verify>
  <done>
System prompt teaches the winning convention with fewshot examples. TestExtractExecutable validates convention-aware extraction. TestEvalLoop uses convention syntax in all test strings. Multi-block notice and illustrative-not-executed scenarios tested. Full test suite green.
  </done>
</task>

</tasks>

<verification>
- `grep -r "extract_code" bae/repl/` returns zero matches (fully replaced)
- `grep "extract_executable" bae/repl/ai.py` returns the new method
- `grep "Code execution convention" bae/repl/ai_prompt.md` returns the convention section
- `pytest tests/repl/test_ai.py -v` -- all tests pass
- `pytest tests/ -v` -- full suite green, zero regressions
- The system prompt contains fewshot examples in `<example>` tags
- The eval loop executes only the first executable block per response
- Extra blocks produce a debug channel notice
</verification>

<success_criteria>
- Convention-marked code blocks are extracted and executed by the eval loop
- Illustrative code blocks are never extracted or executed
- Only the first executable block per response runs; extras are ignored with feedback
- When AI doesn't use the convention, no code executes (clean break, per locked decision)
- All existing tests updated to use new convention syntax and pass
- No regressions in the broader test suite
</success_criteria>

<output>
After completion, create `.planning/phases/21-execution-convention/21-02-SUMMARY.md`
</output>

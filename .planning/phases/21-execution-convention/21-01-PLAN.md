---
phase: 21-execution-convention
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - evals/conftest.py
  - evals/test_convention.py
  - evals/prompts.py
autonomous: false

must_haves:
  truths:
    - "All three convention candidates are tested across Opus, Sonnet, and Haiku"
    - "Each convention x model x scenario combination runs 3 times for confidence"
    - "Eval results clearly show which convention(s) achieve 100% compliance"
    - "Eval uses simple, natural prompts identical across conventions -- only system prompt differs"
  artifacts:
    - path: "evals/conftest.py"
      provides: "Shared pytest fixtures and e2e conftest for evals"
    - path: "evals/prompts.py"
      provides: "Convention-specific system prompts with fewshot examples, regex patterns, and validation logic"
    - path: "evals/test_convention.py"
      provides: "Parametrized eval: 3 conventions x 3 models x 5 scenarios x 3 repetitions = 135 test cases"
  key_links:
    - from: "evals/test_convention.py"
      to: "evals/prompts.py"
      via: "imports system prompts, regexes, and validator"
      pattern: "from evals.prompts import"
    - from: "evals/test_convention.py"
      to: "claude CLI subprocess"
      via: "asyncio.create_subprocess_exec with --no-session-persistence"
      pattern: "create_subprocess_exec"
---

<objective>
Build the eval harness that tests all three convention candidates (fence annotation, wrapper marker, inverse convention) across three Claude tiers (Opus, Sonnet, Haiku) and run it to select the winning convention.

Purpose: The convention choice is an open research question per locked decisions. Empirical validation determines the winner -- no pre-commitment.
Output: Eval results showing compliance rates per convention x model. One convention selected for Plan 02 implementation.
</objective>

<execution_context>
@/Users/dzaramelcone/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dzaramelcone/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/21-execution-convention/21-RESEARCH.md
@.planning/phases/21-execution-convention/21-CONTEXT.md

# Current extraction code being replaced
@bae/repl/ai.py
@bae/repl/ai_prompt.md

# Conftest with existing e2e marker
@tests/conftest.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Build eval harness with convention prompts, regex patterns, and validation</name>
  <files>evals/__init__.py, evals/conftest.py, evals/prompts.py</files>
  <action>
Create `evals/` directory with three files:

**evals/__init__.py** -- empty.

**evals/conftest.py** -- Import and register the e2e marker (same pattern as tests/conftest.py). Add a `--run-e2e` option if not inherited from root conftest. Since evals/ is a separate test root, it needs its own conftest.

**evals/prompts.py** -- Contains all convention-specific logic:

1. **Three system prompts** -- one per convention candidate. Each is a complete system prompt (based on the current `ai_prompt.md` structure) with the convention-specific rules and 3-5 fewshot `<example>` tags.

   - **Candidate A (fence_annotation):** Teach `python:exec` for executable, plain `python` for illustrative. Fewshot shows both types clearly.
   - **Candidate B (wrapper_marker):** Teach `<exec>` wrapper around executable fences. Fewshot shows `<exec>```python...```</exec>` for executable and bare fences for illustrative.
   - **Candidate C (inverse):** Teach `python:example` for illustrative, bare `python` fences remain executable. Fewshot shows `:example` annotation on examples.

   Each prompt must:
   - Explain the convention clearly in a ## Code execution convention section
   - Include 3-5 fewshot examples using `<examples><example>` XML structure (per Anthropic guidance)
   - Cover: pure NL (no code), computation (one exec block), explain-with-example (illustrative only), mixed (both types)
   - Say "Only your FIRST executable block per response is run" in each prompt variant
   - Never hint at the convention in a way that biases the eval prompts

2. **Three compiled regexes** in a `CONVENTION_REGEXES` dict keyed by convention name:
   - `"fence_annotation"`: `r"```python:exec\s*\n(.*?)\n```"` (re.DOTALL)
   - `"wrapper_marker"`: `r"<exec>\s*```(?:python|py)?\s*\n(.*?)\n```\s*</exec>"` (re.DOTALL)
   - `"inverse"`: `r"```(?:python|py)(?!:example)\s*\n(.*?)\n```"` (re.DOTALL)

3. **A `SYSTEM_PROMPTS` dict** keyed by convention name mapping to the full system prompt string.

4. **A `validate_response(response, convention, expected)` function** that checks:
   - `"no_code"`: No Python code fences at all in response. `assert len(all_python) == 0`
   - `"one_exec"`: At least one executable block found by the convention regex. `assert len(exec_blocks) >= 1`
   - `"no_exec"`: Zero executable blocks, but at least one illustrative Python fence. For inverse convention specifically, also verify the illustrative fences have `:example` annotation (not just bare fences -- silent failure detection per Pitfall 5).
   - `"mixed"`: At least one executable block AND at least one non-executable Python fence.

   Use a helper regex for "all python fences": `r"```(?:python|py)\S*\s*\n(.*?)\n```"` (re.DOTALL).

5. **An `eval_send(model, system_prompt, user_prompt, timeout=30)` async function** that dispatches to Claude CLI:
   ```
   cmd = [
       "claude", "-p", user_prompt,
       "--model", model,
       "--output-format", "text",
       "--tools", "",
       "--strict-mcp-config",
       "--setting-sources", "",
       "--no-session-persistence",
       "--system-prompt", system_prompt,
   ]
   ```
   Uses `asyncio.create_subprocess_exec` with `env` excluding CLAUDECODE (same pattern as AI._send). Returns stdout decoded and stripped. On timeout, retry once before failing.
  </action>
  <verify>
`python -c "from evals.prompts import SYSTEM_PROMPTS, CONVENTION_REGEXES, validate_response, eval_send; print('OK')"` succeeds from project root. All three convention keys present in both dicts. validate_response handles all four expected types without error on synthetic test strings.
  </verify>
  <done>
Three convention-specific system prompts with fewshot examples exist. Three compiled regexes exist. Validation function handles all expected types including inverse silent-failure detection. eval_send function uses Claude CLI subprocess with --no-session-persistence.
  </done>
</task>

<task type="auto">
  <name>Task 2: Build and run parametrized eval test matrix</name>
  <files>evals/test_convention.py</files>
  <action>
Create `evals/test_convention.py` with a parametrized pytest test that covers the full matrix:

**Constants:**
```python
MODELS = [
    "claude-opus-4-6",
    "claude-sonnet-4-5-20250929",
    "claude-haiku-4-5-20251001",
]

SCENARIOS = [
    ("What is a Graph in bae?", "no_code"),
    ("What's 2**100?", "one_exec"),
    ("Explain how Dep works with an example", "no_exec"),
    ("What variables do I have?", "one_exec"),
    ("Show me how to define a Node, then create one for me", "mixed"),
]

CONVENTIONS = ["fence_annotation", "wrapper_marker", "inverse"]
```

**Test function:**
```python
@pytest.mark.e2e
@pytest.mark.asyncio
@pytest.mark.parametrize("rep", range(3))
@pytest.mark.parametrize("scenario_prompt,expected", SCENARIOS, ids=[s[0][:30] for s in SCENARIOS])
@pytest.mark.parametrize("convention", CONVENTIONS)
@pytest.mark.parametrize("model", MODELS)
async def test_convention_compliance(model, convention, scenario_prompt, expected, rep):
    system_prompt = SYSTEM_PROMPTS[convention]
    response = await eval_send(model, system_prompt, scenario_prompt)
    validate_response(response, convention, expected)
```

This produces 3 models x 3 conventions x 5 scenarios x 3 reps = 135 test cases.

Import from `evals.prompts`. Use generous timeout (30s per call). Each test is independent and uses `--no-session-persistence`, so no session contention.

The scenario prompts are identical across all conventions -- no convention-specific wording. Natural user prompts that don't reference the convention.

Then **run the eval** from the project root:
```bash
pytest evals/ --run-e2e -v --tb=short 2>&1 | tee evals/results.txt
```

If running all 135 at once is too slow, run per-convention:
```bash
pytest evals/ --run-e2e -v --tb=short -k "fence_annotation" 2>&1 | tee evals/results_fence.txt
pytest evals/ --run-e2e -v --tb=short -k "wrapper_marker" 2>&1 | tee evals/results_wrapper.txt
pytest evals/ --run-e2e -v --tb=short -k "inverse" 2>&1 | tee evals/results_inverse.txt
```

Capture ALL output. The raw results are the data artifact this plan produces.
  </action>
  <verify>
Eval results file(s) exist in `evals/`. Results show pass/fail for each test case. At least one convention achieves near-100% or 100% compliance across all models.
  </verify>
  <done>
Full eval matrix executed (135 test cases). Results captured to file. Raw data available for convention selection.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 3: Review eval results and confirm convention winner</name>
  <files>evals/results.txt</files>
  <action>
Eval harness tested all three convention candidates across Opus, Sonnet, and Haiku with 5 scenarios each, 3 repetitions per combination. Results are in `evals/results*.txt`.

Present the results summary to Dzara for review:
1. Review pass/fail counts per convention:
   - `grep -c "PASSED" evals/results_fence.txt`
   - `grep -c "PASSED" evals/results_wrapper.txt`
   - `grep -c "PASSED" evals/results_inverse.txt`
2. Check if any convention achieved 100% (45/45 per convention across all models and reps)
3. If multiple conventions achieved 100%, any is fine -- pick the simplest one
4. If none achieved 100%, examine which scenarios failed and on which models
5. Confirm the winning convention name (fence_annotation, wrapper_marker, or inverse)
  </action>
  <verify>Dzara confirms the winning convention name.</verify>
  <done>Winning convention identified. Resume signal: "Winner: {convention_name}" or description of issues if none achieved 100%.</done>
</task>

</tasks>

<verification>
- `evals/conftest.py` exists with e2e marker registration
- `evals/prompts.py` exists with 3 system prompts, 3 regexes, validator, and eval_send
- `evals/test_convention.py` exists with 135 parametrized test cases
- Eval results captured to file(s) in `evals/`
- Convention winner identified from results data
</verification>

<success_criteria>
- All three conventions tested equally across all three Claude tiers
- Results clearly show compliance rates per convention x model
- Winning convention identified (or data shows none qualify, triggering fallback analysis)
- Eval uses natural prompts with no convention-specific hints
- No session contention (--no-session-persistence used throughout)
</success_criteria>

<output>
After completion, create `.planning/phases/21-execution-convention/21-01-SUMMARY.md`
</output>

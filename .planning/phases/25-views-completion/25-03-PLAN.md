---
phase: 25-views-completion
plan: 03
type: execute
wave: 1
depends_on: []
files_modified:
  - bae/repl/views.py
  - bae/repl/ai.py
  - tests/repl/test_views.py
autonomous: true
gap_closure: true

must_haves:
  truths:
    - "Tool call results display concisely in user view without dumping full file contents"
    - "AI feedback loop still receives full tool output for reasoning"
  artifacts:
    - path: "bae/repl/views.py"
      provides: "UserView tool result summarization"
      contains: "tool_result"
    - path: "bae/repl/ai.py"
      provides: "Summarized display tag for tool calls"
      contains: "tool_summary"
    - path: "tests/repl/test_views.py"
      provides: "Tests for tool call display in UserView"
      contains: "tool_translated"
  key_links:
    - from: "bae/repl/ai.py"
      to: "bae/repl/views.py"
      via: "metadata type=tool_translated with tool_summary field"
      pattern: "tool_summary"
---

<objective>
Fix spammy tool call display in UserView. Tool call translations (R: tags) currently dump entire file contents as [py] channel lines. User should see a concise summary like "read bae/repl/ai.py (513 lines)" while the AI feedback loop retains full output.

Purpose: Close UAT Test 4 gap — tool results are unreadable in user view.
Output: Concise tool result display in UserView, full output preserved for AI.
</objective>

<execution_context>
@/Users/dzaramelcone/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dzaramelcone/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/25-views-completion/25-01-SUMMARY.md
@.planning/phases/25-views-completion/25-02-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add tool_summary metadata to eval loop and summarize tool display in UserView</name>
  <files>bae/repl/ai.py, bae/repl/views.py</files>
  <action>
Two changes needed:

**bae/repl/ai.py (lines 124-133, the tool_results block in the eval loop):**

Generate a human-readable summary for each tool call tag and pass it as metadata. The tag string already contains the tool type and argument (e.g. `<R:bae/repl/ai.py>`). Build a `tool_summary` string from the tag:
- Read tags: `"read {filepath} ({N} lines)"` — count lines in output
- Write tags: use the output as-is (already concise like "Wrote 42 chars to foo.py")
- Edit tags: use the output as-is (already concise like "Replaced lines 5-10 in foo.py")
- Glob tags: `"glob {pattern} ({N} matches)"` — count newlines in output
- Grep tags: `"grep {pattern} ({N} matches)"` — count newlines in output

Pass tool_summary in metadata for the `tool_translated` write:
```python
self._router.write("py", tag, mode="PY",
    metadata={"type": "tool_translated", "label": self._label,
              "tool_summary": summary})
```

Suppress the separate `tool_result` write to the py channel entirely — it's the source of the spam. The full output still goes to `all_outputs` for the AI feedback (`combined` on line 135), which is unaffected.

Remove these lines (128-132 area) that write the raw output to the py channel:
```python
if output:
    self._router.write("py", output, mode="PY",
        metadata={"type": "tool_result", "label": self._label})
```

Add a helper function `_tool_summary(tag: str, output: str) -> str` at module level (near `run_tool_calls`) that parses the tag to determine tool type and generates the summary string. Use a simple approach: the tag format is `<TYPE:arg>`, extract TYPE, then:
- R/Read: `f"read {arg} ({output.count(chr(10)) + 1} lines)"`
- W/Write: output as-is
- E/Edit (with replacement): output as-is
- E/Edit (read-only): `f"read {arg}"`
- G/Glob: `f"glob {arg} ({len(output.splitlines())} matches)"`
- Grep: `f"grep ... ({len(output.splitlines())} matches)"`

**bae/repl/views.py — UserView.render():**

Add handling for `tool_translated` metadata type. When `content_type == "tool_translated"`, render a single concise line using the `tool_summary` from metadata (falling back to the raw tag if no summary). Style it distinctly:

```python
if content_type == "tool_translated":
    summary = meta.get("tool_summary", content)
    text = FormattedText([
        (f"{color} bold", f"[{channel_name}]"),
        ("", " "),
        ("fg:#808080 italic", summary),
    ])
    print_formatted_text(text)
    return
```

Place this check BEFORE the fallback `_render_prefixed` call (i.e. after the ai_exec/ai_exec_result checks).

No change needed for `tool_result` type in UserView because we're no longer writing it to the channel.
  </action>
  <verify>
Run `uv run pytest tests/ -x -q --ignore=tests/test_integration.py` — all tests pass (existing tool tag tests in test_views.py may need updating since tool_result writes are removed).
  </verify>
  <done>
Tool call eval loop writes a single concise [py] line per tool call (with tool_summary in metadata). Full output still feeds back to AI. No raw file contents dumped to display.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add tests for tool call display summarization</name>
  <files>tests/repl/test_views.py</files>
  <action>
Add tests to `tests/repl/test_views.py`:

1. **test_user_view_tool_translated_shows_summary** — UserView with `metadata={"type": "tool_translated", "tool_summary": "read foo.py (42 lines)"}` renders a single FormattedText line containing "read foo.py (42 lines)" in italic style, NOT the raw tag content.

2. **test_user_view_tool_translated_fallback** — UserView with `metadata={"type": "tool_translated"}` (no tool_summary key) falls back to showing the raw content string.

3. **test_tool_summary_read** — Test `_tool_summary` helper: given a Read tag and multi-line output, returns "read path (N lines)".

4. **test_tool_summary_glob** — Test `_tool_summary` helper: given a Glob tag and multi-match output, returns "glob pattern (N matches)".

5. **test_tool_summary_write** — Test `_tool_summary` helper: given a Write tag, returns output as-is.

Import `_tool_summary` from `bae.repl.ai` for direct testing. Match the existing test style: `@patch("bae.repl.views.print_formatted_text")` decorator, mock assertions, FormattedText fragment checking.
  </action>
  <verify>
Run `uv run pytest tests/repl/test_views.py -x -q` — all tests pass including new ones. Run full suite `uv run pytest tests/ -x -q --ignore=tests/test_integration.py` — no regressions.
  </verify>
  <done>
5 new tests covering tool_summary generation and UserView tool_translated rendering. Full test suite green.
  </done>
</task>

</tasks>

<verification>
1. `uv run pytest tests/ -x -q --ignore=tests/test_integration.py` — full suite passes
2. Manual verification: in REPL, trigger AI tool call (e.g. ask AI to read a file), observe [py] line shows concise summary like "read bae/repl/ai.py (513 lines)" instead of dumping file contents
</verification>

<success_criteria>
- Tool call translations display as single concise [py] lines in UserView
- AI feedback loop still receives full tool output (no change to AI behavior)
- No raw file contents appear in user-facing display
- All existing tests pass, 5 new tests added
</success_criteria>

<output>
After completion, create `.planning/phases/25-views-completion/25-03-SUMMARY.md`
</output>

---
phase: 29-observability
plan: 03
type: execute
wave: 2
depends_on: ["29-01"]
files_modified:
  - tests/repl/test_engine.py
  - tests/test_integration.py
autonomous: true

must_haves:
  truths:
    - "10+ concurrent graphs complete without event loop starvation or memory leaks"
    - "Graph events persist to SessionStore and are retrievable via search"
    - "Channel buffer does not grow unbounded during concurrent execution"
  artifacts:
    - path: "tests/repl/test_engine.py"
      provides: "Concurrent stress test, store persistence verification"
      contains: "test_concurrent_graphs"
  key_links:
    - from: "tests/repl/test_engine.py"
      to: "bae/repl/engine.py"
      via: "GraphRegistry.submit() with 10+ concurrent graphs"
      pattern: "concurrent"
---

<objective>
Validate concurrent graph execution at scale and verify the full observability pipeline from engine event emission through store persistence.

Purpose: This is the scaling and integration validation plan. It proves the system handles 10+ concurrent graphs without degradation, and that graph events survive the full channel -> store persistence pipeline.
Output: Stress tests and integration verification that validate Phase 29 success criteria 4 and 5.
</objective>

<execution_context>
@/Users/dzaramelcone/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dzaramelcone/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/29-observability/29-RESEARCH.md
@.planning/phases/29-observability/29-01-SUMMARY.md
@bae/repl/engine.py
@bae/repl/store.py
@bae/repl/channels.py
@tests/repl/test_engine.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Concurrent graph stress test</name>
  <files>tests/repl/test_engine.py</files>
  <action>
  Add a stress test `test_concurrent_graphs_no_starvation` that:

  1. Creates a minimal test Graph with 3 nodes (Start -> Middle -> End) where each node does `await asyncio.sleep(0.01)` in a custom __call__ to simulate work. Use a mock LM that returns pre-built node instances (no real LM calls).

  2. Creates a `GraphRegistry` and a `TaskManager`.

  3. Submits 15 graphs concurrently via `engine.submit()`, each with the mock LM and a notify callback that appends events to a shared list.

  4. Waits for all 15 to complete (poll `engine.active()` with a timeout of 10 seconds).

  5. Asserts:
     - All 15 runs completed with `GraphState.DONE`
     - Each run has `rss_delta_bytes` as an int (not None)
     - No run took more than 5 seconds (event loop wasn't starved)
     - The notify callback was called with at least one "start" and one "complete" event per graph (so at least 30 lifecycle events total)
     - No exceptions were raised

  6. Tears down the TaskManager.

  Use `asyncio.timeout(10)` around the wait loop. If it times out, that itself proves starvation.

  Also add `test_concurrent_no_channel_flood` that:
  1. Creates a ChannelRouter with a mock store (or real SessionStore to tmpdir)
  2. Registers a "graph" channel with that store
  3. Submits 15 graphs with QUIET policy (so only fail/gate events emit)
  4. Verifies the channel buffer length is bounded (< 100 entries for 15 quiet graphs that succeed -- should be near zero since QUIET only emits on failure)
  5. Verify no events were written to the store for QUIET successful graphs (since QUIET + no failures = no events emitted through notify at all)
  </action>
  <verify>`uv run pytest tests/repl/test_engine.py::test_concurrent_graphs_no_starvation tests/repl/test_engine.py::test_concurrent_no_channel_flood -x -q -s` passes.</verify>
  <done>15 concurrent graphs complete within timeout. No starvation detected. QUIET policy prevents channel flooding.</done>
</task>

<task type="auto">
  <name>Task 2: Store persistence verification</name>
  <files>tests/repl/test_engine.py</files>
  <action>
  Add test `test_graph_events_persist_to_store` that:

  1. Creates a real `SessionStore` with a tmpdir database path.
  2. Creates a `ChannelRouter`, registers a "graph" channel with that store.
  3. Creates a `GraphRegistry` and `TaskManager`.
  4. Builds a notify callback that writes to the channel router (matching the real `_make_notify` pattern): `def notify(content, meta=None): router.write("graph", content, mode="GRAPH", metadata=meta)`
  5. Submits a single graph with VERBOSE policy (so all events emit).
  6. Waits for completion.
  7. Queries `store.session_entries()` and filters for channel="graph".
  8. Asserts:
     - At least 2 graph entries exist (start + complete)
     - At least one entry has metadata containing `"event": "start"`
     - At least one entry has metadata containing `"event": "complete"` or type "lifecycle"
     - The content is non-empty for each entry
  9. Queries `store.search("started")` and verifies it returns at least one result (FTS5 works on graph events).
  10. Cleans up: close store, remove tmpdir.

  Also add `test_store_cross_session_graph_events` that:
  1. Creates store, submits graph, waits for completion.
  2. Creates a NEW SessionStore pointing to the same db file (simulating a new session).
  3. Calls `store2.recent()` and verifies graph events from the first session are visible.
  4. This proves INT-02 (cross-session history).
  </action>
  <verify>`uv run pytest tests/repl/test_engine.py::test_graph_events_persist_to_store tests/repl/test_engine.py::test_store_cross_session_graph_events -x -q` passes.</verify>
  <done>Graph events persist through the full channel -> store pipeline. Events are searchable via FTS5. Events visible across sessions.</done>
</task>

</tasks>

<verification>
```bash
uv run pytest tests/repl/test_engine.py -x -q
uv run pytest tests/ -x -q --ignore=tests/test_integration.py
```
All tests pass. Concurrent execution validated. Store persistence verified.
</verification>

<success_criteria>
- 15 concurrent graphs complete within 10 second timeout
- No event loop starvation (each graph completes in < 5s)
- QUIET policy prevents channel flooding for successful graphs
- Graph events persist to SessionStore with structured metadata
- FTS5 search returns graph lifecycle events
- Cross-session graph event visibility confirmed
- Full test suite green
</success_criteria>

<output>
After completion, create `.planning/phases/29-observability/29-03-SUMMARY.md`
</output>

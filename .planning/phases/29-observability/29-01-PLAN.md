---
phase: 29-observability
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - bae/repl/engine.py
  - bae/resolver.py
  - tests/repl/test_engine.py
autonomous: true

must_haves:
  truths:
    - "OutputPolicy enum controls which event types are emitted per graph run"
    - "Dep function durations are captured via DEP_TIMING_KEY timing hook in resolver"
    - "RSS delta is measured before/after graph execution and stored on GraphRun"
    - "Engine _execute emits structured lifecycle events through the notify callback"
  artifacts:
    - path: "bae/repl/engine.py"
      provides: "OutputPolicy enum, RSS measurement, event emission in _execute/_wrap_coro, dep_timings on GraphRun"
      contains: "class OutputPolicy"
    - path: "bae/resolver.py"
      provides: "DEP_TIMING_KEY sentinel for dep timing hook injection"
      contains: "DEP_TIMING_KEY"
  key_links:
    - from: "bae/repl/engine.py"
      to: "bae/resolver.py"
      via: "DEP_TIMING_KEY import and dep_cache injection"
      pattern: "DEP_TIMING_KEY"
    - from: "bae/repl/engine.py"
      to: "notify callback"
      via: "_emit closure gated by OutputPolicy.should_emit()"
      pattern: "should_emit"
---

<objective>
Add the observability instrumentation layer to the graph engine: OutputPolicy for verbosity control, dep timing hooks, RSS memory measurement, and structured event emission during graph execution.

Purpose: This is the plumbing layer that collects and gates all observability data. Plans 02 and 03 consume these events and metrics for display and validation.
Output: Enhanced engine with per-graph output policy, dep timing collection, memory metrics, and structured notify emissions.
</objective>

<execution_context>
@/Users/dzaramelcone/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dzaramelcone/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/29-observability/29-RESEARCH.md
@bae/repl/engine.py
@bae/resolver.py
@tests/repl/test_engine.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: OutputPolicy enum + dep timing hook + RSS + GraphRun extensions</name>
  <files>bae/repl/engine.py, bae/resolver.py</files>
  <action>
  1. In `bae/resolver.py`, add a `DEP_TIMING_KEY = object()` sentinel alongside the existing `LM_KEY` and `GATE_HOOK_KEY`. In `resolve_fields()`, after each dep is resolved in the `asyncio.gather()` results loop, check if `DEP_TIMING_KEY` is in `dep_cache`; if so, call `dep_cache[DEP_TIMING_KEY](callable_name, duration_ns)` with the dep callable's name and wall-clock elapsed time. Wrap each `_resolve_one()` call with `time.perf_counter_ns()` before/after to capture the duration. Import `time` at module top. Keep the parallel gather structure unchanged -- time each individual dep within the gather, not the batch.

  Specifically, modify the gather loop in `resolve_fields()` (around line 477-482):
  - Before: `results = await asyncio.gather(*[_resolve_one(fn, dep_cache, trace) for fn in to_resolve])`
  - After: Create a wrapper async function `_timed_resolve(fn, cache, trace)` that calls `_resolve_one` and returns `(result, duration_ns)`. Use that in the gather. After unzipping results, call the timing hook if present.

  2. In `bae/repl/engine.py`:

  a) Add `OutputPolicy` enum after `GraphState`:
  ```python
  class OutputPolicy(enum.Enum):
      VERBOSE = "verbose"
      NORMAL = "normal"
      QUIET = "quiet"
      SILENT = "silent"

      def should_emit(self, event: str) -> bool:
          if self == OutputPolicy.SILENT:
              return False
          if self == OutputPolicy.QUIET:
              return event in ("fail", "gate", "error")
          if self == OutputPolicy.NORMAL:
              return event in ("start", "complete", "fail", "gate", "error")
          return True  # VERBOSE
  ```

  b) Add `dep_timings` field to `GraphRun`: `dep_timings: list[tuple[str, float]] = field(default_factory=list)` -- list of (dep_name, duration_ms) tuples. Add `rss_delta_bytes: int = 0` field. Add `policy: OutputPolicy = OutputPolicy.NORMAL` field.

  c) Add `_get_rss_bytes()` module-level helper:
  ```python
  def _get_rss_bytes() -> int:
      rss = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
      return rss if sys.platform == "darwin" else rss * 1024
  ```
  Import `resource` and `sys` at module top.

  d) Update `_execute()` to:
  - Accept `policy: OutputPolicy = OutputPolicy.NORMAL` kwarg, store on `run.policy = policy`
  - Build `_emit` closure: `def _emit(event, content, meta=None): if policy.should_emit(event) and notify: notify(content, meta)`
  - Note: this changes the notify signature from `notify(msg: str)` to `notify(content: str, meta: dict | None = None)`. This is backward-compatible because existing callers (gate hook) only pass a string.
  - Build `_dep_timing_hook` closure that appends (name, duration_ms) to `run.dep_timings`
  - Add `DEP_TIMING_KEY: _dep_timing_hook` to `dep_cache`
  - Capture `rss_before = _get_rss_bytes()` before `arun`, `rss_after` after, store delta on `run.rss_delta_bytes`
  - Emit `_emit("start", f"{run.run_id} started", {"type": "lifecycle", "event": "start", "run_id": run.run_id})`
  - Emit `_emit("complete", ...)` after success with elapsed_ms
  - Emit `_emit("fail", ...)` on exception

  e) Update `submit()` to accept `policy` kwarg and pass to `_execute()`.

  f) Update `_wrap_coro()` similarly: accept `policy` kwarg, build `_emit`, measure RSS, emit start/complete/fail events. Add dep_timing_hook to the engine dep_cache contextvar.

  g) Update `submit_coro()` to accept `policy` kwarg and pass to `_wrap_coro()`.

  h) Update the import line to include `DEP_TIMING_KEY`:
  `from bae.resolver import DEP_TIMING_KEY, GATE_HOOK_KEY, LM_KEY, _engine_dep_cache`

  i) Update `_make_gate_hook` notify call to match new signature: `notify(f"[{g.gate_id}] {g.node_type}.{g.schema_display}", {"type": "gate", "run_id": run.run_id, "gate_id": g.gate_id})` (second arg is now metadata dict).
  </action>
  <verify>
  `uv run python -c "from bae.repl.engine import OutputPolicy, GraphRun; p = OutputPolicy.NORMAL; assert p.should_emit('start'); assert not p.should_emit('transition'); r = GraphRun('g1', None); assert r.dep_timings == []; assert r.rss_delta_bytes == 0; print('OK')"` prints OK.
  `uv run python -c "from bae.resolver import DEP_TIMING_KEY; assert DEP_TIMING_KEY is not None; print('OK')"` prints OK.
  </verify>
  <done>OutputPolicy enum gates events by level. GraphRun has dep_timings and rss_delta_bytes fields. DEP_TIMING_KEY sentinel exists in resolver. Engine _execute/_wrap_coro emit structured events and collect metrics.</done>
</task>

<task type="auto">
  <name>Task 2: Update tests for engine instrumentation</name>
  <files>tests/repl/test_engine.py</files>
  <action>
  Update existing engine tests and add new tests for the observability instrumentation:

  1. Update any existing tests that call `submit()` or `submit_coro()` where the notify callback signature changed. The old signature was `notify(msg: str)`. The new signature is `notify(content: str, meta: dict | None = None)`. Update mock notify functions to accept both args.

  2. Add test `test_output_policy_gating`: Create OutputPolicy instances for each level. Assert VERBOSE.should_emit returns True for all events (start, complete, fail, transition, gate, error). Assert NORMAL.should_emit returns True for start/complete/fail/gate/error but False for transition. Assert QUIET returns True only for fail/gate/error. Assert SILENT returns False for everything.

  3. Add test `test_dep_timing_collection`: Create a mock Graph with a Dep-annotated field that has an async dep function with a small sleep (0.01s). Submit via engine with a mock LM. After completion, verify `run.dep_timings` has at least one entry and the duration_ms is > 0.

  4. Add test `test_rss_delta_recorded`: Submit a graph and verify `run.rss_delta_bytes` is an int (may be 0 on macOS high-water mark -- just verify the field exists and is numeric).

  5. Add test `test_notify_receives_metadata`: Submit a graph with a notify callback that captures (content, meta) pairs. Verify at least one call has meta dict with "type": "lifecycle" and "event" in ("start", "complete").

  6. Run: `uv run pytest tests/repl/test_engine.py -x -q` -- all tests pass.
  </action>
  <verify>`uv run pytest tests/repl/test_engine.py -x -q` passes with 0 failures.</verify>
  <done>All engine tests pass including new observability instrumentation tests. No regressions in existing tests.</done>
</task>

</tasks>

<verification>
```bash
uv run pytest tests/repl/test_engine.py -x -q
uv run pytest tests/ -x -q --ignore=tests/test_integration.py
```
All tests pass. OutputPolicy, dep timing, and RSS measurement are functional.
</verification>

<success_criteria>
- OutputPolicy.should_emit() correctly gates events by verbosity level
- DEP_TIMING_KEY hook captures per-dep durations in resolver
- GraphRun.dep_timings populated after execution
- GraphRun.rss_delta_bytes populated after execution
- Engine _execute/_wrap_coro emit structured lifecycle events through notify
- Full test suite green
</success_criteria>

<output>
After completion, create `.planning/phases/29-observability/29-01-SUMMARY.md`
</output>

---
phase: 29-observability
plan: 02
type: execute
wave: 2
depends_on: ["29-01"]
files_modified:
  - bae/repl/graph_commands.py
  - bae/repl/views.py
  - bae/repl/shell.py
  - tests/repl/test_graph_commands.py
  - tests/repl/test_views.py
autonomous: true

must_haves:
  truths:
    - "Graph lifecycle events render with [graph:run_id] prefix in UserView"
    - "DebugView shows timing/memory metadata on graph events"
    - "The debug command shows asyncio call graph for a running graph task"
    - "Enhanced inspect shows dep timings and RSS delta alongside node timings"
    - "Lifecycle notifications appear inline regardless of current mode"
  artifacts:
    - path: "bae/repl/graph_commands.py"
      provides: "debug command, enhanced inspect, output policy on run, updated notify wiring"
      contains: "_cmd_debug"
    - path: "bae/repl/views.py"
      provides: "Graph lifecycle event rendering in UserView and DebugView"
      contains: "lifecycle"
    - path: "bae/repl/shell.py"
      provides: "Wiring of enhanced notify callback for graph events"
  key_links:
    - from: "bae/repl/graph_commands.py"
      to: "bae/repl/engine.py"
      via: "OutputPolicy import, policy kwarg on submit/submit_coro"
      pattern: "OutputPolicy"
    - from: "bae/repl/views.py"
      to: "bae/repl/channels.py"
      via: "ViewFormatter protocol render() for graph channel events"
      pattern: "lifecycle"
---

<objective>
Wire the observability events from Plan 01 through to display: update graph commands to support output policy and the debug command, enhance views to render graph lifecycle events appropriately, and update the notify callback to carry structured metadata.

Purpose: This connects the engine instrumentation to the user-facing display layer so Dzara sees lifecycle events inline and can debug stuck graphs.
Output: Working graph event display in all views, debug command, enhanced inspect with dep/memory data.
</objective>

<execution_context>
@/Users/dzaramelcone/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dzaramelcone/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/29-observability/29-RESEARCH.md
@.planning/phases/29-observability/29-01-SUMMARY.md
@bae/repl/graph_commands.py
@bae/repl/views.py
@bae/repl/shell.py
@bae/repl/engine.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Graph commands -- debug, enhanced inspect, output policy, notify update</name>
  <files>bae/repl/graph_commands.py, bae/repl/shell.py</files>
  <action>
  1. In `bae/repl/graph_commands.py`:

  a) Update `_make_notify()` to match the new notify signature `(content: str, meta: dict | None = None)`:
  ```python
  def _make_notify(shell):
      def notify(content, meta=None):
          if meta and meta.get("type") == "gate":
              if getattr(shell, "shush_gates", False):
                  return
          shell.router.write("graph", content, mode="GRAPH", metadata=meta)
      return notify
  ```
  The old `_make_notify` only checked shush for all messages. The new one checks the metadata type -- only gate events respect shush. Lifecycle events always flow through (they are gated by OutputPolicy in the engine).

  b) Update `_attach_done_callback()` to include elapsed time in the done/failed message:
  ```python
  def _on_done(task, _run=run):
      elapsed_ms = (_run.ended_ns - _run.started_ns) / 1_000_000
      elapsed_s = elapsed_ms / 1000
      if task.cancelled():
          shell.router.write(
              "graph", f"{_run.run_id} cancelled", mode="GRAPH",
              metadata={"type": "lifecycle", "event": "cancel", "run_id": _run.run_id},
          )
      elif task.exception() is not None:
          shell.router.write(
              "graph", f"{_run.run_id} failed: {_run.error}", mode="GRAPH",
              metadata={"type": "error", "run_id": _run.run_id, "elapsed_ms": elapsed_ms},
          )
      else:
          shell.router.write(
              "graph", f"{_run.run_id} done ({elapsed_s:.1f}s)", mode="GRAPH",
              metadata={"type": "lifecycle", "event": "complete", "run_id": _run.run_id, "elapsed_ms": elapsed_ms},
          )
  ```

  c) Update `_cmd_run()` to parse `--verbose`, `--quiet`, `--silent` flags from the arg string and pass the corresponding `OutputPolicy` to `submit()`/`submit_coro()`:
  ```python
  from bae.repl.engine import GraphState, OutputPolicy
  ```
  Parse flags: split arg, look for --verbose/--quiet/--silent, remove from arg before eval, map to OutputPolicy. Default is NORMAL. Pass `policy=policy` to submit/submit_coro.

  d) Add `_cmd_debug()` handler for stuck graph debugging:
  ```python
  async def _cmd_debug(arg: str, shell) -> None:
      """Show async call graph for a running graph's task."""
      arg = arg.strip()
      if not arg:
          shell.router.write("graph", "usage: debug <id>", mode="GRAPH")
          return
      run = shell.engine.get(arg)
      if run is None:
          shell.router.write("graph", f"no run {arg}", mode="GRAPH")
          return
      if run.state not in (GraphState.RUNNING, GraphState.WAITING):
          shell.router.write("graph", f"{arg} not active ({run.state.value})", mode="GRAPH")
          return
      import asyncio as _aio
      for tt in shell.tm.active():
          if tt.name.startswith(f"graph:{run.run_id}:"):
              try:
                  formatted = _aio.format_call_graph(tt.task)
                  shell.router.write(
                      "graph", formatted, mode="GRAPH",
                      metadata={"type": "debug", "run_id": run.run_id},
                  )
              except Exception as e:
                  shell.router.write("graph", f"debug error: {e}", mode="GRAPH")
              return
      shell.router.write("graph", f"{arg}: task not found", mode="GRAPH")
  ```
  Register "debug" in the handlers dict.

  e) Enhance `_cmd_inspect()` to show dep timings and RSS delta after the trace section:
  - After the trace/timings block, if `run.dep_timings` is non-empty, append a "Dep timings:" section with each (name, ms) pair.
  - If `run.rss_delta_bytes != 0`, append `RSS delta: {delta_mb:.1f}MB`.

  2. In `bae/repl/shell.py`:
  - No changes needed beyond what Plan 01 already provides. The notify callback in `_make_notify()` (in graph_commands.py) already writes to `shell.router.write("graph", ...)` which auto-persists via the store pipeline. Lifecycle events emitted from the engine will flow through the same notify -> router.write -> channel -> store path.
  </action>
  <verify>
  `uv run python -c "from bae.repl.graph_commands import _make_notify; print('OK')"` prints OK.
  `uv run python -c "from bae.repl.engine import OutputPolicy; print('OK')"` prints OK.
  </verify>
  <done>Debug command registered. Inspect shows dep timings + RSS delta. Output policy flags parsed on `run` command. Notify callback carries structured metadata. Done callback includes elapsed time.</done>
</task>

<task type="auto">
  <name>Task 2: View rendering for graph lifecycle events + tests</name>
  <files>bae/repl/views.py, tests/repl/test_graph_commands.py, tests/repl/test_views.py</files>
  <action>
  1. In `bae/repl/views.py`:

  a) In `UserView.render()`, add handling for graph lifecycle events. After the `tool_translated` block and before the fall-through `_render_prefixed`, add:
  ```python
  if content_type == "lifecycle" and channel_name == "graph":
      label = f"[{channel_name}:{meta.get('run_id', '')}]"
      event = meta.get("event", "")
      # Color code: start=dim, complete=green, fail=red, cancel=yellow
      style_map = {"start": "fg:#808080", "complete": "fg:#87ff87", "fail": "fg:red", "cancel": "fg:ansiyellow"}
      style = style_map.get(event, "")
      print_formatted_text(FormattedText([
          (f"{color} bold", label),
          ("", " "),
          (style, content),
      ]))
      return
  ```

  b) No changes needed for `DebugView` -- it already renders all metadata as key=value pairs. The new metadata keys (event, elapsed_ms, run_id) will display automatically.

  c) No changes needed for `AISelfView` -- lifecycle events will render with the "lifecycle" tag.

  2. In `tests/repl/test_views.py`, add a test `test_user_view_lifecycle_graph_event` that:
  - Creates a UserView instance
  - Calls render with channel_name="graph", content="g1 done (2.3s)", metadata={"type": "lifecycle", "event": "complete", "run_id": "g1"}
  - Captures output and verifies it contains "[graph:g1]" and "done"

  3. In `tests/repl/test_graph_commands.py`, add/update tests:
  - `test_notify_carries_metadata`: mock shell with router, create notify via `_make_notify`, call it with meta dict, verify router.write received the metadata.
  - `test_cmd_debug_not_running`: call _cmd_debug with a completed run ID, verify it reports "not active".
  - `test_inspect_shows_dep_timings`: create a GraphRun with dep_timings populated, mock shell, call _cmd_inspect, verify output contains dep timing info.

  4. Run: `uv run pytest tests/repl/test_views.py tests/repl/test_graph_commands.py -x -q`
  </action>
  <verify>`uv run pytest tests/repl/test_views.py tests/repl/test_graph_commands.py -x -q` passes.</verify>
  <done>UserView renders graph lifecycle events with color-coded [graph:run_id] prefix. DebugView displays timing/memory metadata. Debug command works for active graphs. Tests pass.</done>
</task>

</tasks>

<verification>
```bash
uv run pytest tests/repl/test_views.py tests/repl/test_graph_commands.py -x -q
uv run pytest tests/ -x -q --ignore=tests/test_integration.py
```
All tests pass. Graph events render correctly in all views.
</verification>

<success_criteria>
- UserView renders graph lifecycle events with [graph:run_id] prefix and color coding
- DebugView shows all metadata keys on graph events
- `debug <id>` command displays asyncio call graph for running graphs
- `inspect <id>` shows dep timings and RSS delta
- `run <expr> --verbose` sets VERBOSE output policy
- Lifecycle events appear through channel regardless of current mode
- All tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/29-observability/29-02-SUMMARY.md`
</output>

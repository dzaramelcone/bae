---
phase: 26-engine-foundation
plan: 04
type: execute
wave: 1
depends_on: []
files_modified:
  - bae/repl/engine.py
  - bae/repl/shell.py
  - tests/repl/test_engine.py
autonomous: true
gap_closure: true

must_haves:
  truths:
    - "GraphRun stores exception when graph fails"
    - "engine.get('g1') on a failed run shows the error message"
    - "GRAPH mode surfaces error through [graph] channel when graph fails"
    - "Graphs run to completion with timing data when correct kwargs are passed"
  artifacts:
    - path: "bae/repl/engine.py"
      provides: "GraphRun.error field, exception storage in _execute"
      contains: "error"
    - path: "bae/repl/shell.py"
      provides: "Error callback on graph tasks, fixed _run_graph kwargs"
      contains: "graph.*error"
    - path: "tests/repl/test_engine.py"
      provides: "Tests for error storage on failed runs"
      contains: "error"
  key_links:
    - from: "bae/repl/engine.py"
      to: "GraphRun.error"
      via: "_execute except block stores exception"
      pattern: "run\\.error"
    - from: "bae/repl/shell.py"
      to: "bae/repl/engine.py"
      via: "_run_graph passes graph-specific field names"
      pattern: "engine\\.submit"
---

<objective>
Fix the silent graph failure pipeline: (1) GraphRun stores exceptions so failures are diagnosable, (2) GRAPH mode surfaces errors through the [graph] channel, and (3) _run_graph stops passing wrong kwargs to arun.

Purpose: Close UAT gap "Graphs run to completion with timing data captured" -- currently all runs fail silently because of wrong kwargs AND no error visibility.
Output: Patched engine.py, shell.py with tests
</objective>

<execution_context>
@/Users/dzaramelcone/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dzaramelcone/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/26-engine-foundation/26-01-SUMMARY.md
@.planning/phases/26-engine-foundation/26-02-SUMMARY.md
@.planning/debug/engine-silent-fail.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: GraphRun error field and exception storage</name>
  <files>bae/repl/engine.py, tests/repl/test_engine.py</files>
  <action>
**A) bae/repl/engine.py -- Add error field to GraphRun:**

Add `error: str = ""` to the GraphRun dataclass (after `ended_ns`):

```python
@dataclass
class GraphRun:
    run_id: str
    graph: Graph
    state: GraphState = GraphState.RUNNING
    node_timings: list[NodeTiming] = field(default_factory=list)
    current_node: str = ""
    started_ns: int = field(default_factory=time.perf_counter_ns)
    ended_ns: int = 0
    error: str = ""  # ADD THIS -- stores exception message on failure
```

**B) bae/repl/engine.py -- Store exception in _execute:**

In the `_execute` method, the `except Exception` block (currently line 113-114) sets `run.state = GraphState.FAILED` then `raise`. Before the raise, store the error:

```python
except Exception as e:
    run.state = GraphState.FAILED
    run.error = f"{type(e).__name__}: {e}"
    raise
```

Change `except Exception:` to `except Exception as e:` to capture the exception object.

**C) tests/repl/test_engine.py -- Test error storage:**

Add a test to TestGraphRegistry:

```python
async def test_failed_run_stores_error(self, registry, tm):
    """Failed run stores exception message in run.error."""
    from bae.graph import Graph

    graph = Graph(start=Start)
    run = registry.submit(graph, tm, lm=FailingLM(), text="hello")
    for tt in list(tm._tasks.values()):
        try:
            await tt.task
        except Exception:
            pass
    assert run.state == GraphState.FAILED
    assert "LM exploded" in run.error
    assert "RuntimeError" in run.error
```

Also add a test verifying successful runs have empty error:

```python
async def test_successful_run_has_no_error(self, registry, tm, mock_lm):
    """Successful run has empty error string."""
    from bae.graph import Graph

    graph = Graph(start=Start)
    run = registry.submit(graph, tm, lm=mock_lm, text="hello")
    for tt in list(tm._tasks.values()):
        try:
            await tt.task
        except Exception:
            pass
    assert run.state == GraphState.DONE
    assert run.error == ""
```
  </action>
  <verify>
Run: `uv run pytest tests/repl/test_engine.py -x -q` -- all tests pass including new error storage tests.
  </verify>
  <done>
GraphRun.error populated with "ExceptionType: message" on failure, empty string on success. Two new tests verify this.
  </done>
</task>

<task type="auto">
  <name>Task 2: GRAPH mode error surfacing and kwarg fix</name>
  <files>bae/repl/shell.py</files>
  <action>
Two fixes in shell.py `_run_graph`:

**A) Stop passing raw text as kwargs (root cause of all graph failures):**

Currently line 343: `run = self.engine.submit(graph, self.tm, lm=self._lm, text=text)`

The `text=text` flows through engine._execute as **kwargs into graph.arun(), but the start node may not have a `text` field. The ootd graph for example needs `user_info` and `user_message`, not `text`. This causes an immediate TypeError before any node runs.

GRAPH mode should NOT try to map REPL input to graph kwargs. That's Phase 27's job (Graph Mode commands like `run graph(field=val)`). For now, submit with NO extra kwargs:

```python
run = self.engine.submit(graph, self.tm, lm=self._lm)
```

The user provides field values through the namespace (e.g. `graph = Graph(start=MyNode)` in PY mode, then switch to GRAPH and type anything to submit). For graphs that need input fields, the user should set up the graph call in PY mode for now. GRAPH mode just submits.

Note: this means graphs with required start-node fields will fail with a clear TypeError ("X requires: field1, field2") stored in run.error. That's correct behavior -- Phase 27 will add the `run expr(field=val)` syntax.

**B) Surface background graph errors through [graph] channel:**

Currently, graph execution errors are completely invisible because they happen in a background asyncio.Task. Add an error callback.

After `engine.submit()`, register a done callback on the TaskManager task to surface errors. Refactor `_run_graph` to:

```python
async def _run_graph(self, text: str) -> None:
    """GRAPH mode: graph execution via engine."""
    graph = self.namespace.get("graph")
    if not graph:
        self.router.write("graph", "(no graph in namespace)", mode="GRAPH")
        return
    try:
        run = self.engine.submit(graph, self.tm, lm=self._lm)
        self.router.write(
            "graph", f"submitted {run.run_id}", mode="GRAPH",
            metadata={"type": "lifecycle", "run_id": run.run_id},
        )
        # Find the task we just submitted and add error surfacing callback
        active = self.tm.active()
        for tt in active:
            if tt.name.startswith(f"graph:{run.run_id}:"):
                def _on_graph_done(task, _run=run):
                    if task.cancelled():
                        self.router.write(
                            "graph", f"{_run.run_id} cancelled", mode="GRAPH",
                            metadata={"type": "lifecycle", "run_id": _run.run_id},
                        )
                    elif task.exception() is not None:
                        self.router.write(
                            "graph", f"{_run.run_id} failed: {_run.error}", mode="GRAPH",
                            metadata={"type": "error", "run_id": _run.run_id},
                        )
                    else:
                        self.router.write(
                            "graph", f"{_run.run_id} done", mode="GRAPH",
                            metadata={"type": "lifecycle", "run_id": _run.run_id},
                        )
                tt.task.add_done_callback(_on_graph_done)
                break
    except Exception:
        tb = traceback.format_exc()
        self.router.write("graph", tb.rstrip("\n"), mode="GRAPH", metadata={"type": "error"})
```

The callback reads `run.error` (set by engine._execute before re-raising) to get the actual error message. This surfaces it through the [graph] channel so Dzara sees what went wrong.
  </action>
  <verify>
Run: `uv run pytest tests/ -x -q --ignore=tests/test_integration.py` -- full suite passes.
Grep: `grep -n "text=text" bae/repl/shell.py` should NOT match in _run_graph (the old kwarg pass-through is removed).
Grep: `grep -n "run.error" bae/repl/shell.py` should match (error surfacing callback).
  </verify>
  <done>
GRAPH mode submits graphs without passing raw text as kwargs. Background graph errors surface through [graph] channel with the stored error message. Graphs with no required start-node fields run to completion; graphs with required fields fail with a clear error visible in the channel.
  </done>
</task>

</tasks>

<verification>
- `uv run pytest tests/repl/test_engine.py -x -q` passes (all engine tests including new error tests)
- `uv run pytest tests/ -x -q --ignore=tests/test_integration.py` passes (full suite, zero regressions)
- `grep "error" bae/repl/engine.py` shows GraphRun.error field and storage in _execute
- `grep "text=text" bae/repl/shell.py` does NOT appear in _run_graph
- `grep "run.error" bae/repl/shell.py` shows error surfacing callback
</verification>

<success_criteria>
1. GraphRun.error stores "ExceptionType: message" on failure, empty on success
2. _run_graph no longer passes text=text through to arun
3. Graph completion/failure/cancellation events appear in [graph] channel
4. All existing tests pass with zero regressions
</success_criteria>

<output>
After completion, create `.planning/phases/26-engine-foundation/26-04-SUMMARY.md`
</output>

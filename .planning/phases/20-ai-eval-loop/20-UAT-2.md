---
status: complete
phase: 20-ai-eval-loop
source: [20-04-SUMMARY.md, 20-05-SUMMARY.md]
started: 2026-02-14T14:00:00Z
updated: 2026-02-14T14:45:00Z
---

## Current Test

[testing complete]

## Tests

### 1. AI answers naturally (no runaway code loops)
expected: Ask a general question in NL mode. AI responds in natural language without code blocks. No eval loop fires.
result: pass

### 2. AI writes code when appropriate
expected: In NL mode, ask "what's 2**100?" or "show me what's in my store". The AI SHOULD write a code block. The code executes, output displays on [py], and result feeds back for a final NL summary.
result: issue
reported: "AI hallucinates tool calls — generates fake <tool_use> XML blocks (Grep, Read) with fabricated file paths (.bae/store/sessions/*.json) and invented results. The store is a SQLite DB queried via store.search() in the namespace, not a file tree. AI doesn't know it has no tools, only Python code execution."
severity: major

### 3. Unawaited coroutines handled gracefully
expected: In PY mode, type `[asyncio.sleep(30) for _ in range(20)]` (no await). REPL should show a warning like `<20 unawaited coroutines>` instead of crashing. REPL stays alive.
result: pass

### 4. Session indicators on AI channel
expected: In NL mode, type `@2 hello`. AI output should show `[ai:2]` prefix (not just `[ai]`). Switch back with `@1 hi` — output shows `[ai:1]`.
result: pass

### 5. Eval loop output visible to user
expected: Ask the AI something that requires computation. When the eval loop executes code, both the code AND its output should appear on [py] — user sees what ran and what it produced.
result: issue
reported: "Output tee works mechanically — code and results visible on [py:4]. But display is redundant: code appears in AI markdown response AND again as [py] lines. User sees code 2-3x. Needs collapsed/abbreviated code input with just results beneath. What user sees vs what model gets fed back should be different views. UI design question for GWT stream work."
severity: cosmetic

### 6. Cross-session memory still works
expected: Exit and re-launch cortex. Send an AI prompt. AI shows awareness of previous session context.
result: pass

## Summary

total: 6
passed: 4
issues: 2
pending: 0
skipped: 0

## Gaps

- truth: "AI uses Python code fences for namespace inspection, not fabricated tool calls"
  status: deferred
  reason: "AI generates fake <tool_use> XML with fabricated file paths and invented results. Claude CLI subprocess is trained to use tools; prompt needs explicit 'you have NO tools, only Python code execution' constraint. Deferred to prompt engineering iteration."
  severity: major
  test: 2
  root_cause: "Claude CLI model defaults to tool-use patterns; system prompt doesn't explicitly prohibit tool XML"
  artifacts: [bae/repl/ai_prompt.md]
  missing: ["Explicit no-tools constraint in system prompt"]
  debug_session: ""

- truth: "Eval loop output shown cleanly without redundant display"
  status: deferred
  reason: "Output tee works but code appears in AI markdown response AND again as [py] lines. Needs differentiated views — what user sees vs what model gets fed back. Deferred to GWT-inspired stream UX design effort."
  severity: cosmetic
  test: 5
  root_cause: "Same content rendered in both AI response channel and eval output tee"
  artifacts: [bae/repl/ai.py, bae/repl/channels.py]
  missing: ["Collapsed/abbreviated code input with results beneath", "Separate user view vs model feedback view"]
  debug_session: ""

---
phase: 20-ai-eval-loop
plan: 03
type: execute
wave: 2
depends_on: ["20-01", "20-02"]
files_modified:
  - bae/repl/ai.py
  - bae/repl/shell.py
  - tests/repl/test_ai.py
  - tests/repl/test_ai_integration.py
autonomous: true

must_haves:
  truths:
    - "AI-generated Python code blocks are automatically extracted, executed in the REPL namespace, and results fed back to the AI"
    - "The eval loop handles coroutines from async_exec by awaiting them inline"
    - "The eval loop has an iteration limit preventing infinite recursion"
    - "N concurrent AI prompts from PY REPL each route namespace mutations back to their correct sessions"
  artifacts:
    - path: "bae/repl/ai.py"
      provides: "Eval loop inside AI.__call__ that extracts, executes, and feeds back code"
      contains: "extract_code"
    - path: "tests/repl/test_ai.py"
      provides: "Unit tests for eval loop iteration, limit, coroutine handling"
    - path: "tests/repl/test_ai_integration.py"
      provides: "Integration test for concurrent session routing"
  key_links:
    - from: "bae/repl/ai.py"
      to: "bae/repl/exec.async_exec"
      via: "eval loop calls async_exec for each extracted code block"
      pattern: "async_exec"
    - from: "bae/repl/ai.py"
      to: "AI.extract_code"
      via: "eval loop extracts code blocks from AI response"
      pattern: "extract_code.*response"
    - from: "bae/repl/ai.py"
      to: "AI._send or subprocess"
      via: "eval loop feeds execution results back as next prompt"
      pattern: "feedback.*send\\|communicate"
---

<objective>
Implement the AI eval loop: after AI responds, extract Python code blocks, execute them in the REPL namespace, capture output, and feed results back as the next AI prompt. Loop until no code blocks or iteration limit.

Purpose: SC1 (auto extract-execute-feedback) and SC4 (concurrent session routing tested) are the core agent capability. The eval loop turns the AI from a conversationalist into an agent that can act.
Output: AI.__call__ iterates: respond -> extract code -> execute -> feed back -> respond, up to max_eval_iters.
</objective>

<execution_context>
@/Users/dzaramelcone/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dzaramelcone/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/20-ai-eval-loop/20-RESEARCH.md
@.planning/phases/20-ai-eval-loop/20-01-SUMMARY.md
@.planning/phases/20-ai-eval-loop/20-02-SUMMARY.md

@bae/repl/ai.py
@bae/repl/exec.py
@bae/repl/shell.py
@tests/repl/test_ai.py
@tests/repl/test_ai_integration.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Eval loop in AI.__call__</name>
  <files>bae/repl/ai.py, tests/repl/test_ai.py</files>
  <action>
1. Add `max_eval_iters` parameter to `AI.__init__` (default `5`). Store as `self._max_eval_iters`.

2. Extract the subprocess call logic from `AI.__call__` into a private `_send(prompt: str) -> str` method. This method handles: building cmd args, env setup, subprocess exec, timeout, cancellation, error handling, and returns the response string. It increments `_call_count` and handles `--session-id` vs `--resume` logic. It does NOT write to the router — that stays in `__call__`.

3. Rewrite `AI.__call__` as the eval loop:
   ```
   async def __call__(self, prompt: str) -> str:
       context = _build_context(self._namespace)
       # Prepend cross-session context on first call (from Plan 02)
       ...
       full_prompt = f"{context}\n\n{prompt}" if context else prompt
       response = await self._send(full_prompt)
       await asyncio.sleep(0)  # cancellation checkpoint
       self._router.write("ai", response, mode="NL", metadata={"type": "response", "label": self._label})

       for _ in range(self._max_eval_iters):
           blocks = self.extract_code(response)
           if not blocks:
               break

           results = []
           for code in blocks:
               try:
                   result, captured = await async_exec(code, self._namespace)
                   # If async_exec returned a coroutine, await it inline
                   # (eval loop needs the result, unlike shell PY dispatch)
                   if asyncio.iscoroutine(result):
                       result = await result
                   output = captured
                   if result is not None:
                       output += repr(result)
                   results.append(output or "(no output)")
               except Exception:
                   results.append(traceback.format_exc())
               self._router.write("py", code, mode="PY", metadata={"type": "ai_exec", "label": self._label})

           feedback = "\n".join(f"[Block {i+1} output]\n{r}" for i, r in enumerate(results))
           await asyncio.sleep(0)  # cancellation checkpoint
           response = await self._send(feedback)
           await asyncio.sleep(0)  # cancellation checkpoint
           self._router.write("ai", response, mode="NL", metadata={"type": "response", "label": self._label})

       return response
   ```

4. Import `async_exec` and `traceback` at the top of ai.py. Add `from bae.repl.exec import async_exec`.

5. Exception handling in eval loop: catch ALL exceptions from async_exec EXCEPT `CancelledError` (which should propagate to kill the whole loop). `KeyboardInterrupt` and `SystemExit` also propagate. Feed traceback string back to AI as error output so it can self-correct.

6. Add unit tests in `tests/repl/test_ai.py`:
   - `test_eval_loop_no_code_returns_response`: mock _send to return plain text. Verify __call__ returns it without looping.
   - `test_eval_loop_extracts_and_executes`: mock _send to return code block on first call, plain text on second. Verify async_exec was called with the code.
   - `test_eval_loop_feeds_back_output`: mock _send, mock async_exec to return (42, ""). Verify second _send call receives "[Block 1 output]\n42".
   - `test_eval_loop_iteration_limit`: mock _send to always return code blocks. Verify it stops after max_eval_iters.
   - `test_eval_loop_awaits_coroutine`: mock async_exec to return a coroutine. Verify coroutine is awaited inline and result is fed back.
   - `test_eval_loop_catches_exec_error`: mock async_exec to raise ValueError. Verify traceback is fed back to AI (not raised).
   - `test_eval_loop_cancellation_propagates`: verify CancelledError from async_exec propagates (not caught in eval loop).
  </action>
  <verify>
`uv run pytest tests/repl/test_ai.py -v` — all tests pass, including 7 new eval loop tests.
  </verify>
  <done>AI.__call__ implements extract-execute-feedback loop. Code blocks auto-execute in REPL namespace. Results (or errors) feed back as next AI prompt. Iteration limit prevents infinite loops. Coroutines awaited inline.</done>
</task>

<task type="auto">
  <name>Task 2: Concurrent session routing test</name>
  <files>tests/repl/test_ai_integration.py, bae/repl/shell.py</files>
  <action>
1. Add integration test in `tests/repl/test_ai_integration.py` for concurrent session routing (SC4):
   - Create a CortexShell (or construct the components manually: store, router, namespace, tm)
   - Create two AI instances with different labels ("1" and "2"), sharing the same namespace
   - Mock `_send` on both to return code that mutates the namespace (e.g., session 1 sets `ns["from_s1"] = True`, session 2 sets `ns["from_s2"] = True`)
   - Run both `ai1("prompt")` and `ai2("prompt")` concurrently via `asyncio.gather`
   - Verify both namespace mutations exist
   - Verify router.write calls include correct labels in metadata

2. Add integration test for @N prefix routing in shell._run_nl:
   - Create a CortexShell
   - Mock AI.__call__ to avoid subprocess
   - Call `shell._run_nl("@2 hello")` — verify shell._active_session is "2" and a new session was created
   - Call `shell._run_nl("follow up")` — verify it uses session "2" (sticky)
   - Call `shell._run_nl("@1 back to first")` — verify it switches back to session "1"

3. Ensure `_run_nl` in shell.py correctly handles the @N prefix parsing implemented in Plan 02. If Plan 02's implementation needs adjustment for the eval loop (e.g., the task name should reflect the session label), update the task submission in `_dispatch`:
   - `self.tm.submit(self._run_nl(text), name=f"ai:{label}:{clean_text[:30]}", mode="nl")`
   where label is extracted from @N prefix before dispatching.

4. Run the full test suite to verify no regressions.
  </action>
  <verify>
`uv run pytest tests/repl/test_ai_integration.py -v` — integration tests pass.
`uv run pytest tests/repl/ -v` — all repl tests pass, zero regressions.
  </verify>
  <done>Concurrent AI sessions from PY mode route namespace mutations to correct sessions. @N prefix in NL mode creates/switches sessions. Full test coverage for multi-session eval loop.</done>
</task>

</tasks>

<verification>
- `uv run pytest tests/repl/ -v` — full repl suite passes
- AI.__call__ implements eval loop: extract code -> execute -> feed back -> repeat
- Iteration limit (default 5) prevents infinite loops
- Coroutines from async_exec are awaited inline (not fire-and-forget)
- Exceptions from code execution are caught and fed back to AI as error output
- CancelledError propagates correctly (loop terminates)
- Concurrent AI sessions route correctly (tested)
- @N prefix routing works in NL mode (tested)
</verification>

<success_criteria>
SC1: AI-generated Python code blocks are automatically extracted, executed in the REPL namespace, and results fed back to the correct AI session
SC4: N concurrent AI prompts from PY REPL each route namespace mutations back to their correct sessions (tested)
</success_criteria>

<output>
After completion, create `.planning/phases/20-ai-eval-loop/20-03-SUMMARY.md`
</output>

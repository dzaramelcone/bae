---
phase: 20-ai-eval-loop
plan: 04
type: execute
wave: 1
depends_on: []
files_modified:
  - bae/repl/shell.py
  - bae/repl/ai_prompt.md
  - tests/repl/test_exec.py
autonomous: true
gap_closure: true

must_haves:
  truths:
    - "PY mode handles unawaited coroutines gracefully without crashing the REPL"
    - "AI answers naturally in NL and only writes code when computation/inspection is needed"
    - "AI responds concisely without runaway code generation loops"
  artifacts:
    - path: "bae/repl/shell.py"
      provides: "_contains_coroutines helper and safe dispatch for coroutine collections"
      contains: "_contains_coroutines"
    - path: "bae/repl/ai_prompt.md"
      provides: "Prompt that distinguishes NL responses from tool-use code blocks"
      contains: "When to write code"
  key_links:
    - from: "bae/repl/shell.py"
      to: "_dispatch PY handler"
      via: "_contains_coroutines check before repr()"
      pattern: "_contains_coroutines\\(result\\)"
---

<objective>
Fix the BLOCKER unawaited-coroutines crash and the runaway AI code generation loop.

Purpose: These are the two highest-severity UAT failures. The coroutine crash is a blocker (REPL exits entirely). The prompt issue causes every AI response to generate code blocks, triggering infinite eval loops across tests 1, 3, and 6.

Output: Safe PY dispatch for coroutine collections, rewritten AI system prompt.
</objective>

<execution_context>
@/Users/dzaramelcone/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dzaramelcone/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/20-ai-eval-loop/20-UAT.md
@.planning/debug/unawaited-coroutines-crash.md
@bae/repl/shell.py
@bae/repl/ai_prompt.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Safe coroutine collection handling in PY dispatch</name>
  <files>bae/repl/shell.py, tests/repl/test_exec.py</files>
  <action>
Add a module-level helper `_contains_coroutines(obj)` to shell.py that recursively checks if an object IS a coroutine or CONTAINS coroutines inside list/tuple/set/dict. Use a `_seen` set (by id) to avoid infinite recursion on circular references. Only traverse list, tuple, set, dict -- not arbitrary iterables.

In `_dispatch()` PY mode, at line 305 where it currently does:
```
elif result is not None:
    self.router.write("py", repr(result), mode="PY", metadata={"type": "expr_result"})
```

Change to:
```
elif result is not None:
    if _contains_coroutines(result):
        n = _count_and_close_coroutines(result)
        msg = f"<{n} unawaited coroutine{'s' if n != 1 else ''}>"
        self.router.write("py", msg, mode="PY", metadata={"type": "warning"})
        self.namespace.pop("_", None)
    else:
        self.router.write("py", repr(result), mode="PY", metadata={"type": "expr_result"})
```

The `_count_and_close_coroutines(obj)` helper should recursively find all coroutines, call `.close()` on each to prevent RuntimeWarnings, and return the count. This is cleaner than letting them GC.

Also apply the same check inside the `_py_task` inner function (the async callback for awaited results, around line 297) so that if an awaited coroutine returns a collection containing coroutines, those are also handled.

Write tests in tests/repl/test_exec.py:
- `test_contains_coroutines_single` -- single coroutine detected
- `test_contains_coroutines_list` -- list of coroutines detected
- `test_contains_coroutines_nested` -- nested lists detected
- `test_contains_coroutines_dict` -- dict values detected
- `test_contains_coroutines_plain` -- plain values return False
- `test_contains_coroutines_mixed` -- mixed list (ints + coroutines) detected

Import the helpers from shell.py in the test file.
  </action>
  <verify>
`uv run python -m pytest tests/repl/test_exec.py -v --tb=short` -- all new tests pass.
`uv run python -m pytest tests/repl/ -q --tb=short` -- full suite passes with zero regressions.
  </verify>
  <done>
`_contains_coroutines()` detects coroutines in collections. `_count_and_close_coroutines()` closes them and returns count. PY dispatch shows "<N unawaited coroutines>" instead of crashing. 233+ tests pass.
  </done>
</task>

<task type="auto">
  <name>Task 2: Rewrite AI system prompt to distinguish NL from tool-use</name>
  <files>bae/repl/ai_prompt.md</files>
  <action>
Rewrite bae/repl/ai_prompt.md to fix the root cause of runaway code generation. The current prompt says "Use python fences for tool calls. 1 fence per turn" which makes the AI produce code on EVERY response, triggering the eval loop endlessly.

New prompt structure:

```markdown
# Session
You are the AI inside cortex, a Python REPL. You share a namespace with the user.

## Rules
- Answer in natural language by default. Be concise.
- Reference the namespace state directly.

## When to write code
Write a ```python fence ONLY when you need to:
- Inspect something (ns(), store.search(), type(), dir())
- Compute a result (arithmetic, data processing, string ops)
- Demonstrate code the user asked for
- Modify the namespace (define/assign/import)

Do NOT write code to answer questions you already know (general knowledge, explanations, opinions, how-to guidance). Just answer.

1 fence per turn max. Every fence is immediately executed.

## Tools
- `ns()` list all namespace objects with types.
    `ns(obj)` deep-inspect any object.
- `store()` - session timeline, conversation history, context, RAG
    `store.recent(N)`
    `store.search("term")`
- `channels` output routing object.
- Python interpreter: define classes, call functions, import modules.

## Examples

User: what is a Graph in bae?
You: A Graph is a directed agent graph built from Node type hints. It routes execution through nodes based on type annotations, with each node producing a typed output that determines the next node.

User: what variables do I have?
You:
```python
ns()
```

User: how many nodes are in my graph?
You:
```python
ns(graph)
```

User: what's 2**100?
You:
```python
2**100
```

User: explain what Dep does
You: Dep is a marker for field-level dependency injection. When a Node field is annotated with Dep[SomeType], the graph automatically resolves and injects that dependency before the node executes.
```

Keep the Begin section with 1 example that shows a code response (ns() call), but add an NL-only follow-up to demonstrate natural language answering. Remove the second example that currently shows ns(Graph) -- it reinforces the "always write code" pattern.

The key changes:
1. "Answer in natural language by default" (was missing entirely)
2. "When to write code" section with explicit criteria
3. "Do NOT write code to answer questions you already know"
4. NL-only response examples alongside code examples
5. Remove "Conciseness - there is to be 1 short line of text per turn" -- this was forcing terse code-only responses
  </action>
  <verify>
Read the file and confirm it has the "When to write code" section, NL-only examples, and no longer forces code on every turn.
  </verify>
  <done>
ai_prompt.md distinguishes NL answers from tool-use code blocks. Rules section says "answer in natural language by default." Explicit criteria for when to write code. NL-only examples present alongside code examples.
  </done>
</task>

</tasks>

<verification>
- `uv run python -m pytest tests/repl/ -q --tb=short` -- full suite green
- Read bae/repl/shell.py and confirm _contains_coroutines and _count_and_close_coroutines exist
- Read bae/repl/ai_prompt.md and confirm "When to write code" section exists with NL examples
</verification>

<success_criteria>
1. PY mode expression `[asyncio.sleep(30) for _ in range(20)]` shows "<20 unawaited coroutines>" instead of crashing
2. AI system prompt guides NL responses by default, code only for inspection/computation
3. All existing tests pass with zero regressions
</success_criteria>

<output>
After completion, create `.planning/phases/20-ai-eval-loop/20-04-SUMMARY.md`
</output>

---
phase: 20-ai-eval-loop
plan: 05
type: execute
wave: 1
depends_on: []
files_modified:
  - bae/repl/channels.py
  - bae/repl/ai.py
  - tests/repl/test_channels.py
  - tests/repl/test_ai.py
autonomous: true
gap_closure: true

must_haves:
  truths:
    - "[ai] channel output includes session indicator (e.g. [ai:1]) to distinguish multi-session responses"
    - "Eval loop execution output is visible to the user in the terminal"
  artifacts:
    - path: "bae/repl/channels.py"
      provides: "Channel._display uses metadata label for session indicator"
      contains: "label"
    - path: "bae/repl/ai.py"
      provides: "Eval loop tees execution output to [py] channel for user visibility"
      contains: "router.write"
  key_links:
    - from: "bae/repl/ai.py"
      to: "bae/repl/channels.py"
      via: "router.write with metadata containing label"
      pattern: 'metadata=.*"label"'
    - from: "bae/repl/channels.py"
      to: "Channel._display"
      via: "metadata label included in channel prefix"
      pattern: "ai:\\d"
---

<objective>
Fix channel session indicator display and eval loop output visibility.

Purpose: Multi-session AI output shows identical [ai] labels making it impossible to distinguish sessions. Eval loop code execution results are silently piped to AI but invisible to the user.

Output: [ai:N] labels on multi-session output, eval loop results teed to [py] channel.
</objective>

<execution_context>
@/Users/dzaramelcone/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dzaramelcone/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/20-ai-eval-loop/20-UAT.md
@bae/repl/channels.py
@bae/repl/ai.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Channel session indicator from metadata label</name>
  <files>bae/repl/channels.py, tests/repl/test_channels.py</files>
  <action>
Channel._display() currently uses `self.label` which is always `f"[{self.name}]"` = `[ai]`. The metadata containing `{"label": "1"}` is passed through `write()` but never reaches `_display()`.

Fix: Pass metadata through to `_display()` and format the label as `[ai:N]` when metadata contains a "label" key.

1. Change `Channel.write()` signature to pass metadata to `_display()`:
```python
def write(self, content, *, mode="", direction="output", metadata=None):
    # ... record + buffer unchanged ...
    if self.visible:
        self._display(content, metadata=metadata)
```

2. Change `Channel._display()` to accept metadata and use label:
```python
def _display(self, content, *, metadata=None):
    label_text = self.label
    if metadata and "label" in metadata:
        label_text = f"[{self.name}:{metadata['label']}]"
    # Use label_text instead of self.label in the FormattedText tuples
```

Apply this to BOTH the markdown branch and the non-markdown branch of _display().

Add tests in tests/repl/test_channels.py:
- `test_display_with_session_label` -- writing with metadata={"label": "2"} produces `[ai:2]` prefix (mock print_formatted_text, inspect the FormattedText arg)
- `test_display_without_label` -- writing without label metadata produces default `[ai]` prefix
- `test_display_markdown_with_label` -- markdown channel with label shows `[ai:N]` prefix before markdown block
  </action>
  <verify>
`uv run python -m pytest tests/repl/test_channels.py -v --tb=short` -- all tests pass including new ones.
  </verify>
  <done>
Channel._display() shows `[ai:N]` when metadata contains label. Default `[ai]` shown when no label. Tests verify both cases.
  </done>
</task>

<task type="auto">
  <name>Task 2: Tee eval loop execution output to user display</name>
  <files>bae/repl/ai.py, tests/repl/test_ai.py</files>
  <action>
In AI.__call__ eval loop (around line 112), execution results are collected in `results` for AI feedback but never displayed to the user. The code write on line 112 shows the CODE being executed, but not the OUTPUT.

After the code write and before collecting into results, add a router.write for the output:

```python
for code in blocks:
    try:
        result, captured = await async_exec(code, self._namespace)
        if asyncio.iscoroutine(result):
            result = await result
        output = captured
        if result is not None:
            output += repr(result)
        results.append(output or "(no output)")
    except (asyncio.CancelledError, KeyboardInterrupt, SystemExit):
        raise
    except BaseException:
        results.append(traceback.format_exc())
    self._router.write("py", code, mode="PY", metadata={"type": "ai_exec", "label": self._label})
```

Change to add output tee AFTER the code write:

```python
for code in blocks:
    try:
        result, captured = await async_exec(code, self._namespace)
        if asyncio.iscoroutine(result):
            result = await result
        output = captured
        if result is not None:
            output += repr(result)
        results.append(output or "(no output)")
    except (asyncio.CancelledError, KeyboardInterrupt, SystemExit):
        raise
    except BaseException:
        tb = traceback.format_exc()
        results.append(tb)
        output = tb  # Show error to user too
    self._router.write("py", code, mode="PY", metadata={"type": "ai_exec", "label": self._label})
    if output:
        self._router.write("py", output, mode="PY", metadata={"type": "ai_exec_result", "label": self._label})
```

Note: `output` needs to be initialized before the try block (set to empty string) so the except branch can also set it for display. Or restructure so `output` is always set. The key thing is: after writing the code to [py], also write the execution output to [py].

Add/update tests in tests/repl/test_ai.py:
- `test_eval_loop_tees_output` -- verify that router.write is called for BOTH the code AND its execution output. Mock _send to return a response with a code block, then return a response without one. Check that router.write was called with metadata type "ai_exec" (code) AND "ai_exec_result" (output).
  </action>
  <verify>
`uv run python -m pytest tests/repl/test_ai.py -v --tb=short` -- all tests pass including new one.
`uv run python -m pytest tests/repl/ -q --tb=short` -- full suite green, zero regressions.
  </verify>
  <done>
Eval loop writes execution output to [py] channel with type "ai_exec_result". User can see both the code AND its output in the terminal. AI still gets the output as feedback for its next response. All tests pass.
  </done>
</task>

</tasks>

<verification>
- `uv run python -m pytest tests/repl/ -q --tb=short` -- full suite green
- Read bae/repl/channels.py and confirm _display accepts metadata, uses label for prefix
- Read bae/repl/ai.py and confirm router.write for execution output after code write in eval loop
</verification>

<success_criteria>
1. Multi-session AI output shows [ai:1], [ai:2] etc. in channel prefix
2. Eval loop execution results visible to user on [py] channel (teed, not just piped to AI)
3. All existing tests pass with zero regressions
</success_criteria>

<output>
After completion, create `.planning/phases/20-ai-eval-loop/20-05-SUMMARY.md`
</output>

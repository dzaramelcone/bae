---
phase: 22-tool-call-translation
plan: 02
type: execute
wave: 2
depends_on: ["22-01"]
files_modified:
  - bae/repl/ai.py
  - bae/repl/ai_prompt.md
  - tests/repl/test_ai.py
autonomous: true

must_haves:
  truths:
    - "AI response containing tool tags triggers translate_tool_calls before extract_executable"
    - "ALL translated tool calls execute via async_exec in the REPL namespace"
    - "Combined execution output is fed back to AI with [Tool output] prefix in a single feedback"
    - "User sees tool_translated metadata type on the [py] channel write for each tool call"
    - "System prompt teaches the AI all 5 terse tool tag formats"
    - "Tool call translation counts against max_eval_iters (one iteration per batch of tool calls)"
  artifacts:
    - path: "bae/repl/ai.py"
      provides: "Eval loop with tool call translation before extract_executable"
      contains: "tool_translated"
    - path: "bae/repl/ai_prompt.md"
      provides: "Tool tag reference table and fewshot examples"
      contains: "<R:"
    - path: "tests/repl/test_ai.py"
      provides: "TestEvalLoopToolCalls test class"
      contains: "class TestEvalLoopToolCalls"
  key_links:
    - from: "bae/repl/ai.py"
      to: "bae/repl/exec.py"
      via: "async_exec called with translated Python code"
      pattern: "async_exec\\(tool_code"
    - from: "bae/repl/ai.py"
      to: "bae/repl/channels.py"
      via: "router.write with tool_translated metadata"
      pattern: "tool_translated"
    - from: "bae/repl/ai_prompt.md"
      to: "bae/repl/ai.py"
      via: "System prompt loaded by _load_prompt, teaches AI tool tag syntax"
      pattern: "<R:.*>"
---

<objective>
Wire `translate_tool_calls()` into the eval loop and teach the AI the terse tool tag vocabulary in the system prompt.

Purpose: Complete the tool call translation pipeline -- detection (Plan 01) -> eval loop execution -> AI feedback -> user visibility. The AI learns the shorthand syntax via prompt additions and the eval loop transparently translates and executes tool calls.

Output: Modified eval loop in `AI.__call__()`, updated `ai_prompt.md` with tool tag reference + fewshots, eval loop integration tests.
</objective>

<execution_context>
@/Users/dzaramelcone/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dzaramelcone/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/22-tool-call-translation/22-RESEARCH.md
@.planning/phases/22-tool-call-translation/22-01-SUMMARY.md
@bae/repl/ai.py
@bae/repl/ai_prompt.md
@tests/repl/test_ai.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Wire translate_tool_calls into eval loop</name>
  <files>bae/repl/ai.py, tests/repl/test_ai.py</files>
  <action>
Modify `AI.__call__()` in `bae/repl/ai.py`. Inside the `for _ in range(self._max_eval_iters)` loop, insert a tool call translation check BEFORE the existing `extract_executable()` call:

```python
for _ in range(self._max_eval_iters):
    # Tool call tags take precedence over <run> blocks
    tool_codes = translate_tool_calls(response)
    if tool_codes:
        all_outputs = []
        for tool_code in tool_codes:
            output = ""
            try:
                result, captured = await async_exec(tool_code, self._namespace)
                if asyncio.iscoroutine(result):
                    result = await result
                output = captured
                if result is not None:
                    output += repr(result)
                output = output or "(no output)"
            except (asyncio.CancelledError, KeyboardInterrupt, SystemExit):
                raise
            except BaseException:
                output = traceback.format_exc()

            self._router.write("py", tool_code, mode="PY",
                metadata={"type": "tool_translated", "label": self._label})
            if output:
                self._router.write("py", output, mode="PY",
                    metadata={"type": "tool_result", "label": self._label})
            all_outputs.append(output)

        combined = "\n---\n".join(all_outputs)
        feedback = f"[Tool output]\n{combined}"
        await asyncio.sleep(0)  # cancellation checkpoint
        response = await self._send(feedback)
        await asyncio.sleep(0)  # cancellation checkpoint
        self._router.write("ai", response, mode="NL",
            metadata={"type": "response", "label": self._label})
        continue

    # Existing: check for <run> blocks
    code, extra = self.extract_executable(response)
    ...
```

The structure mirrors the existing `<run>` block execution but uses:
- `translate_tool_calls()` instead of `extract_executable()` — returns a **list** of translated code strings
- Each tool call in the list is executed independently via `async_exec`
- All outputs are collected and combined with `---` separators into a single feedback
- `"tool_translated"` metadata type instead of `"ai_exec"`
- `"tool_result"` metadata type instead of `"ai_exec_result"`
- `"[Tool output]"` feedback prefix instead of `"[Output]"`
- One `_send()` call per iteration with combined output (not one per tool call)

Add `TestEvalLoopToolCalls` to `tests/repl/test_ai.py` with these tests:

1. `test_tool_call_triggers_translation` — Response with `<R:foo.py>` triggers `translate_tool_calls` and `async_exec` with the translated code, second response has no tags
2. `test_tool_call_feeds_back_output` — `async_exec` output is fed back with `[Tool output]` prefix
3. `test_tool_call_metadata_type` — Router write uses `tool_translated` and `tool_result` metadata types
4. `test_tool_call_before_run_block` — Response with BOTH a tool tag AND a `<run>` block: tool tag takes precedence, `<run>` block not executed
5. `test_tool_call_counts_against_iters` — Tool call translations count against `max_eval_iters` (loop stops at limit)
6. `test_tool_call_error_fed_back` — Exception from `async_exec` is caught and fed back as traceback (not raised)
7. `test_multiple_tool_calls_all_executed` — Response with 2 tool tags: both are executed, outputs combined with `---` separator, single feedback sent

Use the same fixture pattern as `TestEvalLoop` — `eval_ai` fixture with mocked `_send`. Patch `bae.repl.ai.async_exec` and `bae.repl.ai.translate_tool_calls`.
  </action>
  <verify>
`pytest tests/repl/test_ai.py::TestEvalLoopToolCalls -v` — all 6 tests pass.
`pytest tests/repl/test_ai.py -v` — full suite green.
  </verify>
  <done>
Eval loop checks `translate_tool_calls()` before `extract_executable()`. ALL tool tags in response are translated and executed, outputs combined into single feedback. Router writes use `tool_translated`/`tool_result` metadata per tool call. One eval loop iteration consumed per batch.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add tool tag vocabulary to system prompt</name>
  <files>bae/repl/ai_prompt.md, tests/repl/test_ai.py</files>
  <action>
Add a new section to `bae/repl/ai_prompt.md` AFTER the "Code execution convention" section and BEFORE the "## Tools" section. Title it `## File and search tools`:

```markdown
## File and search tools
You have shorthand tags for file operations. These execute directly -- no <run> block needed.

| Tag | Purpose | Example |
|-----|---------|---------|
| `<R:path>` | Read file | `<R:src/main.py>` |
| `<W:path>content</W>` | Write file | `<W:out.txt>hello</W>` |
| `<E:path:start-end>` | Show lines | `<E:src/main.py:10-25>` |
| `<E:path:start-end>new content</E>` | Replace lines | `<E:src/main.py:10-15>fixed code</E>` |
| `<G:pattern>` | Find files | `<G:src/**/*.py>` |
| `<Grep:pattern>` | Search content | `<Grep:def main>` |

Tags go on their own line. Line numbers are 1-based, inclusive. Output is truncated to 4000 chars. Use `<E:path:start-end>` for targeted reads of large files.
```

Add 3 fewshot examples to the existing `<examples>` block:

```markdown
<example>
User: what's in src/main.py?
Assistant:
<R:src/main.py>
</example>

<example>
User: find all Python test files
Assistant:
<G:tests/**/*.py>
</example>

<example>
User: search for uses of asyncio.gather
Assistant:
<Grep:asyncio.gather>
</example>
```

Update existing `TestPromptFile` in `tests/repl/test_ai.py`:
- Add `test_prompt_mentions_tool_tags` — prompt contains `<R:`, `<W:`, `<E:`, `<G:`, `<Grep:`

Run full test suite.
  </action>
  <verify>
`pytest tests/repl/test_ai.py::TestPromptFile -v` — all tests pass including new tool tag test.
`pytest tests/repl/test_ai.py -v` — full suite green.
  </verify>
  <done>
System prompt teaches AI all 5 tool tag formats with a reference table. Three fewshot examples show read, glob, and grep usage. Prompt test validates tool tag presence.
  </done>
</task>

</tasks>

<verification>
```bash
pytest tests/repl/test_ai.py -v
pytest tests/repl/ -v
```
All tests pass. Tool call translation is wired into eval loop. System prompt teaches tool tags.
</verification>

<success_criteria>
- Eval loop checks `translate_tool_calls()` before `extract_executable()` on every iteration
- ALL tool tags in a response are translated and executed (not first-only)
- Outputs from all tool calls are combined with `---` separators into a single `[Tool output]` feedback
- Router writes use `tool_translated` and `tool_result` metadata types for each tool call (distinct from `ai_exec`/`ai_exec_result`)
- System prompt contains tool tag reference table with all 5 types
- System prompt has 3 fewshot examples showing tool tag usage
- One eval loop iteration consumed per batch of tool calls (not one per tool call)
- Errors from tool execution are caught and fed back (not raised)
</success_criteria>

<output>
After completion, create `.planning/phases/22-tool-call-translation/22-02-SUMMARY.md`
</output>
